{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libs loaded\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "import h5py\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import walk\n",
    "import random\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "def setGPU():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    config = tf.ConfigProto()\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))\n",
    "setGPU()\n",
    "\n",
    "print('libs loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case10_label11.npy', 'case6_label11.npy', 'case8_label11.npy', 'case10_label6.npy', 'case9_label19.npy', 'case2_label13.npy', 'case9_label7.npy', 'case5_label3.npy', 'case6_label15.npy', 'case6_label8.npy', 'case10_label4.npy', 'case9_label13.npy', 'case3_label6.npy', 'case2_label5.npy', 'case9_label12.npy', 'case6_label9.npy', 'case3_label2.npy', 'case9_label17.npy', 'case10_label2.npy', 'case9_label10.npy', 'case8_label12.npy', 'case8_label4.npy', 'case4_label14.npy', 'case9_label5.npy', 'case4_label6.npy', 'case8_label5.npy', 'case5_label8.npy', 'case4_label3.npy', 'case8_label9.npy', 'case8_label18.npy', 'case8_label15.npy', 'case8_label10.npy', 'case9_label8.npy', 'case4_label11.npy', 'case5_label17.npy', 'case4_label12.npy', 'case3_label16.npy', 'case2_label8.npy', 'case2_label7.npy', 'case8_label19.npy', 'case3_label7.npy', 'case8_label8.npy', 'case4_label19.npy', 'case3_label15.npy', 'case6_label6.npy', 'case5_label18.npy', 'case4_label15.npy', 'case5_label5.npy', 'case4_label4.npy', 'case5_label2.npy', 'case3_label14.npy', 'case3_label13.npy', 'case6_label5.npy', 'case10_label18.npy', 'case8_label3.npy', 'case3_label11.npy', 'case6_label18.npy', 'case4_label1.npy', 'case10_label5.npy', 'case4_label8.npy', 'case5_label12.npy', 'case8_label7.npy', 'case5_label10.npy', 'case5_label4.npy', 'case8_label14.npy', 'case3_label10.npy', 'case6_label4.npy', 'case4_label13.npy', 'case6_label16.npy', 'case8_label16.npy', 'case3_label1.npy', 'case5_label19.npy', 'case2_label12.npy', 'case5_label11.npy', 'case2_label6.npy', 'case3_label8.npy', 'case5_label13.npy', 'case10_label17.npy', 'case6_label13.npy', 'case5_label14.npy', 'case2_label9.npy', 'case9_label2.npy', 'case10_label13.npy', 'case3_label5.npy', 'case9_label11.npy', 'case2_label4.npy', 'case8_label2.npy', 'case2_label18.npy', 'case10_label14.npy', 'case2_label3.npy', 'case10_label15.npy', 'case10_label3.npy', 'case5_label6.npy', 'case5_label7.npy', 'case2_label17.npy', 'case10_label8.npy', 'case5_label16.npy', 'case4_label2.npy', 'case2_label10.npy', 'case3_label17.npy', 'case3_label3.npy', 'case9_label3.npy', 'case2_label16.npy', 'case9_label16.npy', 'case4_label7.npy', 'case6_label3.npy', 'case10_label12.npy', 'case4_label10.npy', 'case3_label9.npy', 'case6_label17.npy', 'case6_label7.npy', 'case2_label15.npy', 'case6_label10.npy', 'case2_label11.npy', 'case9_label9.npy', 'case8_label6.npy', 'case10_label10.npy', 'case6_label12.npy', 'case4_label9.npy', 'case8_label17.npy', 'case10_label9.npy', 'case9_label15.npy', 'case10_label16.npy', 'case8_label13.npy', 'case6_label14.npy', 'case9_label14.npy', 'case6_label2.npy', 'case9_label18.npy', 'case5_label15.npy', 'case2_label2.npy', 'case4_label5.npy', 'case3_label4.npy', 'case4_label16.npy', 'case9_label6.npy', 'case10_label7.npy', 'case2_label14.npy', 'case5_label9.npy', 'case3_label12.npy', 'case4_label17.npy', 'case9_label4.npy', 'case4_label18.npy', 'case10_label11.npy', 'case6_label11.npy', 'case8_label11.npy', 'case10_label6.npy', 'case9_label19.npy', 'case2_label13.npy', 'case9_label7.npy', 'case5_label3.npy', 'case6_label15.npy', 'case6_label8.npy', 'case10_label4.npy', 'case9_label13.npy', 'case3_label6.npy', 'case2_label5.npy', 'case9_label12.npy', 'case6_label9.npy', 'case3_label2.npy', 'case9_label17.npy', 'case10_label2.npy', 'case9_label10.npy', 'case8_label12.npy', 'case8_label4.npy', 'case4_label14.npy', 'case9_label5.npy', 'case4_label6.npy', 'case8_label5.npy', 'case5_label8.npy', 'case4_label3.npy', 'case8_label9.npy', 'case8_label18.npy', 'case8_label15.npy', 'case8_label10.npy', 'case9_label8.npy', 'case4_label11.npy', 'case5_label17.npy', 'case4_label12.npy', 'case3_label16.npy', 'case2_label8.npy', 'case2_label7.npy', 'case8_label19.npy', 'case3_label7.npy', 'case8_label8.npy', 'case4_label19.npy', 'case3_label15.npy', 'case6_label6.npy', 'case5_label18.npy', 'case4_label15.npy', 'case5_label5.npy', 'case4_label4.npy', 'case5_label2.npy', 'case3_label14.npy', 'case3_label13.npy', 'case6_label5.npy', 'case10_label18.npy', 'case8_label3.npy', 'case3_label11.npy', 'case6_label18.npy', 'case4_label1.npy', 'case10_label5.npy', 'case4_label8.npy', 'case5_label12.npy', 'case8_label7.npy', 'case5_label10.npy', 'case5_label4.npy', 'case8_label14.npy', 'case3_label10.npy', 'case6_label4.npy', 'case4_label13.npy', 'case6_label16.npy', 'case8_label16.npy', 'case3_label1.npy', 'case5_label19.npy', 'case2_label12.npy', 'case5_label11.npy', 'case2_label6.npy', 'case3_label8.npy', 'case5_label13.npy', 'case10_label17.npy', 'case6_label13.npy', 'case5_label14.npy', 'case2_label9.npy', 'case9_label2.npy', 'case10_label13.npy', 'case3_label5.npy', 'case9_label11.npy', 'case2_label4.npy', 'case8_label2.npy', 'case2_label18.npy', 'case10_label14.npy', 'case2_label3.npy', 'case10_label15.npy', 'case10_label3.npy', 'case5_label6.npy', 'case5_label7.npy', 'case2_label17.npy', 'case10_label8.npy', 'case5_label16.npy', 'case4_label2.npy', 'case2_label10.npy', 'case3_label17.npy', 'case3_label3.npy', 'case9_label3.npy', 'case2_label16.npy', 'case9_label16.npy', 'case4_label7.npy', 'case6_label3.npy', 'case10_label12.npy', 'case4_label10.npy', 'case3_label9.npy', 'case6_label17.npy', 'case6_label7.npy', 'case2_label15.npy', 'case6_label10.npy', 'case2_label11.npy', 'case9_label9.npy', 'case8_label6.npy', 'case10_label10.npy', 'case6_label12.npy', 'case4_label9.npy', 'case8_label17.npy', 'case10_label9.npy', 'case9_label15.npy', 'case10_label16.npy', 'case8_label13.npy', 'case6_label14.npy', 'case9_label14.npy', 'case6_label2.npy', 'case9_label18.npy', 'case5_label15.npy', 'case2_label2.npy', 'case4_label5.npy', 'case3_label4.npy', 'case4_label16.npy', 'case9_label6.npy', 'case10_label7.npy', 'case2_label14.npy', 'case5_label9.npy', 'case3_label12.npy', 'case4_label17.npy', 'case9_label4.npy', 'case4_label18.npy']\n",
      "['case1_label9.npy', 'case1_label6.npy', 'case1_label4.npy', 'case1_label19.npy', 'case1_label18.npy', 'case1_label3.npy', 'case1_label10.npy', 'case1_label2.npy', 'case1_label8.npy', 'case1_label17.npy', 'case1_label14.npy', 'case1_label16.npy', 'case1_label5.npy', 'case1_label15.npy', 'case1_label12.npy', 'case1_label11.npy', 'case1_label7.npy', 'case1_label13.npy', 'case1_label9.npy', 'case1_label6.npy', 'case1_label4.npy', 'case1_label19.npy', 'case1_label18.npy', 'case1_label3.npy', 'case1_label10.npy', 'case1_label2.npy', 'case1_label8.npy', 'case1_label17.npy', 'case1_label14.npy', 'case1_label16.npy', 'case1_label5.npy', 'case1_label15.npy', 'case1_label12.npy', 'case1_label11.npy', 'case1_label7.npy', 'case1_label13.npy']\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "data_path = '../data/seg_data/{}/'\n",
    "size = 128\n",
    "\n",
    "def load_filenames(path):\n",
    "    filenames = []\n",
    "    for (dirpath, dirnames, filenames) in walk(data_path.format('x')):\n",
    "        filenames.extend(filenames)\n",
    "        break\n",
    "    for f in filenames:\n",
    "        if not '.npy' in f:\n",
    "            del f\n",
    "    training_filenames = []\n",
    "    validation_filenames = []\n",
    "    for f in filenames:\n",
    "        if 'case1_' in f:\n",
    "            validation_filenames.append(f)\n",
    "        else:\n",
    "            training_filenames.append(f)\n",
    "            \n",
    "            \n",
    "    return training_filenames, validation_filenames\n",
    "    \n",
    "training_filenames, validation_filenames = load_filenames(data_path)\n",
    "\n",
    "print(training_filenames)\n",
    "print(validation_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 128, 128, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128, 128, 128 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 128, 128, 128 448         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 128 64          conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 128 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_17 (Conv3D)              (None, 128, 128, 128 13856       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 128 128         conv3d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 128 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3D)  (None, 64, 64, 64, 3 0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_18 (Conv3D)              (None, 64, 64, 64, 3 27680       max_pooling3d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 64, 3 128         conv3d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 64, 64, 3 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_19 (Conv3D)              (None, 64, 64, 64, 6 55360       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 64, 64, 6 256         conv3d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 64, 6 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3D)  (None, 32, 32, 32, 6 0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_20 (Conv3D)              (None, 32, 32, 32, 6 110656      max_pooling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 32, 6 256         conv3d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 32, 6 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_21 (Conv3D)              (None, 32, 32, 32, 1 221312      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 32, 1 512         conv3d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 32, 1 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3D)  (None, 16, 16, 16, 1 0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_22 (Conv3D)              (None, 16, 16, 16, 1 442496      max_pooling3d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 16, 1 512         conv3d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 16, 1 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_23 (Conv3D)              (None, 16, 16, 16, 2 884992      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 16, 2 1024        conv3d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 16, 2 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_4 (UpSampling3D)  (None, 32, 32, 32, 2 0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 32, 3 0           activation_21[0][0]              \n",
      "                                                                 up_sampling3d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_24 (Conv3D)              (None, 32, 32, 32, 1 1327232     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 32, 1 512         conv3d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 32, 1 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_25 (Conv3D)              (None, 32, 32, 32, 1 442496      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 32, 32, 1 512         conv3d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 32, 32, 1 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_5 (UpSampling3D)  (None, 64, 64, 64, 1 0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 64, 1 0           activation_19[0][0]              \n",
      "                                                                 up_sampling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_26 (Conv3D)              (None, 64, 64, 64, 6 331840      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 64, 64, 6 256         conv3d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64, 64, 64, 6 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_27 (Conv3D)              (None, 64, 64, 64, 6 110656      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 64, 6 256         conv3d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 64, 64, 64, 6 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_6 (UpSampling3D)  (None, 128, 128, 128 0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 128, 128 0           activation_17[0][0]              \n",
      "                                                                 up_sampling3d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_28 (Conv3D)              (None, 128, 128, 128 82976       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 128, 128, 128 128         conv3d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 128, 128, 128 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_29 (Conv3D)              (None, 128, 128, 128 27680       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 128, 128, 128 128         conv3d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 128, 128, 128 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_30 (Conv3D)              (None, 128, 128, 128 865         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 128, 128, 128 4           conv3d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 128, 128, 128 0           batch_normalization_30[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 4,085,221\n",
      "Trainable params: 4,082,883\n",
      "Non-trainable params: 2,338\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def model():\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, Concatenate, BatchNormalization, Activation\n",
    "    \n",
    "    def conv(input_tensor, depth):\n",
    "        conv_tensor = Conv3D(depth, 3, padding = 'same', kernel_initializer = 'he_normal')(input_tensor)\n",
    "        bn = BatchNormalization()(conv_tensor) # use_bias=False\n",
    "        output_tensor = Activation('relu')(bn)\n",
    "        return output_tensor\n",
    "    \n",
    "    def pool(input_tensor):\n",
    "        output_tensor = MaxPooling3D(pool_size=(2, 2, 2))(input_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "    def up(input_tensor):\n",
    "        output_tensor = UpSampling3D(size=(2, 2, 2))(input_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "    def skip(input_tensor1, input_tensor2):\n",
    "        output_tensor = Concatenate(axis=-1)([input_tensor1, input_tensor2])\n",
    "        return output_tensor\n",
    "    \n",
    "    depth = 16\n",
    "    \n",
    "    input_img = Input(shape=(size, size, size, 1))\n",
    "    \n",
    "    conv1 = conv(input_img, depth)\n",
    "    conv2 = conv(conv1, depth * 2)\n",
    "    \n",
    "    pool1 = pool(conv2)\n",
    "    \n",
    "    conv3 = conv(pool1, depth * 2)\n",
    "    conv4 = conv(conv3, depth * 4)\n",
    "    \n",
    "    pool2 = pool(conv4)\n",
    "    \n",
    "    conv5 = conv(pool2, depth * 4)\n",
    "    conv6 = conv(conv5, depth * 8)\n",
    "    \n",
    "    pool3 = pool(conv6)\n",
    "    \n",
    "    conv7 = conv(pool3, depth * 8)\n",
    "    conv8 = conv(conv7, depth * 16)   \n",
    "\n",
    "    up1 = up(conv8)\n",
    "    \n",
    "    skip1 = skip(conv6, up1)\n",
    "    \n",
    "    conv9 = conv(skip1, depth * 8)\n",
    "    conv10 = conv(conv9, depth * 8)\n",
    "    \n",
    "    up2   = up(conv10)\n",
    "    \n",
    "    skip2 = skip(conv4, up2)\n",
    "    \n",
    "    conv11 = conv(skip2, depth * 4)\n",
    "    conv12 = conv(conv11, depth * 4)\n",
    "    \n",
    "    up3   = up(conv12)\n",
    "    \n",
    "    skip3 = skip(conv2, up3)\n",
    "    \n",
    "    conv13 = conv(skip3, depth * 2)\n",
    "    conv14 = conv(conv13, depth * 2)    \n",
    "   \n",
    "    conv15 = conv(conv14, 1)\n",
    "\n",
    "    output_img = conv15\n",
    "\n",
    "    # model\n",
    "    model = Model(inputs=input_img, outputs=output_img)\n",
    "    print (model.output_shape)\n",
    "\n",
    "    # optimizer\n",
    "    opt = keras.optimizers.Adam(lr=1e-2)#32-5,16-3\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batch_size, filenames):\n",
    "    while 1:\n",
    "        shape = (batch_size, size, size, size, 1)\n",
    "        \n",
    "        x_out = np.zeros(shape)\n",
    "        y_out = np.zeros(shape)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            filename = random.choice(filenames)\n",
    "            #filename = filenames[0]\n",
    "            x = np.load(data_path.format('x') + filename)\n",
    "            y = np.load(data_path.format('y') + filename)\n",
    "            x = np.reshape(x, (size, size, size, -1))\n",
    "            y = np.reshape(y, (size, size, size, -1))\n",
    "            \n",
    "            x_out[i] = x\n",
    "            y_out[i] = y\n",
    "            \n",
    "            # bias\n",
    "#             bias = 3\n",
    "#             x_bias, y_bias, z_bias = random.randint(-bias,bias), random.randint(-bias,bias), random.randint(-bias,bias)\n",
    "#             x_range = (x_bias, 127) if x_bias > 0 else (0, size + x_bias -1)\n",
    "#             y_range = (y_bias, 127) if y_bias > 0 else (0, size + y_bias -1)\n",
    "#             z_range = (z_bias, 127) if z_bias > 0 else (0, size + z_bias -1)\n",
    "\n",
    "#             x_out[i,x_range[0]:x_range[1],y_range[0]:y_range[1],z_range[0]:z_range[1],:] = x[x_range[0]:x_range[1],y_range[0]:y_range[1],z_range[0]:z_range[1]]\n",
    "#             y_out[i,x_range[0]:x_range[1],y_range[0]:y_range[1],z_range[0]:z_range[1],:] = y[x_range[0]:x_range[1],y_range[0]:y_range[1],z_range[0]:z_range[1]]\n",
    "            \n",
    "        yield (x_out, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0055 - acc: 0.9922Epoch 00001: val_loss improved from inf to 0.01113, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 15s 1s/step - loss: 0.0063 - acc: 0.9909 - val_loss: 0.0111 - val_acc: 0.9875\n",
      "Epoch 2/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9922Epoch 00002: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0054 - acc: 0.9921 - val_loss: 0.0120 - val_acc: 0.9867\n",
      "Epoch 3/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0061 - acc: 0.9914Epoch 00003: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0059 - acc: 0.9915 - val_loss: 0.0136 - val_acc: 0.9851\n",
      "Epoch 4/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0063 - acc: 0.9908Epoch 00004: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0063 - acc: 0.9907 - val_loss: 0.0122 - val_acc: 0.9824\n",
      "Epoch 5/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0055 - acc: 0.9918Epoch 00005: val_loss improved from 0.01113 to 0.00733, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0059 - acc: 0.9911 - val_loss: 0.0073 - val_acc: 0.9900\n",
      "Epoch 6/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0060 - acc: 0.9912Epoch 00006: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0063 - acc: 0.9909 - val_loss: 0.0357 - val_acc: 0.9429\n",
      "Epoch 7/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0050 - acc: 0.9931Epoch 00007: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0050 - acc: 0.9930 - val_loss: 0.0114 - val_acc: 0.9831\n",
      "Epoch 8/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0061 - acc: 0.9911Epoch 00008: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0063 - acc: 0.9909 - val_loss: 0.0082 - val_acc: 0.9892\n",
      "Epoch 9/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0071 - acc: 0.9904Epoch 00009: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0068 - acc: 0.9907 - val_loss: 0.0102 - val_acc: 0.9871\n",
      "Epoch 10/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0058 - acc: 0.9923Epoch 00010: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0060 - acc: 0.9919 - val_loss: 0.0095 - val_acc: 0.9880\n",
      "Epoch 11/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0050 - acc: 0.9929Epoch 00011: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0050 - acc: 0.9929 - val_loss: 0.0080 - val_acc: 0.9887\n",
      "Epoch 12/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0053 - acc: 0.9923Epoch 00012: val_loss improved from 0.00733 to 0.00645, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0052 - acc: 0.9924 - val_loss: 0.0064 - val_acc: 0.9908\n",
      "Epoch 13/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0054 - acc: 0.9923Epoch 00013: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0059 - acc: 0.9914 - val_loss: 0.0147 - val_acc: 0.9765\n",
      "Epoch 14/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9926Epoch 00014: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0056 - acc: 0.9917 - val_loss: 0.0090 - val_acc: 0.9877\n",
      "Epoch 15/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9932Epoch 00015: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0050 - acc: 0.9929 - val_loss: 0.0075 - val_acc: 0.9897\n",
      "Epoch 16/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0058 - acc: 0.9921Epoch 00016: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0058 - acc: 0.9920 - val_loss: 0.0071 - val_acc: 0.9898\n",
      "Epoch 17/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0054 - acc: 0.9922Epoch 00017: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0055 - acc: 0.9921 - val_loss: 0.0076 - val_acc: 0.9896\n",
      "Epoch 18/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0055 - acc: 0.9925Epoch 00018: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0057 - acc: 0.9921 - val_loss: 0.0082 - val_acc: 0.9890\n",
      "Epoch 19/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0062 - acc: 0.9911Epoch 00019: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0061 - acc: 0.9913 - val_loss: 0.0140 - val_acc: 0.9855\n",
      "Epoch 20/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0055 - acc: 0.9922Epoch 00020: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0055 - acc: 0.9921 - val_loss: 0.0139 - val_acc: 0.9818\n",
      "Epoch 21/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0063 - acc: 0.9906Epoch 00021: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0064 - acc: 0.9904 - val_loss: 0.0327 - val_acc: 0.9393\n",
      "Epoch 22/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0060 - acc: 0.9915Epoch 00022: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0058 - acc: 0.9917 - val_loss: 0.0240 - val_acc: 0.9667\n",
      "Epoch 23/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0068 - acc: 0.9902Epoch 00023: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0067 - acc: 0.9903 - val_loss: 0.0265 - val_acc: 0.9642\n",
      "Epoch 24/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9929Epoch 00024: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0048 - acc: 0.9929 - val_loss: 0.0087 - val_acc: 0.9881\n",
      "Epoch 25/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0061 - acc: 0.9909Epoch 00025: val_loss improved from 0.00645 to 0.00622, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0059 - acc: 0.9912 - val_loss: 0.0062 - val_acc: 0.9914\n",
      "Epoch 26/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0057 - acc: 0.9915Epoch 00026: val_loss improved from 0.00622 to 0.00562, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0056 - acc: 0.9916 - val_loss: 0.0056 - val_acc: 0.9924\n",
      "Epoch 27/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - acc: 0.9928Epoch 00027: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0053 - acc: 0.9922 - val_loss: 0.0131 - val_acc: 0.9864\n",
      "Epoch 28/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0060 - acc: 0.9914Epoch 00028: val_loss improved from 0.00562 to 0.00533, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0060 - acc: 0.9915 - val_loss: 0.0053 - val_acc: 0.9926\n",
      "Epoch 29/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0057 - acc: 0.9917Epoch 00029: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0060 - acc: 0.9913 - val_loss: 0.0061 - val_acc: 0.9912\n",
      "Epoch 30/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0051 - acc: 0.9924Epoch 00030: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0052 - acc: 0.9923 - val_loss: 0.0126 - val_acc: 0.9857\n",
      "Epoch 31/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0056 - acc: 0.9916Epoch 00031: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0054 - acc: 0.9918 - val_loss: 0.0122 - val_acc: 0.9878\n",
      "Epoch 32/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0055 - acc: 0.9923Epoch 00032: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0054 - acc: 0.9924 - val_loss: 0.0115 - val_acc: 0.9869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0055 - acc: 0.9921Epoch 00033: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0055 - acc: 0.9922 - val_loss: 0.0087 - val_acc: 0.9883\n",
      "Epoch 34/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0059 - acc: 0.9914Epoch 00034: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0060 - acc: 0.9913 - val_loss: 0.0055 - val_acc: 0.9919\n",
      "Epoch 35/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0063 - acc: 0.9906Epoch 00035: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0062 - acc: 0.9908 - val_loss: 0.0107 - val_acc: 0.9889\n",
      "Epoch 36/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0050 - acc: 0.9927Epoch 00036: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0051 - acc: 0.9927 - val_loss: 0.0105 - val_acc: 0.9876\n",
      "Epoch 37/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9924Epoch 00037: val_loss improved from 0.00533 to 0.00482, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0051 - acc: 0.9926 - val_loss: 0.0048 - val_acc: 0.9930\n",
      "Epoch 38/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0045 - acc: 0.9936Epoch 00038: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0044 - acc: 0.9937 - val_loss: 0.0058 - val_acc: 0.9917\n",
      "Epoch 39/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0053 - acc: 0.9919Epoch 00039: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0053 - acc: 0.9919 - val_loss: 0.0076 - val_acc: 0.9896\n",
      "Epoch 40/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0055 - acc: 0.9923Epoch 00040: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0057 - acc: 0.9921 - val_loss: 0.0048 - val_acc: 0.9933\n",
      "Epoch 41/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0062 - acc: 0.9906Epoch 00041: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0060 - acc: 0.9910 - val_loss: 0.0066 - val_acc: 0.9897\n",
      "Epoch 42/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0057 - acc: 0.9919Epoch 00042: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0057 - acc: 0.9918 - val_loss: 0.0072 - val_acc: 0.9898\n",
      "Epoch 43/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9931Epoch 00043: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0050 - acc: 0.9932 - val_loss: 0.0051 - val_acc: 0.9924\n",
      "Epoch 44/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9932Epoch 00044: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0050 - acc: 0.9930 - val_loss: 0.0064 - val_acc: 0.9911\n",
      "Epoch 45/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0051 - acc: 0.9932Epoch 00045: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0053 - acc: 0.9928 - val_loss: 0.0055 - val_acc: 0.9920\n",
      "Epoch 46/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0054 - acc: 0.9922Epoch 00046: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0053 - acc: 0.9923 - val_loss: 0.0049 - val_acc: 0.9927\n",
      "Epoch 47/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9933Epoch 00047: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0048 - acc: 0.9933 - val_loss: 0.0070 - val_acc: 0.9903\n",
      "Epoch 48/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0050 - acc: 0.9928Epoch 00048: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0051 - acc: 0.9926 - val_loss: 0.0071 - val_acc: 0.9901\n",
      "Epoch 49/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9932Epoch 00049: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0045 - acc: 0.9935 - val_loss: 0.0084 - val_acc: 0.9880\n",
      "Epoch 50/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - acc: 0.9933Epoch 00050: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0046 - acc: 0.9934 - val_loss: 0.0071 - val_acc: 0.9900\n",
      "Epoch 51/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9945Epoch 00051: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0043 - acc: 0.9938 - val_loss: 0.0079 - val_acc: 0.9899\n",
      "Epoch 52/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0055 - acc: 0.9925Epoch 00052: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0054 - acc: 0.9928 - val_loss: 0.0061 - val_acc: 0.9917\n",
      "Epoch 53/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0056 - acc: 0.9926Epoch 00053: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0058 - acc: 0.9921 - val_loss: 0.0051 - val_acc: 0.9930\n",
      "Epoch 54/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - acc: 0.9934Epoch 00054: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0046 - acc: 0.9936 - val_loss: 0.0141 - val_acc: 0.9857\n",
      "Epoch 55/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0053 - acc: 0.9925Epoch 00055: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0053 - acc: 0.9925 - val_loss: 0.0122 - val_acc: 0.9875\n",
      "Epoch 56/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0045 - acc: 0.9934Epoch 00056: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0050 - acc: 0.9927 - val_loss: 0.0053 - val_acc: 0.9924\n",
      "Epoch 57/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9933Epoch 00057: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0051 - acc: 0.9933 - val_loss: 0.0110 - val_acc: 0.9851\n",
      "Epoch 58/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9932Epoch 00058: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0049 - acc: 0.9929 - val_loss: 0.0050 - val_acc: 0.9929\n",
      "Epoch 59/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9940Epoch 00059: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0044 - acc: 0.9939 - val_loss: 0.0048 - val_acc: 0.9935\n",
      "Epoch 60/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9925Epoch 00060: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0057 - acc: 0.9916 - val_loss: 0.0101 - val_acc: 0.9858\n",
      "Epoch 61/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0045 - acc: 0.9940Epoch 00061: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0047 - acc: 0.9937 - val_loss: 0.0065 - val_acc: 0.9907\n",
      "Epoch 62/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9929Epoch 00062: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0053 - acc: 0.9928 - val_loss: 0.0049 - val_acc: 0.9928\n",
      "Epoch 63/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9931Epoch 00063: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0050 - acc: 0.9930 - val_loss: 0.0057 - val_acc: 0.9920\n",
      "Epoch 64/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0050 - acc: 0.9928Epoch 00064: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0049 - acc: 0.9930 - val_loss: 0.0088 - val_acc: 0.9888\n",
      "Epoch 65/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0051 - acc: 0.9930Epoch 00065: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0050 - acc: 0.9931 - val_loss: 0.0076 - val_acc: 0.9901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9941Epoch 00066: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9941 - val_loss: 0.0075 - val_acc: 0.9896\n",
      "Epoch 67/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0045 - acc: 0.9937Epoch 00067: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0044 - acc: 0.9938 - val_loss: 0.0063 - val_acc: 0.9907\n",
      "Epoch 68/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9941Epoch 00068: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0048 - acc: 0.9932 - val_loss: 0.0069 - val_acc: 0.9906\n",
      "Epoch 69/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9927Epoch 00069: val_loss improved from 0.00482 to 0.00458, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0049 - acc: 0.9928 - val_loss: 0.0046 - val_acc: 0.9934\n",
      "Epoch 70/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9942Epoch 00070: val_loss improved from 0.00458 to 0.00455, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0041 - acc: 0.9941 - val_loss: 0.0045 - val_acc: 0.9933\n",
      "Epoch 71/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9922Epoch 00071: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0053 - acc: 0.9921 - val_loss: 0.0140 - val_acc: 0.9773\n",
      "Epoch 72/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9930Epoch 00072: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0048 - acc: 0.9932 - val_loss: 0.0190 - val_acc: 0.9696\n",
      "Epoch 73/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0059 - acc: 0.9913Epoch 00073: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0056 - acc: 0.9917 - val_loss: 0.0144 - val_acc: 0.9779\n",
      "Epoch 74/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9938Epoch 00074: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0045 - acc: 0.9936 - val_loss: 0.0209 - val_acc: 0.9624\n",
      "Epoch 75/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0051 - acc: 0.9926Epoch 00075: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0049 - acc: 0.9930 - val_loss: 0.0098 - val_acc: 0.9845\n",
      "Epoch 76/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9940Epoch 00076: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0046 - acc: 0.9933 - val_loss: 0.0050 - val_acc: 0.9927\n",
      "Epoch 77/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0042 - acc: 0.9942Epoch 00077: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0044 - acc: 0.9942 - val_loss: 0.0063 - val_acc: 0.9915\n",
      "Epoch 78/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9929Epoch 00078: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0051 - acc: 0.9930 - val_loss: 0.0070 - val_acc: 0.9892\n",
      "Epoch 79/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9937Epoch 00079: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0044 - acc: 0.9936 - val_loss: 0.0103 - val_acc: 0.9891\n",
      "Epoch 80/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9949Epoch 00080: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0036 - acc: 0.9949 - val_loss: 0.0055 - val_acc: 0.9917\n",
      "Epoch 81/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9941Epoch 00081: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9943 - val_loss: 0.0061 - val_acc: 0.9915\n",
      "Epoch 82/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - acc: 0.9933Epoch 00082: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0048 - acc: 0.9933 - val_loss: 0.0087 - val_acc: 0.9892\n",
      "Epoch 83/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9938Epoch 00083: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0043 - acc: 0.9940 - val_loss: 0.0051 - val_acc: 0.9925\n",
      "Epoch 84/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9947Epoch 00084: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0041 - acc: 0.9945 - val_loss: 0.0064 - val_acc: 0.9907\n",
      "Epoch 85/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9934Epoch 00085: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0046 - acc: 0.9936 - val_loss: 0.0046 - val_acc: 0.9938\n",
      "Epoch 86/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9934Epoch 00086: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0045 - acc: 0.9936 - val_loss: 0.0106 - val_acc: 0.9848\n",
      "Epoch 87/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0050 - acc: 0.9930Epoch 00087: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0048 - acc: 0.9932 - val_loss: 0.0061 - val_acc: 0.9913\n",
      "Epoch 88/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9941Epoch 00088: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0047 - acc: 0.9937 - val_loss: 0.0136 - val_acc: 0.9794\n",
      "Epoch 89/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9942Epoch 00089: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0045 - acc: 0.9939 - val_loss: 0.0110 - val_acc: 0.9872\n",
      "Epoch 90/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9949Epoch 00090: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9949 - val_loss: 0.0052 - val_acc: 0.9925\n",
      "Epoch 91/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0042 - acc: 0.9939Epoch 00091: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9940 - val_loss: 0.0059 - val_acc: 0.9922\n",
      "Epoch 92/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9958Epoch 00092: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9958 - val_loss: 0.0056 - val_acc: 0.9922\n",
      "Epoch 93/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9945Epoch 00093: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0040 - acc: 0.9945 - val_loss: 0.0049 - val_acc: 0.9927\n",
      "Epoch 94/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - acc: 0.9950Epoch 00094: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0034 - acc: 0.9951 - val_loss: 0.0102 - val_acc: 0.9884\n",
      "Epoch 95/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9947Epoch 00095: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9948 - val_loss: 0.0104 - val_acc: 0.9874\n",
      "Epoch 96/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9937Epoch 00096: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9940 - val_loss: 0.0066 - val_acc: 0.9905\n",
      "Epoch 97/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9933Epoch 00097: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0047 - acc: 0.9934 - val_loss: 0.0052 - val_acc: 0.9929\n",
      "Epoch 98/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9946Epoch 00098: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0041 - acc: 0.9942 - val_loss: 0.0094 - val_acc: 0.9877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9930Epoch 00099: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0048 - acc: 0.9931 - val_loss: 0.0049 - val_acc: 0.9930\n",
      "Epoch 100/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0042 - acc: 0.9937Epoch 00100: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0040 - acc: 0.9940 - val_loss: 0.0047 - val_acc: 0.9933\n",
      "Epoch 101/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0061 - acc: 0.9910Epoch 00101: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0058 - acc: 0.9915 - val_loss: 0.0089 - val_acc: 0.9891\n",
      "Epoch 102/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9946Epoch 00102: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0041 - acc: 0.9944 - val_loss: 0.0118 - val_acc: 0.9880\n",
      "Epoch 103/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9951Epoch 00103: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0038 - acc: 0.9949 - val_loss: 0.0124 - val_acc: 0.9875\n",
      "Epoch 104/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9924Epoch 00104: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0054 - acc: 0.9921 - val_loss: 0.0073 - val_acc: 0.9909\n",
      "Epoch 105/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9942Epoch 00105: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0041 - acc: 0.9941 - val_loss: 0.0112 - val_acc: 0.9876\n",
      "Epoch 106/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9959Epoch 00106: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9958 - val_loss: 0.0051 - val_acc: 0.9926\n",
      "Epoch 107/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9944Epoch 00107: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9941 - val_loss: 0.0055 - val_acc: 0.9926\n",
      "Epoch 108/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0042 - acc: 0.9943Epoch 00108: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0043 - acc: 0.9940 - val_loss: 0.0083 - val_acc: 0.9892\n",
      "Epoch 109/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0045 - acc: 0.9937Epoch 00109: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0049 - acc: 0.9932 - val_loss: 0.0103 - val_acc: 0.9883\n",
      "Epoch 110/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9935Epoch 00110: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0044 - acc: 0.9937 - val_loss: 0.0082 - val_acc: 0.9894\n",
      "Epoch 111/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9943Epoch 00111: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0040 - acc: 0.9945 - val_loss: 0.0090 - val_acc: 0.9898\n",
      "Epoch 112/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9949Epoch 00112: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9949 - val_loss: 0.0048 - val_acc: 0.9932\n",
      "Epoch 113/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9945Epoch 00113: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0041 - acc: 0.9944 - val_loss: 0.0086 - val_acc: 0.9898\n",
      "Epoch 114/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9933Epoch 00114: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0047 - acc: 0.9936 - val_loss: 0.0061 - val_acc: 0.9917\n",
      "Epoch 115/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9934Epoch 00115: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0045 - acc: 0.9935 - val_loss: 0.0066 - val_acc: 0.9914\n",
      "Epoch 116/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9929Epoch 00116: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0051 - acc: 0.9926 - val_loss: 0.0088 - val_acc: 0.9897\n",
      "Epoch 117/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9940Epoch 00117: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0043 - acc: 0.9942 - val_loss: 0.0146 - val_acc: 0.9854\n",
      "Epoch 118/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0053 - acc: 0.9933Epoch 00118: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0052 - acc: 0.9934 - val_loss: 0.0140 - val_acc: 0.9859\n",
      "Epoch 119/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9940Epoch 00119: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0043 - acc: 0.9942 - val_loss: 0.0049 - val_acc: 0.9932\n",
      "Epoch 120/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9935Epoch 00120: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0047 - acc: 0.9937 - val_loss: 0.0062 - val_acc: 0.9919\n",
      "Epoch 121/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - acc: 0.9936Epoch 00121: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0047 - acc: 0.9936 - val_loss: 0.0078 - val_acc: 0.9884\n",
      "Epoch 122/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9948Epoch 00122: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0038 - acc: 0.9948 - val_loss: 0.0072 - val_acc: 0.9906\n",
      "Epoch 123/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9936Epoch 00123: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9939 - val_loss: 0.0057 - val_acc: 0.9917\n",
      "Epoch 124/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9933Epoch 00124: val_loss improved from 0.00455 to 0.00453, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0047 - acc: 0.9934 - val_loss: 0.0045 - val_acc: 0.9932\n",
      "Epoch 125/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9941Epoch 00125: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9943 - val_loss: 0.0107 - val_acc: 0.9872\n",
      "Epoch 126/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9945Epoch 00126: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9948 - val_loss: 0.0051 - val_acc: 0.9927\n",
      "Epoch 127/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9937Epoch 00127: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0045 - acc: 0.9936 - val_loss: 0.0070 - val_acc: 0.9899\n",
      "Epoch 128/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0054 - acc: 0.9922Epoch 00128: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0051 - acc: 0.9927 - val_loss: 0.0079 - val_acc: 0.9891\n",
      "Epoch 129/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9938Epoch 00129: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0045 - acc: 0.9939 - val_loss: 0.0134 - val_acc: 0.9865\n",
      "Epoch 130/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9947Epoch 00130: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9948 - val_loss: 0.0133 - val_acc: 0.9867\n",
      "Epoch 131/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - acc: 0.9931Epoch 00131: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0045 - acc: 0.9935 - val_loss: 0.0052 - val_acc: 0.9929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9932Epoch 00132: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0047 - acc: 0.9933 - val_loss: 0.0057 - val_acc: 0.9920\n",
      "Epoch 133/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9948Epoch 00133: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0038 - acc: 0.9947 - val_loss: 0.0046 - val_acc: 0.9931\n",
      "Epoch 134/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9935Epoch 00134: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0044 - acc: 0.9936 - val_loss: 0.0045 - val_acc: 0.9934\n",
      "Epoch 135/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9943Epoch 00135: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0043 - acc: 0.9944 - val_loss: 0.0073 - val_acc: 0.9899\n",
      "Epoch 136/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9936Epoch 00136: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0047 - acc: 0.9938 - val_loss: 0.0161 - val_acc: 0.9765\n",
      "Epoch 137/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9950Epoch 00137: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0038 - acc: 0.9951 - val_loss: 0.0054 - val_acc: 0.9928\n",
      "Epoch 138/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9949Epoch 00138: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9950 - val_loss: 0.0050 - val_acc: 0.9929\n",
      "Epoch 139/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9945Epoch 00139: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0039 - acc: 0.9947 - val_loss: 0.0068 - val_acc: 0.9901\n",
      "Epoch 140/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9956Epoch 00140: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9956 - val_loss: 0.0056 - val_acc: 0.9923\n",
      "Epoch 141/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - acc: 0.9951Epoch 00141: val_loss improved from 0.00453 to 0.00449, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9951 - val_loss: 0.0045 - val_acc: 0.9935\n",
      "Epoch 142/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9945Epoch 00142: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0040 - acc: 0.9946 - val_loss: 0.0052 - val_acc: 0.9924\n",
      "Epoch 143/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9951Epoch 00143: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9953 - val_loss: 0.0057 - val_acc: 0.9915\n",
      "Epoch 144/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9936Epoch 00144: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0044 - acc: 0.9939 - val_loss: 0.0046 - val_acc: 0.9932\n",
      "Epoch 145/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9951Epoch 00145: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0035 - acc: 0.9953 - val_loss: 0.0095 - val_acc: 0.9884\n",
      "Epoch 146/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9940Epoch 00146: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9941 - val_loss: 0.0054 - val_acc: 0.9929\n",
      "Epoch 147/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - acc: 0.9953Epoch 00147: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9954 - val_loss: 0.0050 - val_acc: 0.9930\n",
      "Epoch 148/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9946Epoch 00148: val_loss improved from 0.00449 to 0.00375, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0040 - acc: 0.9941 - val_loss: 0.0037 - val_acc: 0.9947\n",
      "Epoch 149/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9952Epoch 00149: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0040 - acc: 0.9949 - val_loss: 0.0049 - val_acc: 0.9931\n",
      "Epoch 150/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9946Epoch 00150: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0040 - acc: 0.9944 - val_loss: 0.0078 - val_acc: 0.9897\n",
      "Epoch 151/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9950Epoch 00151: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0036 - acc: 0.9951 - val_loss: 0.0058 - val_acc: 0.9918\n",
      "Epoch 152/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9954Epoch 00152: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0034 - acc: 0.9954 - val_loss: 0.0139 - val_acc: 0.9860\n",
      "Epoch 153/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - acc: 0.9954Epoch 00153: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9957 - val_loss: 0.0091 - val_acc: 0.9889\n",
      "Epoch 154/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9948Epoch 00154: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0039 - acc: 0.9946 - val_loss: 0.0066 - val_acc: 0.9910\n",
      "Epoch 155/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9949Epoch 00155: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0035 - acc: 0.9950 - val_loss: 0.0044 - val_acc: 0.9935\n",
      "Epoch 156/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9958Epoch 00156: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9959 - val_loss: 0.0114 - val_acc: 0.9874\n",
      "Epoch 157/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9963Epoch 00157: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9960 - val_loss: 0.0078 - val_acc: 0.9902\n",
      "Epoch 158/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9951Epoch 00158: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0035 - acc: 0.9950 - val_loss: 0.0085 - val_acc: 0.9879\n",
      "Epoch 159/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9948Epoch 00159: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0036 - acc: 0.9950 - val_loss: 0.0143 - val_acc: 0.9857\n",
      "Epoch 160/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9955Epoch 00160: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0034 - acc: 0.9955 - val_loss: 0.0126 - val_acc: 0.9875\n",
      "Epoch 161/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9961Epoch 00161: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0034 - acc: 0.9954 - val_loss: 0.0139 - val_acc: 0.9861\n",
      "Epoch 162/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9950Epoch 00162: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0035 - acc: 0.9951 - val_loss: 0.0128 - val_acc: 0.9861\n",
      "Epoch 163/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9955Epoch 00163: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9954 - val_loss: 0.0116 - val_acc: 0.9876\n",
      "Epoch 164/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9954Epoch 00164: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9954 - val_loss: 0.0070 - val_acc: 0.9901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9939Epoch 00165: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0043 - acc: 0.9939 - val_loss: 0.0092 - val_acc: 0.9884\n",
      "Epoch 166/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9937Epoch 00166: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0044 - acc: 0.9940 - val_loss: 0.0083 - val_acc: 0.9892\n",
      "Epoch 167/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9958Epoch 00167: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9958 - val_loss: 0.0086 - val_acc: 0.9898\n",
      "Epoch 168/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9948Epoch 00168: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9948 - val_loss: 0.0104 - val_acc: 0.9881\n",
      "Epoch 169/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9954Epoch 00169: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9955 - val_loss: 0.0048 - val_acc: 0.9930\n",
      "Epoch 170/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9963Epoch 00170: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0028 - acc: 0.9963 - val_loss: 0.0045 - val_acc: 0.9939\n",
      "Epoch 171/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9964Epoch 00171: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9966 - val_loss: 0.0056 - val_acc: 0.9920\n",
      "Epoch 172/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9960Epoch 00172: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9961 - val_loss: 0.0061 - val_acc: 0.9915\n",
      "Epoch 173/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9959Epoch 00173: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9958 - val_loss: 0.0087 - val_acc: 0.9882\n",
      "Epoch 174/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0045 - acc: 0.9938Epoch 00174: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0043 - acc: 0.9940 - val_loss: 0.0059 - val_acc: 0.9918\n",
      "Epoch 175/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0042 - acc: 0.9944Epoch 00175: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0040 - acc: 0.9946 - val_loss: 0.0069 - val_acc: 0.9907\n",
      "Epoch 176/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9925Epoch 00176: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0056 - acc: 0.9921 - val_loss: 0.0061 - val_acc: 0.9915\n",
      "Epoch 177/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9966Epoch 00177: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0054 - val_acc: 0.9926\n",
      "Epoch 178/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9954Epoch 00178: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9953 - val_loss: 0.0068 - val_acc: 0.9909\n",
      "Epoch 179/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9963Epoch 00179: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9966 - val_loss: 0.0053 - val_acc: 0.9923\n",
      "Epoch 180/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - acc: 0.9935Epoch 00180: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0045 - acc: 0.9938 - val_loss: 0.0086 - val_acc: 0.9886\n",
      "Epoch 181/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0042 - acc: 0.9944Epoch 00181: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0041 - acc: 0.9946 - val_loss: 0.0068 - val_acc: 0.9908\n",
      "Epoch 182/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9951Epoch 00182: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0034 - acc: 0.9954 - val_loss: 0.0077 - val_acc: 0.9902\n",
      "Epoch 183/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9957Epoch 00183: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0034 - acc: 0.9956 - val_loss: 0.0118 - val_acc: 0.9880\n",
      "Epoch 184/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9956Epoch 00184: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9957 - val_loss: 0.0057 - val_acc: 0.9921\n",
      "Epoch 185/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9952Epoch 00185: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0035 - acc: 0.9953 - val_loss: 0.0039 - val_acc: 0.9948\n",
      "Epoch 186/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9970Epoch 00186: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9970 - val_loss: 0.0075 - val_acc: 0.9895\n",
      "Epoch 187/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9954Epoch 00187: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9953 - val_loss: 0.0046 - val_acc: 0.9935\n",
      "Epoch 188/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9941Epoch 00188: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0039 - acc: 0.9944 - val_loss: 0.0049 - val_acc: 0.9934\n",
      "Epoch 189/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9942Epoch 00189: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9944 - val_loss: 0.0054 - val_acc: 0.9928\n",
      "Epoch 190/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9953Epoch 00190: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0038 - acc: 0.9949 - val_loss: 0.0075 - val_acc: 0.9896\n",
      "Epoch 191/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9960Epoch 00191: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9960 - val_loss: 0.0108 - val_acc: 0.9845\n",
      "Epoch 192/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9949Epoch 00192: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0035 - acc: 0.9950 - val_loss: 0.0067 - val_acc: 0.9910\n",
      "Epoch 193/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9964Epoch 00193: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9964 - val_loss: 0.0060 - val_acc: 0.9917\n",
      "Epoch 194/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9954Epoch 00194: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9955 - val_loss: 0.0046 - val_acc: 0.9934\n",
      "Epoch 195/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9946Epoch 00195: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0036 - acc: 0.9947 - val_loss: 0.0047 - val_acc: 0.9935\n",
      "Epoch 196/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9961Epoch 00196: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9961 - val_loss: 0.0107 - val_acc: 0.9881\n",
      "Epoch 197/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9962Epoch 00197: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9962 - val_loss: 0.0109 - val_acc: 0.9875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9954Epoch 00198: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9956 - val_loss: 0.0117 - val_acc: 0.9876\n",
      "Epoch 199/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9956Epoch 00199: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9958 - val_loss: 0.0105 - val_acc: 0.9880\n",
      "Epoch 200/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9962Epoch 00200: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9962 - val_loss: 0.0058 - val_acc: 0.9919\n",
      "Epoch 201/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9964Epoch 00201: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9966 - val_loss: 0.0038 - val_acc: 0.9947\n",
      "Epoch 202/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - acc: 0.9954Epoch 00202: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9956 - val_loss: 0.0067 - val_acc: 0.9906\n",
      "Epoch 203/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9965Epoch 00203: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9967 - val_loss: 0.0054 - val_acc: 0.9924\n",
      "Epoch 204/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9967Epoch 00204: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9969 - val_loss: 0.0041 - val_acc: 0.9944\n",
      "Epoch 205/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9961Epoch 00205: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9961 - val_loss: 0.0061 - val_acc: 0.9918\n",
      "Epoch 206/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - acc: 0.9953Epoch 00206: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9955 - val_loss: 0.0070 - val_acc: 0.9906\n",
      "Epoch 207/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9957Epoch 00207: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9960 - val_loss: 0.0101 - val_acc: 0.9880\n",
      "Epoch 208/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9948Epoch 00208: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9950 - val_loss: 0.0101 - val_acc: 0.9878\n",
      "Epoch 209/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9964Epoch 00209: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9965 - val_loss: 0.0143 - val_acc: 0.9856\n",
      "Epoch 210/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9958Epoch 00210: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0035 - acc: 0.9951 - val_loss: 0.0126 - val_acc: 0.9868\n",
      "Epoch 211/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9950Epoch 00211: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0036 - acc: 0.9950 - val_loss: 0.0086 - val_acc: 0.9897\n",
      "Epoch 212/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9945Epoch 00212: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0039 - acc: 0.9946 - val_loss: 0.0087 - val_acc: 0.9882\n",
      "Epoch 213/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9959Epoch 00213: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9957 - val_loss: 0.0091 - val_acc: 0.9881\n",
      "Epoch 214/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9965Epoch 00214: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9960 - val_loss: 0.0059 - val_acc: 0.9924\n",
      "Epoch 215/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9945Epoch 00215: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9943 - val_loss: 0.0049 - val_acc: 0.9928\n",
      "Epoch 216/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9954Epoch 00216: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9950 - val_loss: 0.0054 - val_acc: 0.9927\n",
      "Epoch 217/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - acc: 0.9952Epoch 00217: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9955 - val_loss: 0.0040 - val_acc: 0.9947\n",
      "Epoch 218/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9959Epoch 00218: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9959 - val_loss: 0.0060 - val_acc: 0.9914\n",
      "Epoch 219/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9970Epoch 00219: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9972 - val_loss: 0.0084 - val_acc: 0.9896\n",
      "Epoch 220/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9968Epoch 00220: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9970 - val_loss: 0.0062 - val_acc: 0.9917\n",
      "Epoch 221/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9959Epoch 00221: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9961 - val_loss: 0.0061 - val_acc: 0.9921\n",
      "Epoch 222/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9958Epoch 00222: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9957 - val_loss: 0.0085 - val_acc: 0.9897\n",
      "Epoch 223/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9949Epoch 00223: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0036 - acc: 0.9951 - val_loss: 0.0091 - val_acc: 0.9887\n",
      "Epoch 224/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9961Epoch 00224: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9963 - val_loss: 0.0097 - val_acc: 0.9887\n",
      "Epoch 225/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9964Epoch 00225: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9966 - val_loss: 0.0091 - val_acc: 0.9889\n",
      "Epoch 226/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9961Epoch 00226: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9961 - val_loss: 0.0075 - val_acc: 0.9897\n",
      "Epoch 227/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - acc: 0.9955Epoch 00227: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9955 - val_loss: 0.0079 - val_acc: 0.9895\n",
      "Epoch 228/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00228: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9973 - val_loss: 0.0116 - val_acc: 0.9869\n",
      "Epoch 229/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9947Epoch 00229: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0038 - acc: 0.9947 - val_loss: 0.0069 - val_acc: 0.9907\n",
      "Epoch 230/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9961Epoch 00230: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9963 - val_loss: 0.0084 - val_acc: 0.9892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9956Epoch 00231: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0038 - acc: 0.9949 - val_loss: 0.0064 - val_acc: 0.9919\n",
      "Epoch 232/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9953Epoch 00232: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9955 - val_loss: 0.0063 - val_acc: 0.9912\n",
      "Epoch 233/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9965Epoch 00233: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0054 - val_acc: 0.9920\n",
      "Epoch 234/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9963Epoch 00234: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0028 - acc: 0.9962 - val_loss: 0.0039 - val_acc: 0.9950\n",
      "Epoch 235/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9953Epoch 00235: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9956 - val_loss: 0.0067 - val_acc: 0.9913\n",
      "Epoch 236/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9956Epoch 00236: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9949 - val_loss: 0.0055 - val_acc: 0.9926\n",
      "Epoch 237/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9972Epoch 00237: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0059 - val_acc: 0.9920\n",
      "Epoch 238/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9965Epoch 00238: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9967 - val_loss: 0.0054 - val_acc: 0.9925\n",
      "Epoch 239/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9959Epoch 00239: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9961 - val_loss: 0.0070 - val_acc: 0.9904\n",
      "Epoch 240/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9963Epoch 00240: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9966 - val_loss: 0.0082 - val_acc: 0.9883\n",
      "Epoch 241/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9969Epoch 00241: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9968 - val_loss: 0.0076 - val_acc: 0.9894\n",
      "Epoch 242/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9955Epoch 00242: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0036 - acc: 0.9952 - val_loss: 0.0056 - val_acc: 0.9915\n",
      "Epoch 243/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9946Epoch 00243: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0041 - acc: 0.9946 - val_loss: 0.0071 - val_acc: 0.9907\n",
      "Epoch 244/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9956Epoch 00244: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0034 - acc: 0.9953 - val_loss: 0.0065 - val_acc: 0.9918\n",
      "Epoch 245/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9960Epoch 00245: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9959 - val_loss: 0.0099 - val_acc: 0.9884\n",
      "Epoch 246/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9967Epoch 00246: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9966 - val_loss: 0.0049 - val_acc: 0.9937\n",
      "Epoch 247/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9964Epoch 00247: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9964 - val_loss: 0.0081 - val_acc: 0.9902\n",
      "Epoch 248/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9965Epoch 00248: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9965 - val_loss: 0.0048 - val_acc: 0.9937\n",
      "Epoch 249/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9960Epoch 00249: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9958 - val_loss: 0.0040 - val_acc: 0.9948\n",
      "Epoch 250/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9962Epoch 00250: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9964 - val_loss: 0.0048 - val_acc: 0.9938\n",
      "Epoch 251/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9961Epoch 00251: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9962 - val_loss: 0.0048 - val_acc: 0.9938\n",
      "Epoch 252/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9964Epoch 00252: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9965 - val_loss: 0.0054 - val_acc: 0.9935\n",
      "Epoch 253/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9969Epoch 00253: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9964 - val_loss: 0.0048 - val_acc: 0.9937\n",
      "Epoch 254/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9960Epoch 00254: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9960 - val_loss: 0.0058 - val_acc: 0.9921\n",
      "Epoch 255/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9969Epoch 00255: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9969 - val_loss: 0.0052 - val_acc: 0.9931\n",
      "Epoch 256/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00256: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0045 - val_acc: 0.9939\n",
      "Epoch 257/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9960Epoch 00257: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9960 - val_loss: 0.0053 - val_acc: 0.9927\n",
      "Epoch 258/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9978Epoch 00258: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9974 - val_loss: 0.0066 - val_acc: 0.9912\n",
      "Epoch 259/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9971Epoch 00259: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9969 - val_loss: 0.0054 - val_acc: 0.9928\n",
      "Epoch 260/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9972Epoch 00260: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9970 - val_loss: 0.0042 - val_acc: 0.9944\n",
      "Epoch 261/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9973Epoch 00261: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0079 - val_acc: 0.9911\n",
      "Epoch 262/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9961Epoch 00262: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9960 - val_loss: 0.0055 - val_acc: 0.9925\n",
      "Epoch 263/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - acc: 0.9954Epoch 00263: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9957 - val_loss: 0.0064 - val_acc: 0.9913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 264/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9965Epoch 00264: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9965 - val_loss: 0.0073 - val_acc: 0.9908\n",
      "Epoch 265/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9959Epoch 00265: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9960 - val_loss: 0.0117 - val_acc: 0.9868\n",
      "Epoch 266/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9957Epoch 00266: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9957 - val_loss: 0.0141 - val_acc: 0.9857\n",
      "Epoch 267/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9950Epoch 00267: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0039 - acc: 0.9948 - val_loss: 0.0137 - val_acc: 0.9863\n",
      "Epoch 268/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9948Epoch 00268: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0038 - acc: 0.9948 - val_loss: 0.0095 - val_acc: 0.9888\n",
      "Epoch 269/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9967Epoch 00269: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9967 - val_loss: 0.0103 - val_acc: 0.9884\n",
      "Epoch 270/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9957Epoch 00270: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9959 - val_loss: 0.0088 - val_acc: 0.9887\n",
      "Epoch 271/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9962Epoch 00271: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0028 - acc: 0.9964 - val_loss: 0.0117 - val_acc: 0.9882\n",
      "Epoch 272/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9966Epoch 00272: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9967 - val_loss: 0.0062 - val_acc: 0.9918\n",
      "Epoch 273/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9973Epoch 00273: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9972 - val_loss: 0.0091 - val_acc: 0.9888\n",
      "Epoch 274/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9966Epoch 00274: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9967 - val_loss: 0.0058 - val_acc: 0.9921\n",
      "Epoch 275/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9957Epoch 00275: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9958 - val_loss: 0.0075 - val_acc: 0.9906\n",
      "Epoch 276/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9956Epoch 00276: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9958 - val_loss: 0.0075 - val_acc: 0.9898\n",
      "Epoch 277/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9970Epoch 00277: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9968 - val_loss: 0.0085 - val_acc: 0.9887\n",
      "Epoch 278/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9973Epoch 00278: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0060 - val_acc: 0.9920\n",
      "Epoch 279/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00279: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9972 - val_loss: 0.0113 - val_acc: 0.9871\n",
      "Epoch 280/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00280: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0118 - val_acc: 0.9868\n",
      "Epoch 281/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9970Epoch 00281: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9966 - val_loss: 0.0101 - val_acc: 0.9881\n",
      "Epoch 282/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00282: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0121 - val_acc: 0.9879\n",
      "Epoch 283/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9947Epoch 00283: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0038 - acc: 0.9949 - val_loss: 0.0107 - val_acc: 0.9876\n",
      "Epoch 284/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9959Epoch 00284: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9958 - val_loss: 0.0097 - val_acc: 0.9877\n",
      "Epoch 285/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00285: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0096 - val_acc: 0.9872\n",
      "Epoch 286/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9962Epoch 00286: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0028 - acc: 0.9963 - val_loss: 0.0089 - val_acc: 0.9882\n",
      "Epoch 287/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9975Epoch 00287: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9974 - val_loss: 0.0087 - val_acc: 0.9896\n",
      "Epoch 288/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9956Epoch 00288: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9957 - val_loss: 0.0107 - val_acc: 0.9883\n",
      "Epoch 289/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9953Epoch 00289: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9950 - val_loss: 0.0085 - val_acc: 0.9894\n",
      "Epoch 290/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9952Epoch 00290: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0040 - acc: 0.9947 - val_loss: 0.0083 - val_acc: 0.9896\n",
      "Epoch 291/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9963Epoch 00291: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9964 - val_loss: 0.0058 - val_acc: 0.9926\n",
      "Epoch 292/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9962Epoch 00292: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9963 - val_loss: 0.0072 - val_acc: 0.9906\n",
      "Epoch 293/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9973Epoch 00293: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9971 - val_loss: 0.0049 - val_acc: 0.9937\n",
      "Epoch 294/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9965Epoch 00294: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9966 - val_loss: 0.0077 - val_acc: 0.9896\n",
      "Epoch 295/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9960Epoch 00295: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0028 - acc: 0.9962 - val_loss: 0.0067 - val_acc: 0.9912\n",
      "Epoch 296/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9977Epoch 00296: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0070 - val_acc: 0.9906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00297: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9977 - val_loss: 0.0049 - val_acc: 0.9937\n",
      "Epoch 298/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9969Epoch 00298: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9969 - val_loss: 0.0078 - val_acc: 0.9894\n",
      "Epoch 299/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9970Epoch 00299: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9967 - val_loss: 0.0085 - val_acc: 0.9891\n",
      "Epoch 300/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00300: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0063 - val_acc: 0.9915\n",
      "Epoch 301/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00301: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0086 - val_acc: 0.9892\n",
      "Epoch 302/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00302: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0117 - val_acc: 0.9866\n",
      "Epoch 303/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9962Epoch 00303: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0028 - acc: 0.9963 - val_loss: 0.0079 - val_acc: 0.9901\n",
      "Epoch 304/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00304: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9972 - val_loss: 0.0066 - val_acc: 0.9912\n",
      "Epoch 305/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00305: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0060 - val_acc: 0.9921\n",
      "Epoch 306/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9968Epoch 00306: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9968 - val_loss: 0.0096 - val_acc: 0.9886\n",
      "Epoch 307/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9969Epoch 00307: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0065 - val_acc: 0.9915\n",
      "Epoch 308/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00308: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0065 - val_acc: 0.9910\n",
      "Epoch 309/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9970Epoch 00309: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9971 - val_loss: 0.0092 - val_acc: 0.9891\n",
      "Epoch 310/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9974Epoch 00310: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0069 - val_acc: 0.9906\n",
      "Epoch 311/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9968Epoch 00311: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9968 - val_loss: 0.0043 - val_acc: 0.9942\n",
      "Epoch 312/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9961Epoch 00312: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9957 - val_loss: 0.0074 - val_acc: 0.9903\n",
      "Epoch 313/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9963Epoch 00313: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0028 - acc: 0.9965 - val_loss: 0.0121 - val_acc: 0.9828\n",
      "Epoch 314/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9958Epoch 00314: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9956 - val_loss: 0.0076 - val_acc: 0.9894\n",
      "Epoch 315/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9966Epoch 00315: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9965 - val_loss: 0.0048 - val_acc: 0.9939\n",
      "Epoch 316/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9956Epoch 00316: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9955 - val_loss: 0.0050 - val_acc: 0.9935\n",
      "Epoch 317/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9969Epoch 00317: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9970 - val_loss: 0.0046 - val_acc: 0.9942\n",
      "Epoch 318/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9964Epoch 00318: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0063 - val_acc: 0.9918\n",
      "Epoch 319/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9954Epoch 00319: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0033 - acc: 0.9957 - val_loss: 0.0073 - val_acc: 0.9909\n",
      "Epoch 320/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9960Epoch 00320: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9960 - val_loss: 0.0048 - val_acc: 0.9937\n",
      "Epoch 321/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9970Epoch 00321: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9971 - val_loss: 0.0066 - val_acc: 0.9921\n",
      "Epoch 322/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9974Epoch 00322: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9973 - val_loss: 0.0072 - val_acc: 0.9912\n",
      "Epoch 323/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9961Epoch 00323: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9959 - val_loss: 0.0053 - val_acc: 0.9931\n",
      "Epoch 324/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9973Epoch 00324: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0148 - val_acc: 0.9851\n",
      "Epoch 325/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9952Epoch 00325: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0034 - acc: 0.9953 - val_loss: 0.0070 - val_acc: 0.9906\n",
      "Epoch 326/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9960Epoch 00326: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9959 - val_loss: 0.0087 - val_acc: 0.9897\n",
      "Epoch 327/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9980Epoch 00327: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9977 - val_loss: 0.0124 - val_acc: 0.9859\n",
      "Epoch 328/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9964Epoch 00328: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0064 - val_acc: 0.9911\n",
      "Epoch 329/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9975Epoch 00329: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9976 - val_loss: 0.0084 - val_acc: 0.9895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9973Epoch 00330: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0119 - val_acc: 0.9876\n",
      "Epoch 331/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00331: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9977 - val_loss: 0.0090 - val_acc: 0.9897\n",
      "Epoch 332/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9967Epoch 00332: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9967 - val_loss: 0.0100 - val_acc: 0.9881\n",
      "Epoch 333/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9970Epoch 00333: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9969 - val_loss: 0.0085 - val_acc: 0.9892\n",
      "Epoch 334/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9975Epoch 00334: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9974 - val_loss: 0.0117 - val_acc: 0.9870\n",
      "Epoch 335/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9962Epoch 00335: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0028 - acc: 0.9964 - val_loss: 0.0090 - val_acc: 0.9895\n",
      "Epoch 336/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9967Epoch 00336: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9968 - val_loss: 0.0059 - val_acc: 0.9928\n",
      "Epoch 337/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00337: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9973 - val_loss: 0.0125 - val_acc: 0.9870\n",
      "Epoch 338/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9978Epoch 00338: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9978 - val_loss: 0.0145 - val_acc: 0.9853\n",
      "Epoch 339/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9975Epoch 00339: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0111 - val_acc: 0.9881\n",
      "Epoch 340/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9975Epoch 00340: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9975 - val_loss: 0.0081 - val_acc: 0.9909\n",
      "Epoch 341/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9967Epoch 00341: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9968 - val_loss: 0.0079 - val_acc: 0.9903\n",
      "Epoch 342/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9958Epoch 00342: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0032 - acc: 0.9958 - val_loss: 0.0087 - val_acc: 0.9893\n",
      "Epoch 343/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9969Epoch 00343: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9968 - val_loss: 0.0078 - val_acc: 0.9902\n",
      "Epoch 344/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9965Epoch 00344: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0062 - val_acc: 0.9921\n",
      "Epoch 345/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00345: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9970 - val_loss: 0.0086 - val_acc: 0.9887\n",
      "Epoch 346/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00346: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0091 - val_acc: 0.9885\n",
      "Epoch 347/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9972Epoch 00347: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9972 - val_loss: 0.0092 - val_acc: 0.9893\n",
      "Epoch 348/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9964Epoch 00348: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9967 - val_loss: 0.0107 - val_acc: 0.9893\n",
      "Epoch 349/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9965Epoch 00349: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9964 - val_loss: 0.0120 - val_acc: 0.9859\n",
      "Epoch 350/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00350: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0083 - val_acc: 0.9891\n",
      "Epoch 351/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9967Epoch 00351: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9967 - val_loss: 0.0097 - val_acc: 0.9883\n",
      "Epoch 352/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00352: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0093 - val_acc: 0.9886\n",
      "Epoch 353/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9973Epoch 00353: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9974 - val_loss: 0.0080 - val_acc: 0.9902\n",
      "Epoch 354/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00354: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9972 - val_loss: 0.0058 - val_acc: 0.9926\n",
      "Epoch 355/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00355: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0067 - val_acc: 0.9916\n",
      "Epoch 356/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9973Epoch 00356: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0098 - val_acc: 0.9873\n",
      "Epoch 357/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9969Epoch 00357: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9969 - val_loss: 0.0079 - val_acc: 0.9890\n",
      "Epoch 358/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00358: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9972 - val_loss: 0.0107 - val_acc: 0.9851\n",
      "Epoch 359/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9959Epoch 00359: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0029 - acc: 0.9961 - val_loss: 0.0198 - val_acc: 0.9742\n",
      "Epoch 360/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00360: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9976 - val_loss: 0.0123 - val_acc: 0.9823\n",
      "Epoch 361/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9981Epoch 00361: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9981 - val_loss: 0.0081 - val_acc: 0.9900\n",
      "Epoch 362/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9973Epoch 00362: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9973 - val_loss: 0.0064 - val_acc: 0.9916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00363: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0065 - val_acc: 0.9915\n",
      "Epoch 364/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9978Epoch 00364: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9978 - val_loss: 0.0083 - val_acc: 0.9896\n",
      "Epoch 365/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9971Epoch 00365: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9972 - val_loss: 0.0087 - val_acc: 0.9896\n",
      "Epoch 366/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9968Epoch 00366: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9968 - val_loss: 0.0062 - val_acc: 0.9921\n",
      "Epoch 367/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00367: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9978 - val_loss: 0.0074 - val_acc: 0.9908\n",
      "Epoch 368/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9967Epoch 00368: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9968 - val_loss: 0.0067 - val_acc: 0.9917\n",
      "Epoch 369/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9972Epoch 00369: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9974 - val_loss: 0.0064 - val_acc: 0.9921\n",
      "Epoch 370/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9969Epoch 00370: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9969 - val_loss: 0.0092 - val_acc: 0.9890\n",
      "Epoch 371/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9972Epoch 00371: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9972 - val_loss: 0.0074 - val_acc: 0.9909\n",
      "Epoch 372/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9969Epoch 00372: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0098 - val_acc: 0.9882\n",
      "Epoch 373/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9966Epoch 00373: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9965 - val_loss: 0.0096 - val_acc: 0.9891\n",
      "Epoch 374/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9973Epoch 00374: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0087 - val_acc: 0.9898\n",
      "Epoch 375/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9975Epoch 00375: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9977 - val_loss: 0.0088 - val_acc: 0.9901\n",
      "Epoch 376/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00376: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0078 - val_acc: 0.9905\n",
      "Epoch 377/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00377: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0059 - val_acc: 0.9927\n",
      "Epoch 378/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9973Epoch 00378: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9972 - val_loss: 0.0056 - val_acc: 0.9928\n",
      "Epoch 379/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00379: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9974 - val_loss: 0.0080 - val_acc: 0.9904\n",
      "Epoch 380/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9970Epoch 00380: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0049 - val_acc: 0.9938\n",
      "Epoch 381/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00381: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9973 - val_loss: 0.0069 - val_acc: 0.9912\n",
      "Epoch 382/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9974Epoch 00382: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9975 - val_loss: 0.0045 - val_acc: 0.9941\n",
      "Epoch 383/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00383: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9976 - val_loss: 0.0064 - val_acc: 0.9918\n",
      "Epoch 384/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9974Epoch 00384: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0063 - val_acc: 0.9923\n",
      "Epoch 385/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00385: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0050 - val_acc: 0.9935\n",
      "Epoch 386/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00386: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0069 - val_acc: 0.9913\n",
      "Epoch 387/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00387: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0072 - val_acc: 0.9907\n",
      "Epoch 388/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00388: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0082 - val_acc: 0.9898\n",
      "Epoch 389/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00389: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9979 - val_loss: 0.0071 - val_acc: 0.9906\n",
      "Epoch 390/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00390: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0077 - val_acc: 0.9905\n",
      "Epoch 391/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9977Epoch 00391: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9977 - val_loss: 0.0075 - val_acc: 0.9906\n",
      "Epoch 392/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00392: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0071 - val_acc: 0.9913\n",
      "Epoch 393/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9966Epoch 00393: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0030 - acc: 0.9962 - val_loss: 0.0089 - val_acc: 0.9897\n",
      "Epoch 394/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9974Epoch 00394: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0096 - val_acc: 0.9886\n",
      "Epoch 395/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9981Epoch 00395: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9982 - val_loss: 0.0110 - val_acc: 0.9873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00396: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0065 - val_acc: 0.9919\n",
      "Epoch 397/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9966Epoch 00397: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9967 - val_loss: 0.0089 - val_acc: 0.9887\n",
      "Epoch 398/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00398: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0094 - val_acc: 0.9885\n",
      "Epoch 399/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00399: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0073 - val_acc: 0.9910\n",
      "Epoch 400/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9974Epoch 00400: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9973 - val_loss: 0.0070 - val_acc: 0.9918\n",
      "Epoch 401/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9979Epoch 00401: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9978 - val_loss: 0.0114 - val_acc: 0.9875\n",
      "Epoch 402/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00402: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0100 - val_acc: 0.9881\n",
      "Epoch 403/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00403: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0135 - val_acc: 0.9863\n",
      "Epoch 404/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00404: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0120 - val_acc: 0.9877\n",
      "Epoch 405/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00405: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9972 - val_loss: 0.0076 - val_acc: 0.9911\n",
      "Epoch 406/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00406: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0087 - val_acc: 0.9898\n",
      "Epoch 407/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00407: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0096 - val_acc: 0.9888\n",
      "Epoch 408/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00408: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0099 - val_acc: 0.9890\n",
      "Epoch 409/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9977Epoch 00409: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0083 - val_acc: 0.9903\n",
      "Epoch 410/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00410: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9972 - val_loss: 0.0114 - val_acc: 0.9869\n",
      "Epoch 411/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9967Epoch 00411: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9969 - val_loss: 0.0115 - val_acc: 0.9871\n",
      "Epoch 412/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9965Epoch 00412: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0026 - acc: 0.9965 - val_loss: 0.0134 - val_acc: 0.9865\n",
      "Epoch 413/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9964Epoch 00413: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9966 - val_loss: 0.0126 - val_acc: 0.9840\n",
      "Epoch 414/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9962Epoch 00414: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0031 - acc: 0.9961 - val_loss: 0.0066 - val_acc: 0.9920\n",
      "Epoch 415/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9964Epoch 00415: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0028 - acc: 0.9963 - val_loss: 0.0069 - val_acc: 0.9914\n",
      "Epoch 416/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00416: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9977 - val_loss: 0.0071 - val_acc: 0.9915\n",
      "Epoch 417/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00417: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0090 - val_acc: 0.9894\n",
      "Epoch 418/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00418: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0109 - val_acc: 0.9879\n",
      "Epoch 419/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00419: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0105 - val_acc: 0.9882\n",
      "Epoch 420/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9972Epoch 00420: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9974 - val_loss: 0.0073 - val_acc: 0.9910\n",
      "Epoch 421/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9973Epoch 00421: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0072 - val_acc: 0.9910\n",
      "Epoch 422/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00422: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0058 - val_acc: 0.9927\n",
      "Epoch 423/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9979Epoch 00423: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0065 - val_acc: 0.9921\n",
      "Epoch 424/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9975Epoch 00424: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9974 - val_loss: 0.0084 - val_acc: 0.9898\n",
      "Epoch 425/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9972Epoch 00425: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9974 - val_loss: 0.0095 - val_acc: 0.9884\n",
      "Epoch 426/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00426: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0075 - val_acc: 0.9909\n",
      "Epoch 427/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9979Epoch 00427: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0083 - val_acc: 0.9899\n",
      "Epoch 428/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00428: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0087 - val_acc: 0.9895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00429: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9982 - val_loss: 0.0078 - val_acc: 0.9906\n",
      "Epoch 430/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00430: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0054 - val_acc: 0.9934\n",
      "Epoch 431/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9968Epoch 00431: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9970 - val_loss: 0.0088 - val_acc: 0.9900\n",
      "Epoch 432/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00432: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0119 - val_acc: 0.9868\n",
      "Epoch 433/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00433: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0076 - val_acc: 0.9911\n",
      "Epoch 434/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00434: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0084 - val_acc: 0.9902\n",
      "Epoch 435/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00435: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0065 - val_acc: 0.9919\n",
      "Epoch 436/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9970Epoch 00436: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0085 - val_acc: 0.9891\n",
      "Epoch 437/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00437: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0126 - val_acc: 0.9858\n",
      "Epoch 438/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00438: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0065 - val_acc: 0.9921\n",
      "Epoch 439/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00439: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9978 - val_loss: 0.0063 - val_acc: 0.9922\n",
      "Epoch 440/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00440: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0084 - val_acc: 0.9899\n",
      "Epoch 441/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9980Epoch 00441: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9980 - val_loss: 0.0092 - val_acc: 0.9890\n",
      "Epoch 442/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9984Epoch 00442: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0093 - val_acc: 0.9889\n",
      "Epoch 443/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00443: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0077 - val_acc: 0.9908\n",
      "Epoch 444/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9974Epoch 00444: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9974 - val_loss: 0.0071 - val_acc: 0.9918\n",
      "Epoch 445/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00445: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0048 - val_acc: 0.9940\n",
      "Epoch 446/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9974Epoch 00446: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0053 - val_acc: 0.9929\n",
      "Epoch 447/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00447: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9980 - val_loss: 0.0085 - val_acc: 0.9899\n",
      "Epoch 448/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00448: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0073 - val_acc: 0.9915\n",
      "Epoch 449/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00449: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9978 - val_loss: 0.0122 - val_acc: 0.9861\n",
      "Epoch 450/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00450: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0076 - val_acc: 0.9910\n",
      "Epoch 451/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00451: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0061 - val_acc: 0.9927\n",
      "Epoch 452/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00452: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0088 - val_acc: 0.9891\n",
      "Epoch 453/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9983Epoch 00453: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0088 - val_acc: 0.9890\n",
      "Epoch 454/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00454: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0087 - val_acc: 0.9893\n",
      "Epoch 455/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00455: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0097 - val_acc: 0.9883\n",
      "Epoch 456/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00456: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0107 - val_acc: 0.9876\n",
      "Epoch 457/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00457: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9980 - val_loss: 0.0063 - val_acc: 0.9925\n",
      "Epoch 458/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00458: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0092 - val_acc: 0.9891\n",
      "Epoch 459/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9979Epoch 00459: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9979 - val_loss: 0.0100 - val_acc: 0.9882\n",
      "Epoch 460/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9985 Epoch 00460: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0102 - val_acc: 0.9883\n",
      "Epoch 461/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9977Epoch 00461: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0085 - val_acc: 0.9903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9982Epoch 00462: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9983 - val_loss: 0.0096 - val_acc: 0.9888\n",
      "Epoch 463/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9982Epoch 00463: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0119 - val_acc: 0.9867\n",
      "Epoch 464/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00464: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0102 - val_acc: 0.9883\n",
      "Epoch 465/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00465: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0110 - val_acc: 0.9873\n",
      "Epoch 466/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00466: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0105 - val_acc: 0.9875\n",
      "Epoch 467/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00467: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0104 - val_acc: 0.9877\n",
      "Epoch 468/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.7746e-04 - acc: 0.9987Epoch 00468: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0101 - val_acc: 0.9885\n",
      "Epoch 469/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00469: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0100 - val_acc: 0.9882\n",
      "Epoch 470/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9978Epoch 00470: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0102 - val_acc: 0.9877\n",
      "Epoch 471/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9986    Epoch 00471: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0098 - val_acc: 0.9883\n",
      "Epoch 472/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00472: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0087 - val_acc: 0.9892\n",
      "Epoch 473/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9983Epoch 00473: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0079 - val_acc: 0.9905\n",
      "Epoch 474/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00474: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0101 - val_acc: 0.9884\n",
      "Epoch 475/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987   Epoch 00475: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0101 - val_acc: 0.9886\n",
      "Epoch 476/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00476: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0096 - val_acc: 0.9887\n",
      "Epoch 477/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.5901e-04 - acc: 0.9987Epoch 00477: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0106 - val_acc: 0.9874\n",
      "Epoch 478/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00478: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9980 - val_loss: 0.0092 - val_acc: 0.9889\n",
      "Epoch 479/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986 Epoch 00479: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0101 - val_acc: 0.9885\n",
      "Epoch 480/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00480: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0122 - val_acc: 0.9867\n",
      "Epoch 481/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00481: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0101 - val_acc: 0.9883\n",
      "Epoch 482/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00482: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0088 - val_acc: 0.9892\n",
      "Epoch 483/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9978Epoch 00483: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9978 - val_loss: 0.0133 - val_acc: 0.9838\n",
      "Epoch 484/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9979Epoch 00484: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0104 - val_acc: 0.9876\n",
      "Epoch 485/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00485: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9982 - val_loss: 0.0091 - val_acc: 0.9894\n",
      "Epoch 486/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984  Epoch 00486: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0082 - val_acc: 0.9901\n",
      "Epoch 487/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9974Epoch 00487: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0077 - val_acc: 0.9909\n",
      "Epoch 488/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00488: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0065 - val_acc: 0.9922\n",
      "Epoch 489/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00489: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0067 - val_acc: 0.9917\n",
      "Epoch 490/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9978Epoch 00490: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0080 - val_acc: 0.9903\n",
      "Epoch 491/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9977Epoch 00491: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9977 - val_loss: 0.0087 - val_acc: 0.9885\n",
      "Epoch 492/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00492: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0062 - val_acc: 0.9919\n",
      "Epoch 493/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9983Epoch 00493: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9981 - val_loss: 0.0098 - val_acc: 0.9878\n",
      "Epoch 494/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00494: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9983 - val_loss: 0.0069 - val_acc: 0.9921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 495/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00495: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0075 - val_acc: 0.9910\n",
      "Epoch 496/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9982Epoch 00496: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9981 - val_loss: 0.0134 - val_acc: 0.9832\n",
      "Epoch 497/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9980Epoch 00497: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0082 - val_acc: 0.9903\n",
      "Epoch 498/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00498: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0067 - val_acc: 0.9920\n",
      "Epoch 499/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00499: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9980 - val_loss: 0.0080 - val_acc: 0.9908\n",
      "Epoch 500/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00500: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9982 - val_loss: 0.0086 - val_acc: 0.9898\n",
      "Epoch 501/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00501: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0087 - val_acc: 0.9898\n",
      "Epoch 502/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00502: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9986 - val_loss: 0.0094 - val_acc: 0.9892\n",
      "Epoch 503/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00503: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0087 - val_acc: 0.9898\n",
      "Epoch 504/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00504: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9975 - val_loss: 0.0094 - val_acc: 0.9887\n",
      "Epoch 505/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9973Epoch 00505: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9970 - val_loss: 0.0069 - val_acc: 0.9915\n",
      "Epoch 506/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9984Epoch 00506: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9984 - val_loss: 0.0086 - val_acc: 0.9905\n",
      "Epoch 507/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983 Epoch 00507: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0106 - val_acc: 0.9883\n",
      "Epoch 508/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00508: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0081 - val_acc: 0.9906\n",
      "Epoch 509/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00509: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9909\n",
      "Epoch 510/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00510: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0100 - val_acc: 0.9885\n",
      "Epoch 511/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00511: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0085 - val_acc: 0.9901\n",
      "Epoch 512/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.7743e-04 - acc: 0.9988Epoch 00512: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.8908e-04 - acc: 0.9987 - val_loss: 0.0097 - val_acc: 0.9888\n",
      "Epoch 513/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00513: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0092 - val_acc: 0.9891\n",
      "Epoch 514/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9979Epoch 00514: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9980 - val_loss: 0.0111 - val_acc: 0.9871\n",
      "Epoch 515/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980  Epoch 00515: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0089 - val_acc: 0.9889\n",
      "Epoch 516/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00516: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0121 - val_acc: 0.9860\n",
      "Epoch 517/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9983Epoch 00517: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0105 - val_acc: 0.9882\n",
      "Epoch 518/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00518: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0101 - val_acc: 0.9885\n",
      "Epoch 519/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00519: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0097 - val_acc: 0.9893\n",
      "Epoch 520/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.3597e-04 - acc: 0.9990Epoch 00520: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.6926e-04 - acc: 0.9988 - val_loss: 0.0100 - val_acc: 0.9890\n",
      "Epoch 521/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9987Epoch 00521: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0112 - val_acc: 0.9877\n",
      "Epoch 522/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9987Epoch 00522: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0097 - val_acc: 0.9886\n",
      "Epoch 523/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00523: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0114 - val_acc: 0.9873\n",
      "Epoch 524/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.7163e-04 - acc: 0.9987Epoch 00524: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.8757e-04 - acc: 0.9987 - val_loss: 0.0091 - val_acc: 0.9893\n",
      "Epoch 525/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.5449e-04 - acc: 0.9987Epoch 00525: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.3768e-04 - acc: 0.9988 - val_loss: 0.0089 - val_acc: 0.9896\n",
      "Epoch 526/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986   Epoch 00526: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0101 - val_acc: 0.9880\n",
      "Epoch 527/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.7744e-04 - acc: 0.9989Epoch 00527: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.0452e-04 - acc: 0.9988 - val_loss: 0.0100 - val_acc: 0.9885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 528/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987    Epoch 00528: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0104 - val_acc: 0.9876\n",
      "Epoch 529/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9985Epoch 00529: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0123 - val_acc: 0.9861\n",
      "Epoch 530/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.6395e-04 - acc: 0.9987Epoch 00530: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0110 - val_acc: 0.9876\n",
      "Epoch 531/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00531: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0126 - val_acc: 0.9858\n",
      "Epoch 532/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.2157e-04 - acc: 0.9988Epoch 00532: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.5423e-04 - acc: 0.9987 - val_loss: 0.0109 - val_acc: 0.9872\n",
      "Epoch 533/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.2557e-04 - acc: 0.9988Epoch 00533: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.2776e-04 - acc: 0.9988 - val_loss: 0.0108 - val_acc: 0.9878\n",
      "Epoch 534/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.2615e-04 - acc: 0.9988Epoch 00534: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.7877e-04 - acc: 0.9987 - val_loss: 0.0104 - val_acc: 0.9880\n",
      "Epoch 535/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9986    Epoch 00535: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9986 - val_loss: 0.0112 - val_acc: 0.9874\n",
      "Epoch 536/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.6838e-04 - acc: 0.9988Epoch 00536: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.1693e-04 - acc: 0.9988 - val_loss: 0.0115 - val_acc: 0.9869\n",
      "Epoch 537/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.6274e-04 - acc: 0.9990Epoch 00537: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 7.5245e-04 - acc: 0.9990 - val_loss: 0.0110 - val_acc: 0.9872\n",
      "Epoch 538/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.6133e-04 - acc: 0.9989Epoch 00538: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.6773e-04 - acc: 0.9987 - val_loss: 0.0121 - val_acc: 0.9862\n",
      "Epoch 539/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.0550e-04 - acc: 0.9988Epoch 00539: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.9780e-04 - acc: 0.9987 - val_loss: 0.0119 - val_acc: 0.9861\n",
      "Epoch 540/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9987 Epoch 00540: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0113 - val_acc: 0.9872\n",
      "Epoch 541/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.4494e-04 - acc: 0.9989Epoch 00541: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.2916e-04 - acc: 0.9988 - val_loss: 0.0083 - val_acc: 0.9902\n",
      "Epoch 542/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.1180e-04 - acc: 0.9990Epoch 00542: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 8.8774e-04 - acc: 0.9989 - val_loss: 0.0112 - val_acc: 0.9871\n",
      "Epoch 543/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00543: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0093 - val_acc: 0.9890\n",
      "Epoch 544/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.8170e-04 - acc: 0.9989Epoch 00544: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 8.8497e-04 - acc: 0.9989 - val_loss: 0.0104 - val_acc: 0.9879\n",
      "Epoch 545/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00545: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0097 - val_acc: 0.9887\n",
      "Epoch 546/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.1468e-04 - acc: 0.9989Epoch 00546: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 7.9988e-04 - acc: 0.9990 - val_loss: 0.0109 - val_acc: 0.9874\n",
      "Epoch 547/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.5717e-04 - acc: 0.9989Epoch 00547: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 8.4273e-04 - acc: 0.9989 - val_loss: 0.0105 - val_acc: 0.9877\n",
      "Epoch 548/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.3953e-04 - acc: 0.9988Epoch 00548: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.8963e-04 - acc: 0.9988 - val_loss: 0.0115 - val_acc: 0.9865\n",
      "Epoch 549/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.1751e-04 - acc: 0.9989Epoch 00549: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 8.2960e-04 - acc: 0.9989 - val_loss: 0.0109 - val_acc: 0.9876\n",
      "Epoch 550/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.9673e-04 - acc: 0.9987Epoch 00550: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0118 - val_acc: 0.9865\n",
      "Epoch 551/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.8754e-04 - acc: 0.9990Epoch 00551: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 7.6712e-04 - acc: 0.9990 - val_loss: 0.0103 - val_acc: 0.9885\n",
      "Epoch 552/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.5636e-04 - acc: 0.9990Epoch 00552: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 7.4802e-04 - acc: 0.9990 - val_loss: 0.0098 - val_acc: 0.9889\n",
      "Epoch 553/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.6619e-04 - acc: 0.9989Epoch 00553: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 8.5539e-04 - acc: 0.9989 - val_loss: 0.0098 - val_acc: 0.9890\n",
      "Epoch 554/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9986Epoch 00554: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.7969e-04 - acc: 0.9987 - val_loss: 0.0092 - val_acc: 0.9895\n",
      "Epoch 555/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.6180e-04 - acc: 0.9989Epoch 00555: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 8.3013e-04 - acc: 0.9989 - val_loss: 0.0108 - val_acc: 0.9877\n",
      "Epoch 556/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987    Epoch 00556: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.7114e-04 - acc: 0.9988 - val_loss: 0.0086 - val_acc: 0.9900\n",
      "Epoch 557/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.6162e-04 - acc: 0.9988Epoch 00557: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0010 - acc: 0.9986 - val_loss: 0.0099 - val_acc: 0.9888\n",
      "Epoch 558/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00558: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.8260e-04 - acc: 0.9987 - val_loss: 0.0091 - val_acc: 0.9896\n",
      "Epoch 559/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9985Epoch 00559: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0096 - val_acc: 0.9891\n",
      "Epoch 560/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.7715e-04 - acc: 0.9988Epoch 00560: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0087 - val_acc: 0.9902\n",
      "Epoch 561/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.6927e-04 - acc: 0.9988Epoch 00561: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 9.8473e-04 - acc: 0.9988 - val_loss: 0.0108 - val_acc: 0.9877\n",
      "Epoch 562/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00562: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0110 - val_acc: 0.9872\n",
      "Epoch 563/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00563: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0079 - val_acc: 0.9912\n",
      "Epoch 564/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00564: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0069 - val_acc: 0.9919\n",
      "Epoch 565/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9986Epoch 00565: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9987 - val_loss: 0.0101 - val_acc: 0.9888\n",
      "Epoch 566/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00566: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0103 - val_acc: 0.9886\n",
      "Epoch 567/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9972Epoch 00567: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9971 - val_loss: 0.0081 - val_acc: 0.9907\n",
      "Epoch 568/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00568: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0118 - val_acc: 0.9879\n",
      "Epoch 569/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00569: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0092 - val_acc: 0.9893\n",
      "Epoch 570/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9963Epoch 00570: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0027 - acc: 0.9964 - val_loss: 0.0232 - val_acc: 0.9741\n",
      "Epoch 571/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00571: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9980 - val_loss: 0.0078 - val_acc: 0.9903\n",
      "Epoch 572/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00572: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0101 - val_acc: 0.9888\n",
      "Epoch 573/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00573: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0101 - val_acc: 0.9884\n",
      "Epoch 574/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00574: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0093 - val_acc: 0.9889\n",
      "Epoch 575/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986 Epoch 00575: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0107 - val_acc: 0.9878\n",
      "Epoch 576/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9984Epoch 00576: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0107 - val_acc: 0.9878\n",
      "Epoch 577/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00577: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0092 - val_acc: 0.9893\n",
      "Epoch 578/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00578: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0088 - val_acc: 0.9897\n",
      "Epoch 579/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987   Epoch 00579: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0011 - acc: 0.9987 - val_loss: 0.0105 - val_acc: 0.9874\n",
      "Epoch 580/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976   Epoch 00580: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9978 - val_loss: 0.0075 - val_acc: 0.9910\n",
      "Epoch 581/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9974Epoch 00581: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0025 - acc: 0.9967 - val_loss: 0.0111 - val_acc: 0.9881\n",
      "Epoch 582/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9949Epoch 00582: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0037 - acc: 0.9953 - val_loss: 0.0132 - val_acc: 0.9859\n",
      "Epoch 583/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9949Epoch 00583: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0042 - acc: 0.9951 - val_loss: 0.0121 - val_acc: 0.9879\n",
      "Epoch 584/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9975Epoch 00584: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9975 - val_loss: 0.0090 - val_acc: 0.9904\n",
      "Epoch 585/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9969Epoch 00585: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0023 - acc: 0.9971 - val_loss: 0.0101 - val_acc: 0.9891\n",
      "Epoch 586/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00586: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0016 - acc: 0.9980 - val_loss: 0.0092 - val_acc: 0.9901\n",
      "Epoch 587/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00587: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0020 - acc: 0.9973 - val_loss: 0.0130 - val_acc: 0.9866\n",
      "Epoch 588/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00588: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0111 - val_acc: 0.9877\n",
      "Epoch 589/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00589: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0110 - val_acc: 0.9883\n",
      "Epoch 590/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00590: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0106 - val_acc: 0.9881\n",
      "Epoch 591/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00591: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0099 - val_acc: 0.9893\n",
      "Epoch 592/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00592: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0085 - val_acc: 0.9906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 593/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9985Epoch 00593: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0092 - val_acc: 0.9896\n",
      "Epoch 594/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00594: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0083 - val_acc: 0.9908\n",
      "Epoch 595/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9972Epoch 00595: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0024 - acc: 0.9970 - val_loss: 0.0100 - val_acc: 0.9886\n",
      "Epoch 596/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9985Epoch 00596: val_loss did not improve\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.0014 - acc: 0.9986 - val_loss: 0.0098 - val_acc: 0.9888\n",
      "Epoch 597/2000\n",
      " 4/10 [===========>..................] - ETA: 5s - loss: 0.0015 - acc: 0.9982"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-844ce3c4775c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                 callbacks=[checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2094\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2095\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss',\n",
    "#                                min_delta=0,\n",
    "#                                patience=2,\n",
    "#                                verbose=0,\n",
    "#                                mode='auto')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./save/model.weights.best.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='min') \n",
    "\n",
    "history = model.fit_generator(  generator(1, training_filenames),\n",
    "                                validation_data=generator(2, validation_filenames),\n",
    "                                steps_per_epoch=10,\n",
    "                                validation_steps = 10,\n",
    "                                epochs=2000,\n",
    "                                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2c9bf5d668a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mplot_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-2c9bf5d668a9>\u001b[0m in \u001b[0;36mplot_result\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# list all data in history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_result():\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.save_weights('./save/model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.load(data_path.format('x') + validation_filenames[0])\n",
    "plt.imshow(x[:,:,65], cmap='gray')\n",
    "x = np.reshape(x, (1, 128, 128, 128, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 128, 1)\n",
      "1.2680912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7df84c3b00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = model.predict(x)\n",
    "print(y.shape)\n",
    "y = np.reshape(y, (128, 128, 128))\n",
    "print(np.max(y))\n",
    "plt.imshow(y[:,:,65], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f803dc2e630>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.load(data_path.format('y') + validation_filenames[0])\n",
    "plt.imshow(y[:,:,65], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./save/model.weights.best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztvXuMZNldJvidyIyMyMx6P7q6urq6qx/V7efaIAvZsFoBntEAy+BZCVmwiPGAV9ZK7MKMRgJ7+INdaUbCmtGAV5o12wIGzwphWIYdG5YdhvWAYJHttXuM3Ka7q93uZ3VX16O7qrIyMzIiI/PuH5Hfye9+9xdRlZld1THW+aRSZEXce+455957fq/v9zupqioUFBQUEK23ugMFBQXThbIoFBQU1FAWhYKCghrKolBQUFBDWRQKCgpqKItCQUFBDWVRKCgoqOG2LQoppR9IKZ1LKT2bUvr47bpOQUHBm4t0O8hLKaUZAM8A+NsAzgP4CoAfr6rqyTf9YgUFBW8qZm9Tu98F4Nmqqp4DgJTSZwF8CEC4KKSUCq2yoOD240pVVcdvdtDtMh9OAXhZ/n9+67uMlNLHUkpfTSl99Tb1oaCgoI4Xb+Wg26Up3BRVVT0G4DGgaAoFBdOE26UpvALgtPz/3q3vCgoKphy3a1H4CoCzKaUHUkpzAH4MwOdv07UKCgreRNwW86GqqmFK6X8A8CcAZgD8ZlVVf7OTNn7+538eANButwEA6+vr2NjYAAB0u10AwN13342ZmZn8OwDMzc2h1+sBQD7+jTfeAAAMh0Ncv34dAHDvvfcCABYWFvAXf/EXAIC/+qu/AgAMBoN8bqs1Wjd5nXvvvRe/9mu/BgD47u/+bgDAJz/5yXxNHrexsYHhcFhrY3NzM/fxnnvuAQDs27cPALC0tIS1tbXa8YwMtVotpJQAAJ1OJ89Rv98HAMzOzub2V1dXASCPk+f1+/08lwSvw7nhNf1avM76+noe38LCAgDg+PHjePe7353HAABXrlzJ88h22cbm5mYe18GDBwEABw4cyPPAeXzttdcAANeuXcv3lkgp5Tb4ubm5mY/jfOiYOPfLy8v5GJ7L8UbgHA0GA8zNzQHYfq7a7XbjOeExKaXcj8FgAAB48skn8cQTTwAAXn/99XwN7ScA7N+/HwDw7ne/G3fffTcA4OmnnwYAvPTSS3ks8/Pz+fjTp0eK+cMPPwxg+x1JKeFXf/VXx44vwm3zKVRV9ccA/vh2tV9QUHB78JY5Gm8GlU7AaOXjakypsrm52ZDCs7OzeZXkKs7Vu6qqLOkoUQ8fPoxDhw7l34HRys2/eTzb7/f7WYrx2ocOHcrtubTScxU3btxoHOcaAq/tf/v/r169msfL6/N3bcv7oRKSc7ayslLTLvyabINze/369XzckSNHanPQ6/WyZFatiX9TK0gp5bFTm+E9AbY1PUrc2dnZPD6et76+XtOsdHyzs7O5j6r9+BzNzs7mc3yc2q72w++ZHs+x8B7Pzc01tBLVAv1ZHg6HNQ2LbVKj5HnaJo+LfrtVFJpzQUFBDVOrKXB1VUlKia/HcGXmSj0YDBq2MDULtQtVE6F9TGm5traWV21KNUqVfr+fbWZe+9VXX8XXvvY1AMAjjzwCYCTp1L+gY9nc3MzXZ7+73W6WnES02nNsw+Ew/802+v1+/o72OqXacDjEhQsXavPYarUa9nqn06lJcL8m2+N5rVYL3/rWtwAg+xYOHz6c+8M5UM3G79nq6mpuj/eCn4cOHcrSjz4L1Xj4d7fbzX+zLZ03l+izs7MNm7+qqtw31TJ4vvsPNjY2an4ZhR5PHDp0KLenfXPtJNIs9XnhXPK8jY2Nxpz6s7cTTO2iwAeXgxsMBtmxoiqj37yUUr7JPFcXEzpxVDXmYsBPhb+Yg8EA165dA7B983q9Hp599lkA2wvQ4uJi2Df2myogb9r8/Hx++dxpqS8jjxkMBmH77BPHyYfk8uXL+UXj2CPVUhcbQl8GNbHYx0uXLgEAXn55xFd7xzveAWBkTjzzzDO14zc3Nxum0HA4zPPh6u/CwkJ2trH/V69ezeaa0vTdfNCXnt/pS8xruRmhx+n9H2eeKKJFSk0j9skdpXocv1tbW2ssqsPhsLao8zt/TtWc3imK+VBQUFDD1GoKqhIDI0nqK7Oqe9Qi1Gnlq+fCwkKWuAcOHAAwWkm58lLKqzTxa/Z6vRxOUo2F/VCnn6t5HFO73c4r+crKSq3/7BNQl5r8XSWuqo/ASJqwTzRxqMrPzc01VEmV2iqxtD39bWNjoxHWXF9fz31i2Iy45557cObMGQDbjsOXX345t6/9cefj4uJinhfeH2oKKaWGM3SSKq9QyalzyTZUEwO2tcder1cLRXJ+3MHoJq62PzMz09B22Yfo+EgDUG2Q0JCrP69R+zdD0RQKCgpqmFpNwaXazMxMXvXUweahLHXEuM3Y6XQa9ubGxkaWSmp3UmLxWmqr0WGntj9tXDrD1tbWsubhtqhKNbYxHA4bRCmOV4lQ7EdEnNnY2Gg4aLVNl/zr6+sN6aohLyKyndUmplSl1kOCzrlz53D06FEAwEMPPZQ/6YPg8ZcvX87tcpycx3379uH48eO132ZmZvL9pn+n3+/nfvC+KzkpCtFyXLwHrVarYYurE9J9BEr0csmv4UoNU1PbUSIb59uf26WlpYZTW595nTMnyhG7CUlO7aKgZgMwesncwZJSyi+tevjdARfF/fWFU9UdGN0Uf6m0Dd4oxuAXFhby7+Qf9Hq97OyLYt5ex0Idn/6yR210Op2a85Pj9YWF/19YWGiwP1UVVSeoe661rUh11QUWQM2Jev78eQDbL/m73vWuHKHhPA6Hw7xARN5yNykOHDhQi35wDng/OB/8f8TS1MWMmPQCbW5u1vgu+j2wLZR0QeIzpqxcPmtsS81AP/7SpUsNU1UXOOXORDwJPW8nKOZDQUFBDVOrKTgbTPkHGjLjcVFM2kNv3W43Sw89lr9T3Y80CmWbsV1KvyNHjmRJx/5q7N25+DMzM414+GAwyJJQtRg93+eD0kZNBmc0qtbkEiZyhm5ubjYcahqGjBh5hGtcGqunA/YrX/lK1iTe9ra3ARhpMTQDXHovLCzkfqjp53M1MzOT54OmHM9bXV2tSVWO082Bubm5fH138EUa6Pr6ep5vtqWaiLfR7XYb2q7mpDiLN+rPgQMHGk74qHqam1A7QdEUCgoKaphaTYFQjcHto26321gJVVq6o3FcqJErP30AyqN3v0RKKfsNKPH27dvX4K1fu3YNp06dql1L7X2OSx2Ibg/q2CiF2Z92u53DqpSMeg2ey2NarVZ28ClRiH1T7UgzD30eXcMZx7Dz9tXn8vjjjwNA7s+jjz6a515DkRwn26C2sbq6mrNANSRJ0JnHOVaJS6ysrGSpzXugOS8REYrPFT/b7Xb+2+dlOBzmfmhbEXvWtVGdR++Htqf3RcegiNiRN8PULwq8+fv372/E7yP1d2Njo/GgT1Kh+/1+Nhuo+ilzz1X4qqqyCaKxafc093q9mhqox6vTStV2XpO/8YUeDocNFuDa2lo2WTRNmi8J2+BLtn///nx9TTJzemxVVflcvkj6YnhsX/vtplzENhwMBvn63/zmNwEA58+fx4kTJwDUGaHAiK6tizUwSjvmwqYvJa/B+6jzzuO42CirVBeHcQlO6lCNeBsa0fG5UnAsPK/T6WQh48JDr6kmlDvO1anpDEZ/bm4FxXwoKCioYWo1Bc9b6HQ6DSnVarUaTjzlAHi8H9iWFOpwUnWQ7brqrFLenWeHDx/Oqze1iH6/32DuafiPUlhDWdQM2EemDK+urmZHnIa3XO1ULYm/URpevXq1IfkjyVhVVcP80pCjpw+rw84dcTdzTGpolElVrnWoc041OjcNV1ZWsko+7t5p+61Wq6HBRf1UTY59UvPL2/A51n602+08BpoRly9fbrBE1Unoz7LyK9SRPo65uBvzoWgKBQUFNUytpkBpydU/Kqk1Pz/fSP3VEGNUVITfcUXXwhdKKPGQjtrHtAHp4FtYWGiEn5aWlhpSNQqbsv/D4bBBdmLI8/r16xNDS5E0Iyhh2u12ttvp4Ov1eg0pr+nX7sNpt9sNZqCme3NeVLtzIlan02kUb9F761Jb0+O1XB2vwedDfRWuJWmOgl7b75k6Uv2YmZmZxrOmOSwRSciL/ayvr+c5pVYYpVDrpzuwJ2mxOnbVHneKoikUFBTUMLWagnPJV1ZW8mpPz/Ty8nLWKDRy4MU21LtM6R5lqamnl559Qld0nkPJqAVkee3V1dUsldhf9Ro7QejGjRu59oDWafC+qec7KkXnJBqe98ADD2R7lmN57rnnwsK3LvWUEKVhPsIlFudFCWe8T+pRV8nudGG1pb3AquaCKGnMJblGHCIJGkUYCH8+1Ouv0Rj3Iaim5v2em5trRAnm5uZyP/34qqomfke/xPLyco3qvldM7aLgCUDD4bDhsNMHTB8ifscHkpO+urraCB11Op38YnhbbA+ohyb5Nx/IxcXFhtre6/WyI9JrF7ZarVDd5Mvkob1xnHxvQ+sa6iIJjF5QL8Cyb9++xuIXVUhSk8EdcDMzM/mB5OKniMKEGrYFRmaSh2OVn+JxeR13lIZNaJWoKDfBX+TNzc1GRe1oEdFr+/OhphzNXJ0XzUXh8f7i6yLs90CvSczNzTUSuIgSkiwoKNgzplZTcAefrprUIpaXl/PqytVYy2B5FmFKqVEGTVVOLctGQpCvtMrSoymyuLiYz2V7a2tr2XxwR6NKOlVPOYazZ8/WzlteXm7Mh6Z8K/uP1/cqwJE01rRdlcauJms4LMrgdIIN+7OwsJD313j729+ej+G8MUvypZdeytmUzhDUCtKabqw1M7UP2obCv1MmZsTAjFL33Vm5ubnZKH6jUt+1E/2O2ppqjT7vGv6OcoAIdbw6SkiyoKBgz5haTcHpo+12uxHi04KmKvndtqVkGgwGDU1BefEa/nTik9p77BttRnVuavYbfQpOPY5Cnhrychprt9vN/dDx0h+g0tIlrc4fj9ddnrxCskpLh86LFsd1hx3DbUePHsX9998PALlQihK9OFdra2u56Otf/uVfAtjWIrT0WjR/hErcqCqyOzJ1bgh1pLomqf4rlfwRDZ6ffo+1j5yjhYWFhlanfgnXItbX1xuhzkl1IO5okZWU0mkA/wbACQAVgMeqqvpUSukIgN8FcAbACwA+XFXV1Z22H8Vn6ZwhlL+u3lln4ml1o8gh5C+hssZcnVT2mPIl2DflH9AEiVRuT5fVB53sReUCRA9M5DX3dHF30gF1BylVUY5FFyB3gHEOvT1/QdmHtbW1PPdkLK6srOSxa37DyZMnAQDf933fB2CUYg0AFy9eDJmhvghvbGyMrVPYarVqZeqBukNOzapxTk1dLPUzig7w000+/Y5jX1xczMxVN2fUGaoLi0dStEq0H7+bRWEv5sMQwD+uquodAN4P4GdSSu8A8HEAX6iq6iyAL2z9v6Cg4D8T7FpTqKrqAoALW3/fSCk9BeAUgA8B+N6twz4D4M8B/MJO2x/nfAHqRVbcfADQqDFIaL6A5lZ4+GnSFmuqiVD1TinlcKCeR4k/qayZqp0R25LHOL9CS8ZxTFpz0VXozc3Nhuq/f//+Ws1HHucqdKQVRNx+9kcdrK5ev/HGGw2H8YEDB3ItR+7xwI1lqqrKpoVzQYD6ffGiNipxPX1eQ9d8Xvr9fmMs6hyOtAI/XvkNkTbjNTT37dvXCCdGc6sajJcqjLIpnSOzE7wpPoWU0hkA3wHgywBObC0YAPAaRuZFdM7HAHzszbh+QUHBm4c9LwoppX0A/i2Af1hV1ZLaMFVVVSmlkLRfVdVjAB7baqNxjGfcqSag5bO8/Fi/32+QeqLt4Ql15kTag4emdG8Ako2uXbsWhoS4WtOG14InbrtquS/a98xViBxr6vdQB1gUZgPqtRZ47evXr4cEqXH8f5VIeky02Ssw8oXQp0BNav/+/VnDIq5fv94ok8dQ5okTJ7JvhvOoodeo9gChcxwVNI1Yjs7OjEhRWiDHfU7KyPRQtGZ36vZ+4/w/nU6noQlrQRUer0xWHuch8p1gT4tCSqmN0YLw21VV/cHW1xdTSierqrqQUjoJ4NJu2o7MBkLVSJ/QVqvVUCP5YCorTVU2Tign8mYbehB8WF944YVw8lUt1T7qg8aHaDAYNOLgemMnxbx1kVSPvo5TH2C+lDdu3GjMs9YMjJxV7rHX/7uqu7GxkU0omgXdbrdRwaiqtisrs3y+qtc0LXxnb+1bVVUNMy3iuKjwcNan1pQklMHpEQldhPnM6D6c0b6R3u/5+flGMSBCU+EJNRF17j0y4v3fCXbtaEyj3v4GgKeqqvqX8tPnAXxk6++PAPjcbq9RUFBw57EXTeF7APwkgCdSSn+99d0/AfDLAH4vpfRRAC8C+PBuGo9WvEiFjpJrIr44P90RF5kU3W63kX6t0ofX4iYmTz/9dOYkqDpO6edhU3USTgodKb/Bf1fzISpc4mp+VJRFr6EmgBedIcYlDPmcsq3V1dUs3dWM8M1hdXdqHvfqq68CAO66667sUOOnVmeOOAmRKu87UUfPk+5EHYWiI+ez59JMuheRA3v//v21vU04p2zLNWFNp1Ytz+c+Kol3q9hL9OH/BTAuCPrB3bZbUFDw1mJqGY2TMuMILcem2W08lysu7b1+v99gKM7NzWVpSR+BOr18BdYstZdeegkA8Id/+Ie4ePFi/p394N+0q7XfnrKsm8P66q6FW9UOj7Z0dxuX2opuda9suldeeaU2dnXUOqlL7e1J90XJOiTmUAM4cuRI3vSWGoP2m/3lnHU6nUb69dzcXG5P77VnxUbZnZNYgCmlBslJ/QfjMhH1Wtquh3Z1rw4ed+DAgew7Uv8SED/LqsVqyHWcL2Q3mkLJfSgoKKhhajUFt5c1dKg2l4fDNLTnlFxdxZUgxOOYqbe6utrItXfKKrDNz//iF79Yy4PgtShlGErTPRGj6IprRzxe94GkJIvCbKo5ePaj2p1KfmH2IjeF1T0kXKvSWhKqPbBd3wtR/S/USO6+++5wDwvP++D96fV6uTgMMwuXlpYaxUvn5ubGhlLVNlcfikcpNjc3w418gfo+EeqvcUKVFqvx+6kaBts6dOhQrrfx2muv5f7qfPp8u3ZWVVXDhxDR228VU7soEBEjL2J8cZKVzRgtLJoIBdQf3Mgh6Y5ATQCKbjahN4ohQGWZTaqdx994PF8G/U1zMHQu/EHXqtEey7506VJNjQXqO0C7M1TV38hMisKm/I2Ow4MHD+bU8LvuugvAaMHgAuhsTl3ouCDu27dvbHET9kk/Vb3WbdvcSakChYhSylUwRGFmfu8cEHXcqmB773vfC2D72X366adzG9EC4GadPgseFr7TuQ8FBQXfhphaTSHKkhznTAHqK3ZUGIVwSVpVzS3Dozp3kURSKeJqpIarqCl4aNLbcNIUpafuIaHSxyW/hquimo6EajZkZRLdbrdGwNE2lE0XpfdOAsf+wgsvZDLSfffdl9tne1Shlfh18ODBWj8OHTrUkPyRyadjjtiOrlpHqraW6ouOd0msDl7Ph+j3+41sWgB5Pt73vvcB2H5eLl68mOfBw8M6vihU7DkZO0HRFAoKCmqYWk2BcMmu32lWm/42jhi0vr7e0CKUjkqbtdvthnYp23cSkEodleTOn6dUnpQ3oFCnpW+zru3omCbZsZEG5QVgdHPdiAAT2ajjnFutVitLOt296amnnqqdf/LkyUaFbN3v0jU53X9Cd5Jye1p9Lu7v0PlQgpdrG1p92e+nbn4bkagILfDidHgtLUeH4zvf+U4Ao7Cs5zmMq25N+PF3lLx0p6DqkD906uFXL7unFOv/fZHRl1ErCTuzzRca/9tfRuURUF0nFwBovqCTCnb0+/3chl7Tqz5ru3xQNM3cH+Cqqhr8//n5+UahFp2ziLMwriCNHsO/tRz5l770JQDAAw88gNOnTwPYNikY2RkMBo3036hdrWrkjl+tCK3HT+KKEPx/t9vN3AlVySNHID99MdvY2Gikkus5PI7Vqp5//nk899xzjbH7i64p2S4oiqOxoKBgz5haTSEyCzzMFpXUUgnsBS1UfYvyBrhSHzx4MDPxqM5Gmoj2MdJinAGn5kOkeRDjUr+1jz4uv2a0f4LvtzA7O5tTmrWIi+eTKPszyp9wnkKUaq1tuXr/zDPP5Os/8sgjtT5GjuPhcNjQ9IbDYR7ruPqaPpc+Pg1d+j2YnZ3NWpVmoHrBGDVFqXGpNqsmIcfnmbu8Jw888EB2vCoPxlPUO51O4/nfjdlAFE2hoKCghqnVFHyl29zc3jLet0oH6o411zIiLULtUrfpdMcnOsgoyaIKuxpS0/64s4eZlL1eL7P6oqrIPnaV3sSBAwey5Iq2DHNWpNqzGq5iiIyswVdeeSWHxKK6Cp7zoNIvykCNsl3ZD157OBzmOgr8jgSnS5cu1ba95/GUzOpMdDKXalCuWUTksUl1D6JCN1rnItrVi/dH62l4VqWGkd2XdM899+DYsWMAUKvE7b6KiM25F0z9oqA32p2KQHPPRK1W446kyOuvLzlVbq1y7F5uLYYyyemm3AWCN3Z5eblWhYnnuQlCB9uhQ4fy4sRxLi8v5+vSganVk6PKS15DcW5uLr+MxNraWh6rq9d6L6KFIjKhfI6UthwxQbkjNo85duxYgyZ+48aNbN4R7XY7Lyiesq70ZTUp/QWNFnxV1Xk/NE3fC93o/XeOQVRxS80B9lGZm2R9koKvJlnEiXGzuCREFRQU7BlTqylETivf8VidQbqRi9en01WTUlIlgIfeOp1Ow1GnOQQuhZVHr/11CcprX7t2LZcnIyIVl+efPn06f8edqS9cuJAlUeRwdVU4Yt9tbm6GFYS9jF3kFI3i8R6+1TRs1d5cc5udnc1j4SfNtfvvvz873vhMaGEcnXdqFJ6cxOvqOHU+PM3cx8pj6OyLtqjzEKBqHZFqr6nZbqZpyj/Zjm4q6jX1untxMBJFUygoKKhhajUFt73a7XZ2zkVbe0dbbnN1pdRRJ00UriK0mCZtW2USRqtylPvgUClICUM7WNuiZHn99dcBjEg+/I5t9Hq9sERblDk5Dp1OJ7PoSBp64403cO7cuXBMQLw92jiGnRZMVQccx+A7fmm7JC+trKzkoiz8TTcW1nvh9zRifKrG4hpit9ut+SEUERNS4ZqLZlyqZuTzp2XyqOEqY5cZspyr9fX1xjOmBDXPhyjkpYKCgj1jajUFagPqR/BSY1EBVK3nz9+4ymooUItkOtml2+3mzDz6AbzUl0LrL0TSVY8DRpLO7U2l3bqWtLKy0pA6UWhKQ6Nu487OzjY49t1uN0sijrPdbmftyPfM7Ha7YWGXcXsldDqdhuSK6Orr6+v5vjhZrNfrNYrLRkQlpWATUT2FyJdEaE5FlBXrY5lUMHUwGDQK40SZmRoV8ijP3NxcftY5P0tLS41nTbcpGEfx3wmmdlFgyE5vdKS2TVKPfaJmZmZqlYv46Q/64uJivj7Dff7Q+rWj4hbjYuPXr1/P7XDxiVKQ2e8zZ87ggQceALAdsrt69Wp2BGoI0cOrUWIUF5bBYJBrS1J11erCmqvB/nv+xHA4bDzUyvX3Mc3PzzeqRWu1ajoVyZXQRZiLg85NBF8s3aQinCGrLyih4Uo3WYBb21dB1X1fRKL+6nXcua4Vo6JFwZ+5O7rvQ0FBwbcnplZTUOkOxOWt/HcgzoTkKqrptepc4kqtpgIlJ6UmVeqrV682SD2KyMHjYaLl5eWJbUTMOTretLRblGmpxBod59raWiMcq9fRcJ9LLNWuIu6+Xl+vyb4D2/O4ubmZTRZui/fOd74zn0Pn6gsvvABgVGgkyq3wrMfomdB77cfp86SkIa38rXOlORh8NhkC1e8U3o9xmuW4TEtgW4OjCTw/P99Ij1aGpz/zu6nRWDSFgoKCGt6MDWZnAHwVwCtVVf1wSukBAJ8FcBTA4wB+sqqqJp/15u0CqEteJ4qoRFKpcCtSRO1gb1elmWcdLi4u5rz6SdmAKolcO1lfX6/Zx95vQkNwXoRENSK/tl4zIh6pVHGi0ubmZrbrPaQLNJ1tuquS13yI6LdqE1NTeOihh/I1KRFJ6/3Sl76UQ6TqJIyIXk6bVo3Hw5NKOSZWV1fDWgyEhwI189Sfteh+ppQa2u44Uhnbd2f5/Px8dvyqtuH7VETa4K3izTAffg7AUwAObP3/kwB+paqqz6aUfg3ARwF8eqeN3kriShSXjzbljF5e3RDUTZXBYJDVNt+IZN++ffkliRhw6uBx5iOxvr5eqzrNfrsayfOuX7/eyMXQHAxlVrpa78fo35Hqr1EKRoC0GlKU9OSLkxZp8TRf5SbQJOPcKrjr9Ac+8IF8rW9+85t5Dvy+R9fSRDfvt0ZNdG68MjWhFb0I3UU6yj2IIg1u2qrz0etOAttzyjmKoiwbGxu1Rd37sVPsyXxIKd0L4L8G8Otb/08Avh/A728d8hkAf28v1ygoKLiz2Kum8KsAfh4ANyY4CuBaVVVcxs8DOLWbhj1FVyWSSkPPBlN1zLP9tCIvf+v3+41SVppxpxwHYBSyo7NPC2x4fxX+3WAwCDMEx1Xg7fV6DU1Es/xU+vh3UcaitqUbw7BvkUPXr6k1DyOTgvPiYUHVwmiGvfLKK/lvHv/www8DAI4fP453vOMdALbDsZcvX24U0NHUeucaROHE6D6pI9XvZ6vVanBUos11VROZ5IxVuOSP2KLK23E+SJSirjUud4q9bEX/wwAuVVX1+C7P/1hK6asppa/utg8FBQVvPva6Ff2PpJR+CEAXI5/CpwAcSinNbmkL9wJ4JTq5qqrHADwGACmlRmqX22hR8ZTIDldnTtRmJC3dqbm5uZlXWBa5oKYwHA7zqs2QlNqzkWT2EmYbGxu1zVWBui0fkZ58PwctSUa0Wq1GVqdqDJ77v7a21rDDW61WrlWgDsZxc6sOOy9KOq6StG+nt7Gxkf9m+TGGJk+ePJnvAQuvXL58ueE4jByCej+9tkan02loFPp8uFaqUpukLiVdjdPyeByvHWkPrpVEPiI6vqPMYG2KV4uJAAAgAElEQVR3koP5VrFrTaGqqk9UVXVvVVVnAPwYgP9YVdVPAPgzAD+6ddhHAHxut9coKCi487gd5KVfAPDZlNI/BfA1AL+xm0aiCjK+CkZFLMeFKdmmS2Et5qqrMM9hqIz27Pz8fI3IxOu41zfKh9B+UQprf3xV1/EyDEUtRenF2udJJb6jqIyHM9fX12uknJu132q1GjtK6Tz6vphKIOOYNE+AIP368uXLeM973gNgeyelEydO5L0pqRF1Op0G738SyU2fJ71347TMaCuAwWDQ0L50fiLfjF6fx0VRJH6yPY3UeF2Hdrsd7iCl19kJ3pRFoaqqPwfw51t/Pwfgu96MdoH4JdOdff3Fj1TtKFaraqL/rqq51zBUx2TkrNJ49TjzAdh+IXQjEmcSqjrpjDUNvUY8d58XfaEJTVjShC/vr7IY/cXT+Y54GRrq5G9aUo5zyiQtmg10PG5sbODFF18EgFyY5q677soMTy42yttwh6C+MJOcf1rHMrq3agbweDVHgNjhHYXVdT7cfNUFUkPnAHD48OG8YGptTl7/VhyqN0NhNBYUFNQwtbkPkeOO0G3kI4bYOJVJj9dsQpV6eqx+p1mTXsy11+s1JIw6H11izMzMZGcVzYhjx441wogRwYVtRmnJ6+vrDQeTnufELd1GTxmeNFE89NXr9RqOuI2NjUbVYjVdopwDL5e2urqanYnsm2ok1B54zIEDB2qaG/vhZgP7Fe0T0Wq1GqXoosKqSpTjd+xjFBKPnj29B/67po27c1j7RkcjNSq/ljt3I3P6VlE0hYKCghqmVlOYRBvVMF6U5+A2WmTnEyrRFZFNzk/vR+Qs0sIeLkWGw2G4y5VrCEp75XeUGMPhsLZrEM9z3wOxvr7eKP6pYTatccBQlzuv+v1+eF8iRy0xjpijuHLlSg43sr6E8vvZX/oRtI9eNEeh8z+O+u59G1caXZ+ryE/jTmX1+aim6A5PJSMRk+ovHDhwoOFnmJkZvwfmbjC1i4JPxmAwCGPAUSWgKBGKiB5qVzcBNKIDhFbn0TajBB13jOrxfOh1K7mIA0A4t30Sr0G/U0fmuM1SxvWN3v6bOerGsfrUqagbomjlLGBUF5LX5EazL7/8cr6e74uwsLCQFw+yS3VDFB0z5ypienoVZ41ORWxRd2TqS0uoEzqKJkwyDX1jGV3ItRALa1YyStTpdBpzFCUN3iqK+VBQUFDD1GoKkUQnIpaZStmI5cjjJ9U1jEJ7rjJqWC66hqqf0QamPIb9UE6AS6dIwuj2YZMYnvykhNbqwnq8X0sdtdRiVOr4ln2R9FNp7NpDlD34+uuv4+mnnwaAnOdAfsj58+cbIWCttk1z6saNGw1zSjWHSeHp6L7zb3X+udRtt9u1Wp86duUYaJtR33hfPP0ZaOaTdLvdxq5omn/iGMe7mISiKRQUFNQwtZqCr9RKVCKU064SyUlDRFRoRFdvDWm5vU4Sk27KSmdXJO0jVl/kNGW4TWs9+PFRqFFDewq3JcexKn0+oraiHbYIbddDaXpPfH/EqK4DADz33HMAtkNu1BQuXryYbX8tfMO513wL9xGo7yLy+bgtn4KCO8Q4H4ozGpVYN8nZp/1xbSpiRapj0mtrqObqmkhUffxmmNpFwR2NquZPSlPWBy2K2buqFiVEra+vj40cdDqdfFMYz+eLrdAXyNtXijIdmhEzUB+OiB/garuqlp7irOPUxSlyrLkqzOO73W5W0XWcvghrkZBoXlwlBrYXVtZmfNvb3gZglAT1/PPPA9iOMOzfv79hrkWLnjpltfALPyctdp7cNY62zLHqruRs04+PFnI1aV3AtdvtxjOvi0Jk5ka8np2imA8FBQU1TK2mMGl7MtUiori3hjGBOPSmq7g7Kzc2NkIHJjDSFLzC8+zsbK3gip8X5QSwfUo/3YzVVf+UUoN3ryXGVIug9sIQJvuqjEai3W43TCxl2LkWoRvy6HiVZam/KVTz8rEvLCzkNi5fvlxr68yZM5lNqklh7jDu9Xrh1m28dsSQdU2h1Wo1Nv+JND5lNmpKPTC5fmjkGNdQe2R6EmqecD4iB+mk3I1bRdEUCgoKaphaTcFXVOXRRzsQqV0bEU/46aurSgIn/Oh3XM337duXJRxXbLW1Iwnhq7aSeihpVldXGwVMVeJpqjcwkmTHjx+vHaeb8Pr27evr67WNdoF68VcNn1Lb4PgoPV999dWQ6ecFavk5Pz+fz9XfPMzb7/dzv9lfFlsZDAaZrMPz+v1+Y38L9bH4lnKqhU3KQ9DnKSLF+W+RL8K1Wf1bWZGqWUTFh/mb76alTnDVRn2buLekHFtBQcG3J6ZWU4hKpGmIjt9RwmgYzz2wqnX4d+oR1t/cE6z2Ke17SuNut5sloobnxmXOqR+DK/za2tpYHv3c3BwefPBBANs1BfRcFoBZXl7Omge1GWowVVU1pPbs7GwjV2JmZia3R+mkJJmo1Nk48le0bbp65dU/obUBtP+XLl3KGhHvda/Xa/RjbW2tMc+RtFaNwf1FUc0JQslIEaHOC/yq/4CIxh6VzfdjvI++d6dqPR4i3Q2mdlHw0I0OXNUnZ9gBsWMRiAuT6EOg4bao8ArP42LAB/fw4cNZNde4cBRa9Pb5MqysrDRCeerMY1tMH75x40aO7bOm4tWrV8NKxj5OdXJpGjCP99RmcgfUPNEQHOfSzaqVlZXQieZmnf7t/en3+3mRYphyMBjk+8GXUDeIIfReeLhXzQE1UceFvfV5iBa46Nn0UO04Bq6bPXqtaK7YJ92Exxmb7qzeCYr5UFBQUMPUawqqgkc1+JxkEpXUitS9STkTUek3PVd56MDIjLhw4UKtXeWjR4U4fGXXvSP8t16vh69//esAgJdeegnASIJSy+B8DIfDhmNK1UovgqLZg1p0hte/7777ACCbLpcuXcr9IDS0x/nQrEOHquER+UYzG4F6OTZqcvPz843UczUbI4d05PSNwt56jo/Bnc6qJUXhxIhgFZlr3oYyMzWkrOPV47VPnm8zzhyahKIpFBQU1DC1moJLy2jFVkROl6jysK/oSmlWG95txOhatJ2PHj0ahjh9tZ7Ux2jXKNV0dIclfuf2t1b19RBVq9XKuQPE7OxslkQ8TslZ7C8JRcC2Q411DJQQFVWSdgmtdjj72O12G3aySlS2wX60Wq0cNlX729vQ77WEH/sa5Uh4WFXt/Oh+er4KMRwOa0VyfOyR1uqaTuQI5nX1M3IA3wrlfBymdlEgVGXz+LA6rSKvcsSAdOePOit912Q9V28E21BOgHvqq6pqOPuiDUt4zPLycuhV5v+5AL33ve8FMKpQRCcbC5Lo9nJcANjX5eXlRhEP7aOmALN+pNZm5DV9M15Nv3ZzQV9oLRziTl4dH00W3otLly7lys7qOGRat4LX51h0a7koEhSZiP6M6aLjAmVS9efICTkYDGpVs3me902fV6+2rSZi5Kh1kzlKdLsZivlQUFBQw9RqCr7yRU6gqqoaOQHKASCilVdVWNcQVG2LinO4E+/o0aO4//77AdQ3OHEWomozXhFa04yZU6Hj4DjZ79XV1VyzUCUHr+EbtPT7/Ybk1+/oJFQHaVQ4hmq7lllz56bmBri2pM5hNX845oceeggAMjfh/PnzeQt6fqq2oebAuP04VLqqpuMSepKTUJ2VRLSVIdFqtRql1FRjUcegp2tHz5yaJ/4sKPxZLkVWCgoK9ow9aQoppUMAfh3AuwBUAH4awDkAvwvgDIAXAHy4qqqru2i79hmRl3RlV7vd/QbjiCfASGpydVWnmJNWdOV1qdPpdHDy5EkA2/UAer1ew+kX5XFENf6d2DQ7O5sdeyQsHTp0KBOI+JsWY4nsfC+8EbFEdawENYu5ublMZDp//nxuU3dM8mtH0tiZjymlRo0KkrT279/fyH14/vnnG+HVSMpHElc1C5eiug+GO5+9v0C8FT01rsihqj4cvbY7s1XT8n7r8xdlRxJa5Han2Kv58CkA/76qqh9NKc0BWADwTwB8oaqqX04pfRzAxzHaX3JH8GpI+jD5jdPj9RxCKcvK5uOx/nBED5HeYGeZbW5u5irETz75JIARuzByUvLTnUpaQ9G3bauqKveXC4Amumh/IrWa14mcbRHl2L9TpxWvy2rKly5dysfxNx2Ht6X90IefRVa8+MzCwkLmSaj5Q5ZjRG/3RTullBc2jTiMY5ACTcarRnt0E5nIVHGokHKhpJjELVDh6NycVqvVuK5HcXaCXZsPKaWDAP4rbG0gW1XVoKqqawA+BOAzW4d9BsDf2+01CgoK7jz2oik8AOAygH+dUnoPgMcB/ByAE1VVXdg65jUAJ3bTuMe3tfAJEfEOxiU98RhXC9XZRmgojYjUff2Oai8djhcuXGg4ffTTHVnKjpukCWnOgSfhAE1+QpTToBLUNSFNMvOYfa/Xy6FDrbnoGo7OjyeWAc2NWvU8Dz+2Wq2sldBEe/DBB/OO32oijktB1nB2VM4u4lpwTBpKdQ1OHcaE3mNnF0aObtXMnNGoznVN/HItJkrumlSy8GbYi6NxFsB3Avh0VVXfAWAFI1MhoxqNpKknAUgpfSyl9NWU0lf30IeCgoI3GXvRFM4DOF9V1Ze3/v/7GC0KF1NKJ6uqupBSOgngUnRyVVWPAXgMAFJKjYXDV3YtTKIhuEnOJJeCVVVlYgs/q6pq2OkRPz9qXyUMv3v00UdHk3P+PL71rW/Vrk+o/TuJ4aZz4USbpaWlTFCKSCwRx95Dhq1Wq5FRqmXeIucVMzIju9qdsjouLewyyVGmIV1gxGJkH7knxOnTp3N4UjWncRmi+pv2lf2Mwtnu09IcD50rL8OnpDTPWFQtNmrDQ5N6vG4OrM8u54pao2sud9SnUFXVawBeTik9uvXVBwE8CeDzAD6y9d1HAHxut9coKCi489hr9OF/BPDbW5GH5wD8FEYLze+llD4K4EUAH97LBdQG8+y7cWSkSXaUS8G1tbUGuSha5WnnaV4EpZTmC/C4s2fP5tLvPE5XbZdIi4uLDanjx2q/NYSoEpp/R0VUNc+C13TprjtPcV7UN8IoAaVVt9vNf7vXX6E2dBSC4zXoK6D/YGNjI2snPO/hhx/GXXfdBWBUIo6/eaTDc0l0riLNbBJtWDWLKIrkz9X6+nqD+q6EtshXFtHbCS0t52PRSNSkwrO3ij0tClVV/TWA9wU/fXAv7SqikF3EIlOV3h1khLLYdL8Fb6OqqjzRXp13ZWUlLHzhZsyDDz6YF4Vz584BqBcEcUbjvn37Gg+Mhhf95upYqDouLy+PzftQNh0fmF6v10h+0heaL7uqszyXD1+/3284E7V9r6gU5X/Mzs7mBeXixYsAtheF+fn5vBAxhfrAgQPZ4an3J6oKNe6aQJOZGpk9yp51Ey5yDrujV79T+NwC2/PG3w4ePNgIC6+vrzdCqYPBoFGLMerHraIwGgsKCmqY+twHlQSTUld19eZK6g6Zzc3NWg09b4NSRKV2RMzx0NRwOMyahxYrOXv2LIDtjD5lARIs7aZ7KkxyUGlo0o9ThyShjkQnNOnYCdW09Dt+MvTKNq5fv97Yg0HrVLoTUs0HvS+cI7ZPM+Lo0aM5ZZrXfOmll3DixIna/C0tLTX2XlCJ6vOi/48cq3x2VFuLKiu741DZsZOK/Oh1PAVew8nOio2ybyPHtWsuO0HRFAoKCmqYWk2BUDsvIuZEzhnPN1cNw4uVqE1HkkyU/cb/q0NQOf++l6C25zTdN954I4fe6A84cODA2IIdUThMy7epP8C1DT3Gw4+R9FOHmkvQ4XBYK0HHsbANJ+uoHe5ajR63vr6e/2Z4mPNz8uTJhrR8/fXX8/UZlr127drYnZYGg0HDh6PHaX8nZT1GDkEviad+DfcDKYmKiPaTiMhl/Ox2u1nbjfI9/PioSNDNMLWLgnvPx+UjeLESVRl9wVhbWwudj8pP0LaAJjOQ/eK5/I3tqtrJc1mWnedevHgxFzJh4s/CwsLYmo7R5qb6QmtilptMep63q/MXMeai3A06GJX16IxGfaijpCNPKQbqKjmAXPPykUceqb34nA/2kW3pQj6JW+IvnsP5MZEzWZ2nUV6LXzPKaYiS9KJPN2011VrNWF9seJ92kxBVzIeCgoIaplZTiNhxHqvtdDqNlT/KFFTuvm+dtrCwkDWFSXs2aM07V+U3NjYa24Ovrq7ma/C3e++9F8BIO+AYVMr7tZWF5/UH1WmqzEY3nXRefDt2DeMpmy5SY9kG54Ftjdsyj+e5U7bVaoU5AZ7CTY3nxo0bOTxJTWFzczNrFnRMRnwJZU6qAxiocwZUarv5FY1dQ5nuTIw0hknZj5oH4+ZPpLGurq7m+VYtmePyQjO74SkUTaGgoKCGqdUUJuWUE4PBIHQ0Ou/fmXzaVuRAikgs2r46sPhbJCXHMdXm5+cbXPVut9vgvus4XGoDTW1qMBg0JJw6HL08nIZvleij+0hof3RuOfZOp9PY6i3K/FRnoTMONQzqNvTKygqOHj1aa0NZfWxjUpEYLdirZCTPmI12FovIS5GfxscU+VNUM9N6B85C1HvtjuBer5c1xKjWg7NRd4OiKRQUFNQwtZpCFFrxMJuG4KJzuVJzJVY+unq7o5oCHtZSW9Ov2Wq1GnahRgIIjaS4dIrKshFa2k1z7b26TrvdDkvQ8Tyeq1LTIwYqVaO8fc/CVM+35oewj+4rALbnWbef5znuE3nuuedyVSvmO1y5cqWxh2iU2cpxaMiTn3Nzc2GOic+b+3ccUbk+H7s+y/5czc3NhX4dv2ZUhl7vu/tCovoft4qpXRQmFQSJVGJ9MD3tNQrBEXrzdGIj52P0f29X1Td3akaqv95gLejh/SY4tm63m19CLVrCdrkQ6mLi7XW73YapoIueMwPb7XZ29umLR7XXx6vJPno/I/6DL9a879evX8dXvvIVANuh3Xa73Ui/1k19Iiebv6BR0R7tO6Hmg5tkkemp9ypKEPNQpy5E4xYkHdPi4mLDVFBTKDp3pyjmQ0FBQQ1TqykQ6mhxJ0rEdqyqaqz6m1IKHU4emtKsPSLa5yBKtVUHj+9ToRIhSs2NitWyHV6fbR45cqTB6ltZWak5HbVfWmYt0nYo5efm5hopxzqfDKuqdkAiFkNlHj7V73R8On+cG46F/x8Oh5nIpOnpbrJ0Op08hkhKupqvqnyUdu8MTH2uJqVaq9bm0lsLBalTdhyLUqGaCJ3Uk7JoOcca6r5VFE2hoKCghqnVFNxxEkkVDftwVe52u/k7Sjx1+HmZbnX06Fbq4zLb2u12Qyvo9XoNCaMFR5nxF+UEEFpExncAGg6Hjfz7AwcO4JFHHqnNzeXLl/OuUZTaLFCiNjGhezawfS3HRuiYOEckDa2trTXseyVORdKSUPvdac7cX+LSpUu5fY7t5MmTWWtQrSYKC/J718LUp6CkoSjzdNxYIideVPBGn6UoJOnZpTrfXsOhqqo8dj3P/SmTMjNvhqldFPwGqMqr3AF/QbXghDtwVD2MVFuNW3shi0hljAqY6IsfRTXY18iJ6Nx6Vf38wXn++efz3/zt6tWrOaGIjkZdrPjia6qu8x/6/X7D8apjYsGTqNCIP8C6uPoceN9cCOjiw0WV49XUdk1Z96iD8hn4IhFapEY99d5fLXzi+RJRFSlCTQWNKoyLagHxbtn+cm9uNqtyRybzXlDMh4KCghqmVlPwjDddUXWF9y3ZNDzjsW+eE11H2x1X0IXHu4qmmoKqjOM4A+MKcPhxUdVjzWZktehxDj1FpOFoEY8ou89V0rm5uWw2nDp1CsAohZqVlVkoRecqymKN0rpd+imrkxoCzYc33ngjX59QpmSkojuvQf+elIKsrM6oOMyktOfIfJiUru3Zptq+OrrdFFINxLXBUmSloKBgz5haTcHt1KjUmP5ODIfDbPdylVSnVORsIyZxztXO9tVXawpMcvBobQa3e7VfbqcuLCxkCUCpuX///uyMY2my9fX1vMeib/q6vLzc2OtRQ52RbeuSd2FhoRHWfOONNxr7SUSORtXeeBztdSWmEbzO8ePHM2GK9/W5557LYVDdXDfaWYtwTU73W4i0NdeSut1uLYzNT2cOqkNQ2ZPj+qXalNeIGAwGjarMqpVqG+4Hinxgt4qpXRScfhtVaY4q+Kpn318MrRxEbG5uYt++fbV2gaZjLFIB9cHx5CH17EeeYXc4TVpEhsNhwwl17NixXNGJi8P6+nqucsxK0jyv1+s1HpTooeb1tE+qGnsC0uLiYr4G55ZmRFTwRtvVxcEdu+Rg3HXXXTmCwn68/vrreTHQl91T1XVOPTKiTjy9P+OSo5QtqoumPx+EzpVW6vJxqoPU2aXdbrexmFVV1UiTjqJlPN6vdyso5kNBQUENU6spuAobFaPg90B9tfdCE6ruU8JRmszMzGQVPipq4o4hLc6h2oGn/EYxb0ofrSeo4VPdGkyvrYVJ2Nbq6mquckxVuqqqHKLz7dei5DFNFFK1102miEGq5/l8U/VXyRs5MlWSu6lCh+YjjzySv/v6178OAHjttdcavA0tDuN7TWgIWFV4NzdSSvl+RNvuueak14jG5tpfFL5UByYRPctRMp1ey8PHrp3uBEVTKCgoqGFPmkJK6R8B+O8AVACewGjbuJMAPgvgKEbb0/9kVVXN/btuAifraMmzaBXUFVVZa0DTCaR/t9vtBpNQMwqjUKPbbeqLULtQi7DocZrGrKEjz0rUHaV8L4tr167lMTB7cGZmJmsIdMqpVPH2VcpHY/DcjfX19SxJ1ebmd/zk8cp2VF9ORL7hXLGgyuHDhwGMiErve9/7ar89/vjjeOaZZ2ptdTqdfC1qS3qPPQtzbW2tptFof/Q7vf+u+UVp99Hcapvjwo/aLttcX19v9E2fk6iosTstbyWvwrFrTSGldArAzwJ4X1VV7wIwA+DHAHwSwK9UVfUwgKsAPrrbaxQUFNx57NWnMAtgPqW0DmABwAUA3w/gv936/TMA/icAn95pw17cM7JPNbQXrZZueynJQ/0Ibo/pPgGE9sO59do3tr+0tNSIjGh4S6nX3m/PcNTQnkpXagUcy+LiYsMLrrZ/pDlRG9HwYET75jW9hLz6KjRbU6+jY9Pv1UfhPhPVTti3hx56KM8ZIxwkNClZLLLzvbZBVOI9omzzc2VlpVEQR6NCrlFGdT1U29DnO8rv8d/UvxOFPwkvfxeFQW+GXS8KVVW9klL6FwBeAtAD8B8wMheuVVXFJ/08gFNjmpiISP1xFqBWUtIX2x9KVfH4HV+G5eXlnIqqL6iH5Yj5+fkGz1z3StBNZvwFjcKm/C6Kg0cPn4Jj4Aui9RI9bq0LYuTA5Of+/fsbD7jCF4yZmZnGrt063z6mSXUQtR+64SzvJysvnTp1KpsX5DBED7+yYd1xGDFIleHpjMa5ubnGwqK5NL6Y6YKoeRRu1qW0vbcDF9zIpCOiNGmdZ16LbdFhuxPsxXw4DOBDAB4AcA+ARQA/sIPzP5ZS+mpK6au77UNBQcGbj72YD38LwPNVVV0GgJTSHwD4HgCHUkqzW9rCvQBeiU6uquoxAI9tndugXbmUV0mnoUAPwczOzubVngSXSMqqCsbVW4tXuETUMKGnZrMdPV6vEWk9bipo2EzmKLfj5pSCIcnDhw83JIZWX1YnGPvgJC1N5XUmnqq/UYao7/wUZQpWVZX7S9KYSmiaQpR03W43mwj87u67784kLdV+eH13tm1sNLcXVKdzpE05GU1NuKicnd+7wWDQCI1ubGw0NIlxKdzsq2t3ajKrVuPEp90wGYm9hCRfAvD+lNJCGs3SBwE8CeDPAPzo1jEfAfC5PVyjoKDgDmMvPoUvp5R+H8B/AjAE8DWMJP//BeCzKaV/uvXdb+ym/Sin36VU5OBTbj1DTpo55ra/1izwkJP+zc+1tbVGubIo3KfFSpwwtby83JBc6s/wMJS2rxJMsxeButSJagpof9k+x8KQoWpf7gyNskE3NzcbBV3UcRbRb91mbrfbjTCsbkXP67/wwgu5j9QUdEyRA9P/r3UmorCz05x1nN7+xsZG6AjknEXEOve7KA3ZfWY6V6qBRlR9fz72UldhT9GHqqp+CcAv2dfPAfiuvbQLoPHi6YTqxEaJSFowBIhVNVc1td21tbVGXFtVYmdW6osUxbwJffG8nqGe487CqI9qUriJo/1VOG/CxwCMHI1esUqZgR7Z6ff7jVR1XXijefDxadUp/vbyyy8DGDkVeU3O2blz53DPPfcA2M6RuHjxYmOOVN13PoHmq+jzNM5pqu2p8PAoVWRW6YLofBNN9PM5iMoF6OKni+u4nI2SOl1QULBnTG3ug6/A6kzR8JYXlVBJ7upYlPYclUbb3NwMHU1s0/cmmJ+fzyu1bnDimWqqVtOpGbEio4rTXs1Z2W7RHDnrs9PpNLSqiEUZmSCacu2Ozojjr8xGLzSjlZgjSU688srIP33u3Lm8GQzTwq9cuZL7xtDklStX8rNAB6aalO5kbbVatZqcHK9zUJS/4Q4+zU50LaLVauVngRrO3Nxco+SelsSjZqtt+IY/UahbTSF/bu8oo7GgoODbE1OrKRAqSX1V7na7jTCO5r07Ir+ErsaqKWjYST+j4q/q6ImKgHoYanZ2ttHvpaWlsTalElbUURZlxkVViHm8S2j1EahT1Flx6mh0ydXtdvPxrF6tZdnc/6KsQWJzc7N2//gdALz44ou5XTImr127lsdAco5u0xc5gt2xF2UbRrt0RTkvWubPfRDuIPdxelGd4XDYKCpLDIfDsL3I0eiZqvqO7BRTuyi4oydK6VXqqcd69Ti9Yc5AUwafVmrydvUBirgCkzzH3sbs7GxjwQCaSWD6wI1LrvJre1UjJgcNh8NG8ZEopVxNIV9cNzc383E8t9/v4/jx47WxRw5SVZvd+agvI19s5YKo+s12eRyTpObn5zP/wRO6VL1WIZfSiyEAACAASURBVBOldRNuUup3XhcUiLck8Oevqprbu+3bt6/xnS6a7qzUiIcKgHH1QHeDYj4UFBTUMLWaQlTkwldNjdWrA8nVR12xKS01dOOSa1KZrXGVniOTwlftKDxHidfpdGpJQNq+FnFRB2IUG9cdttkPoK5CR/sQRJqLI6onCGxrIy7lIwmt5+q1PCmI96nX62UTgdyEa9euNepwttvtRmq9Oi99rqINaRXOpdAQMNHpdBrji9LzVfPyRCVNG3duhO4iHoW/dUy+o/heUDSFgoKCGqZWU3CGlqbGqqMsIjQR7rCLwkrD4bBhW2oWo36n19F2VSJ6iFT7odeJQkcusfR4lw6aGUdnkobZqDGor8U1l3GS1ElfUdEPzd3we+UEHb+Wp0lroRuCfVhYWMi+gpMnT+bzfE+PiC2o2of7iNSZrGxVd7LqfHvKsjqdI63DHbXtdjtMEfcwueaLeGk+Da+q05fOWGpQe0HRFAoKCmqYWk2BHHiVYFGhEefnKw3Uvfma+09oaO9mhUB47WgjUA87ap58VDDGpcjc3FxNsurx2g/17DvxSIuJuGYR5XOoL0Rtbd+LU7Ump9NGFGLNt4hKmHkGp0ZNnF4ObOdB0Kdw1113NchFWkLP6cvqI4p2fFI/gPuS1B/gfglFFGmISq9FmsW4DWb1uCiTUwln1BBI/96Lb2FqFwVnEurLrip9VB3IQ4GqbvmDrsU29AFw7oI/cApdABR8OdxppaxBDbNF9fh4bX+5Z2dn87m6b0WUBwGMTAxn66W0Xb1Y++9OP61hyeszTFhVVcNc0PCjO8+igiDqxHNnaKvVqtWqBID77rsvcyH0PL+30T2LckcI5R34XOj83UrYLzJjo+pXar4Sapb6s6CVriKHuztZd5NCXcyHgoKCGqZWU3BH3ziiyCTnmTvF1EkT5RwQ6giM2nLVTE0KDZFGYUS9hkJJKdFWcq6xrK+vZyYcQ3ZHjhzJTjmGCVWTcm0jkvIppUY4UzNKPcdDGaSRY9IZjT4PPM7zLdRJTE3hiSeeAACcOXMGx44dq41T1Xttl/1xk0i3ltdr+RhUy4vSwKMiPPxNHZiES3414Vxj1ZCkam1RZmbkzN4tiqZQUFBQw9RrClq7f1I4jLZgr9fLqyUljJ5HyaIbd0Z2l0uFaPXWQiaRBqIOTiAOryoV1wu6qIbjfgmVwqwpcOjQodwu95Lkp2ZJKk+etrn2kVLV56Xf7+fveM2DBw/mMbCmAXeuWlhYqPkG2KYTclJKWfNgP0hfnpmZydrPc889l8f+6KOPAtjeR/PZZ59t5H1MohfrXKovYhyZS9uInLaESmi/5uzsbI0eDtTzdzwsq1psRP7SULRrCB5W3gmmflGIVGh+6k7Dqnq56qRqs+c+qBNPF6AovZf9claavuSREzRKXHI1cmNje7Mb9kMdg+7JbrVa2Sv/zW9+E8DoBWVdSqYZs43r1683ch60wIcunL6pS5S2y/b379+fF0Q6H3Wx1MQpnyuFM03pRWc72o+rV682qhVHyW7atm/402q1GlvPzc7ONqJeaka4EzK67wpN8QfqJqIKJY+q6WLl1cqrarvMvkZZ3HyYVMr+ZijmQ0FBQQ1Tqym4Cqibgqgq7yqarrxeck0lI6ErvF7LpQLbHAwGjVRXVYkpibT2Y1Q3z82etbW1hkmhDj+XuJqG+/zzz+f23VGnDlCfU5UiKok8dBmxADmn169fbzgr1TnmFZ61MIle2x2N5CRcvXq1IY2V6cf4/PLyciPESERclHHp7pETmce4dqf9dZNS+6vX4d9ReDPKwfAam6qx6IbEvj3fuEzXW0HRFAoKCmqYWk2B0JwDt/NV8isjznn/hJJvonx5l9D6nbLS3HmWUmqQdBQufdSZp9IkymLkMZHvhNekNI4INrqBrbMGo9x8lVbux9ByZWz3+PHjeecmSsunnnoKQF370ZoS7vCMnI/qTKNmRn/J1atX8eKLL9b62Ov1Qsch2/eK2lEdC3XeuhbhY2D7Uek89sH3sogKpaytrTW0L51390so2S561iLH7k4xtYuCx2xnZmYai0LEhNvY2GhEAghV3wh14qnK5RV21eHjNF224/1wNY/X0Y1FnD3o/fDfdJGkiv3II48AGDH9GG04d+4cgHqCjFdviiov6cISPVhMvCGL8vDhw40IzYkTJwBsl2TXNlRF1wXJ1XC2efr06Rx94Nhu3LiBr33ta7Xj+/1+YxdpfV6igjeRZ97vY4SIAh+p/v6cKMtU6ejOiYg2dIlS1vU3N79uZRzjUMyHgoKCGqZWU4jgjjJVidXR43kLWmPfVWjNqVDVyyWWhvHcpFDefaRuRkU0nJc/NzfXGIsnVAF19Zd/sxyaci4ordVR5deMeB7D4TCH+Tw9WVOQ2deVlZUG10GTrNxRpslJKtnd3KF6vW/fPpw6Ndqj+Pz58wBG95MakKr7DCd6P6ISeuqc0+M8tKfcEdegIslP6DNE6LOhfXRzVzfoca1EzRLVQDXUyu90fnaCoikUFBTUcFNNIaX0mwB+GMClqqretfXdEQC/C+AMgBcAfLiqqqtptCx9CsAPAVgF8A+qqvpPu+lYZO+5I1D9AdEGrJQcmjHm0lIlFzE7Oxv6KtiHaPX1kJRmTvI3zS+IyEgubdwJqP1hPwHgpZdeyu1zzJN8MtpHXpPnaT9degPbuQbqg3DCjF7HNQvVzNiG7sjlc3v06NFGP/SeRdKSklelpzsada713k2ywf1+atajH6Oh1yi1Xf0ePm+RT0yJb95HLdTiWuZucCuawm+hucX8xwF8oaqqswC+sPV/APhBAGe3/n0MwKd33bOCgoK3BDddTqqq+ouU0hn7+kMAvnfr788A+HMAv7D1/b+pRkvZl1JKh1JKJ6uqurDTjo3zpio0PKhEjnF59ZqRFnn0J5XqUgqthzNTSjXaKr/z1TsqrU5M8mMo0UY1mGvXrgFALVfBQ5KRr4WSNArLpZQabajUURIXj/FSZ4SSvKh9RDtbaY0FjpM7aPX7fdx9990AgPe85z0AgC9+8YvZp6AanGuGkxDNt0YkXFPVNjmuqqoaNTM0yqG0ec5BFPb2En5RG1HBGNU2PdKxF5/CbnWME/KivwbgxNbfpwC8LMed3/pux4uCLwYaQ+aD0O/3GxuoqNPKb5iGEyOegrbvFYC0SnJUeYeIkliiXAlPuOr3+42XRR9EH7uGshiyO3To0Ni8D1V1I9VSHzQP6UX7Q0QFYLiFG9tfWVnJ3IJoIRxXEEZ/GwwG2fH5zne+E8AoNPnMM8805sX5D9FcKZ/Ar6tp4M6X0HnV1Ha/lr7Efi90m0MNwXIBdCGysLAQ9sM5EZr7QESO71vFnqMPVVVVKaUdB0NTSh/DyMQoKCiYIux2UbhIsyCldBLApa3vXwFwWo67d+u7BqqqegzAYwAQLSruEGy1mvs5KMtRTQDd6Wnc8WpiuImg0j7itBPaRsQyiyQFUJfaVP2joixa3sxNHHWKaciVZozXe9R2CZXyVL11LwMnGem2caqtOUtQ2Zo8nv3pdrshE29cKbr19XW89tprALbv59mzZ3N6NjURHbszQ9kXbVfNKXUAe2hPtaUoC3Pc1vXqUGX7anpG5qtrpWtra5mgRszMzIQMSTe/PCy/E+w2JPl5AB/Z+vsjAD4n3//9NML7AVzfjT+hoKDgrcOthCR/ByOn4rGU0nkAvwTglwH8XkrpowBeBPDhrcP/GKNw5LMYhSR/arcdi0hGHt5S8o1y4McVXY3KoakNTehmpS4Fb0aAUikYOTCBkcSLHEKu9US1GTTz051JSuby2hNRLYkoYzEi8OhvUQiY7XpWarvdzm1pdqePU6W2F6ZZW1vLWgEdjvfddx9effVVANt5FhFpLNq/Um1+11i0XB+h4VB3TCoJLaqF4MVQtI+RtuQamvrAVMP1TMv19fXsl3A/UOSvuRluJfrw42N++mBwbAXgZ3bciwBR3oI7i1ZXVxuMvCg5KfLcTlIFdbdfXlPLafvLEi026ox0b/7MzEzjAZudnc0Pj9Yd5KePSdOpqVarqknvvBZs8cVJNydRFqKr8sox8N/m5uby32qC8JioTmaUEOXVsmgWnDp1Kv+tCwydj8pN8IVZn5eo5mfEIKVqzjHogutRITUHomiP32N1PkbCw6EOUkKfBUILxkRFc3aKwmgsKCioYepzH1QF5yrOsl+a5UdoHJyf6nDUDEFgJEWiVGUP8SgfIjILvECK/u6x6ajYi/bD1fzIxFFzQ4/n+KidqNnjTqjouKismXIBoqIiztnXsCWPowag90zVauc/8Lx9+/blzEzmPqyurmL//v2NPjpnRccRZQ/686GOSS/DFzFfVXtwfoOmu0c5D4Q+O4QW6ImKw3juiJp8jpL7UFBQsGdMraYQVV12Ka+EJkIr5vpKHZW+WlpaahQ80fCg5yNoOSy1Gd3OGwwGDSmi53nfVKp6HyPyjbYX2b3R2N2fopx57b/XkqBEmp2dzbZr5DPhnPGeKWtQ59j3pNC54ifJSYcOHcqVnV95ZRTdXlpayuOjxrC0tNTQMuhv0C3lVBvj75Hz0W35aHcndZB6DYThcNgoRru2thb6o7yNyOcT5XhoqNh9CCVLsqCg4E3D1GoKXHFpi25sbDTISyphVHuYVH3GSSxK1omIUk5OUQ+yU369Hx4W1JCTU7D1uGi1jyjHURhU50Z/UymoIUFeU+1YD4M5/VbnQ493u7bf79f24+AxDJ+5L0evybyOJ554AmfPnq2Npdfr5eiDaizjSqNppEHvo0cHNHIV0YP9/mi4L8qWjMoHRpsOjyMaafhW+6iRHx23/j1O47kVTO2iQERFQqh+alXfKK/AJ2Q4HDZU7ojrPzMz00ixVfUzistHVXc93q+qvN/Y69evN46Lwlf64EQc+MghSbgZ0+/38+9cRDQ8GJUT8xdZ7wHb4AurqerqlI3Snv04zQnxF25zczObMQzDzs3N5ecimh8PLatA0ZwQ5wpoZetxDlhg+1nQvAhni87OzuZQsb68Pj6971Htz6jIixYB0mNKSLKgoGDPmFpNwasGr66uNspWdTqdvBIqeYMhLK6elCBRNeJou/SIWKIru0vQlFJ2eCnxaNxeEFrQZFK1YCU7RSs++8T2tWiKSlq25aqlOjBVbXYNSzUvL4CqKeLuAOv1eg1SjTrPNLOQ7WmYjfPJdnlfr1y50iAjzc3N5VC1z6eyYVWSuiam9yJixXqWqZogbp7ofKtD0zWiSPOLWKvKnNR7yvM874PPOU21naBoCgUFBTVMraZAaJjLJag6kIiUUi4kyjLklD5K9dVVOQp1Os2Vx6h0UIem73fIEJxeS6WCF0WNimdoONRJL5qDodLMnZ+TwppKDSbUb+DQUKo6HSNasf4fiMuUqb3uzlvV5KhtUOppQVOOLyreEtGR1fnr9rc6XnU+2A/XhNTP4BqD2vsc+/LycqONqNQ8n6WFhYUGjVrvo4430i45pp1i6hcFrW/nL1eU6KKFMqJYfVT11iMMGxsb2Qzw1NUoCQto1sZTR5Y7LfWB0eiKq7F6nWjLMnc4ra+vN3ba1hfcH2CtNxlFaCJTIarH6JugTopWKI9EoaxJzh//zzHwXnQ6nWxKKCPTq1PptaPEL4ce4y+jJifxpV1cXAyP49idu6JJVfpi+0uui6sWCOJnFCEZ18ZuFoViPhQUFNQwtZqCl1TT1V9XT5c6GmZTNYznRbsTEapqRyq8HgPEoUB1WrqJoKnWvuX5YDDI2kkk1dShxmuy7wwBqorrTEUtMKOMQx+f8jZcA2i1WrmPyknQVHYgLvCizlnXQFZWVvI9ijI0PVX92LFj2bH7xhtv5D5GW7TzOux3tNdE5Ozz/Az9jtCaix6WVXi4WseuIXHfJFaZtZ6xqn2NHMC7SZkmiqZQUFBQw9RqCh6+U4muNpUzA9vtdsOmjWx5Xamd0agVoSc5o/yT7fG8qBYDP106qLbhWtLi4mIYDmOh1CNHjuS+8Vza3+xDv98Py5S5c06JWxyXSkuXemqvj3P0aT8ANLSNTqfTqA2hc0rCz7333tvovzI33eGp/igPP66trU3cS9LzP7S9iCUaFVvx50rnQO+1ayBRSbdImyE0TO7j/LZyNLoTTU0FrYjrjqlOp9OIzTptV79TamtUTYhQVTOq5qxcAaCenBTRlv3hSyk1Eot0Drxuo76o999/P4DRC3v16tV8fZ0XoJlANTMz03gJW61Wo9CInu9VizXZiP0lX0CvrUVIIo86+8b+MHK0srKSTQRllzLCpALAF3CtwRhV11KeCdsdV2BG74+aVeOqJUdFVqqqygucluUfl7QUldZXgaUmsG/gs5vFgCjmQ0FBQQ1TqykQGv/1reE0DKWqnbPdIl6Dahge856ZmWk44HwjFaBeeMUZbVrYxRl82ie2e+PGjVCVZ5vOu2+1Wjlp6Bvf+AaAkXSllOZvlG6rq6uhhuPMPU3D9RCjMgP5ubi4mE0VmnoXLmzX6o2KlXj+xMzMTG7jwQcfBLBt/ly5ciVLQXU6u4RWc033nQBG98IdiBruI6KSa5on4rUWVcK7aaGJeZrfEuUyuEZBqFmg3xF+vP4eOdJvFUVTKCgoqGFqNYWoGrHz+qMdfTTrzKsLKyPPiUV6LZUiUfZe5MiKCqREjkjCU4rVz+C2roam9Dse961vfSvPkWssxObmZnbwqU3PMVDD0H0ZODdRMRktr0b73/NQ1EaPMv9UsvO+kJT0tre9DQDw8ssv44UXXqj1Q0lrmt3p95vY2NhozK32ST/HFSlR0pXO8aRUZbfr1QmufgYPnavz3H1PUVblcDis+Zj0+N2gaAoFBQU1TK2mEJGSooITGnID6h5yD0kOh8PGrlGa56B8cZfuakd67oMWL9UwFPvp+wcC22E59Z5zDFz1tf+ew69ZeCqtPPylmpbnF3Q6nQZhRot4RLUTPG9/ZmamUV5NyU/jCscAdaq0al3sGzDSHHzXKw3jacjaN2rVcm9evyKy1xVOAlNNjtCxuP+l3W6HxXvcZ6F1N8YVq/G5cu1EQ+2TyHm3iqldFHxrLA3/6HZqkXrF73Q7MB4TOXX8gR0XCmJbkcPTQ6Ozs7ONAi1RzoSq3FFRGLblocCUUuYpnDlzJl/79ddfrx2noTtdINiGLwD6gk7iY1DVnZ+fzy8tWYZ0cl67dq0Rrux2uzWVHxi9QFysyc5UliT7potrxPr0xKko4SpaCFTYuBmgpoIvcJqk54JCGaSTnKzRS67CSU1f7/8kRua4GqO3gmI+FBQU1HAr28b9JoAfBnCpqqp3bX33zwH8XQADAN8C8FNVVV3b+u0TAD4KYAPAz1ZV9Se76ViUKuyZdJo5GaW4uoTWlVdXVycNaYZbFJZzadJqtbKkU5Xf1UHPyeC42AfPhFQtycuDzczM4MSJEwCARx99NP9GpyP7Rql948aNhvRTU4vo9XphtWpgJOVpZnCOjxw50iB/qYlGTUXDt1plmd9RQ+An+93r9XIb/Dx9+nQ2WagZaT1IQsfpGqWSnW5FMqsG6pmI2oayVqOq4q6pKFHOTTkNAUfp8aqJugOdiMLQN8OtaAq/BeAH7Ls/BfCuqqr+CwDPAPjEVsffAeDHALxz65z/NaW0c6OmoKDgLcOt7CX5FymlM/bdf5D/fgnAj279/SEAn62qqg/g+ZTSswC+C8AXd9oxX1FnZmYaGkBU1FWlX1QlNyrmETkwndIa2ZaaGenZcWqfRpLFnaAaliOU7uyZh1qK7tlnn81z5FmlbOPYsWMTHWVak8GlHv+/sLCQ22AIc2lpqbGJrBa38WKqg8GglqnI86j1sC1qClVV5b0kSUa6du1aPvf69et5LON8SUpsivw5ej/9nmmoO9qDwans6pT137TIrYaH3bHI47XISlQ9W0Ok7u8YR7++FbwZjsafBvC7W3+fwmiRIM5vfbdjuCdbmXZR9Rx9gKMkEqBe5ILQhzQqqOHOP72B6uH3LdnUsedp2MPhsPHgqjPMWZe6EGn0gTkBaibxYaOqTXVcN3Ih9KHW6InnnfC3tbW1hmdfHWpckLgoHD9+PL+07lzUsc/MzDQ25FHmJn/j7tNXr17NG8Ro/yNeAOfbHY36kmny07iIhKr5uphExXX4fy/6onklery/wFpf0Z3f2r+o8pL2V9vaCfa0KKSUfhHAEMBv7+LcjwH42F6uX1BQ8OZj14tCSukfYOSA/GC1vUS/AuC0HHbv1ncNVFX1GIDHttpqpHS5ahcxxaKYd7TSa/Vbl36bm5uNLdQHg0GtcIm2H4X2FKq5OCNQJapKX/YnMmP4mzrleB7HTxbgwsJCgxOhtSM9RKqlziIJ5P1RUyAKg3n46+DBg7j77rsBbG/51ul0cjvs2/79+/N31AC0kM6pUyNl8+LFi7nffg+0YIzX3Gy32405VQmtIUbfFk9LoznTU8ccaZRRTVHXBjVvhtAiK34vVCtVuMbnLMmdYFchyZTSDwD4eQA/UlWVGo2fB/BjKaVOSukBAGcB/H+7uUZBQcFbg1sJSf4OgO8FcCyldB7AL2EUbegA+NOtle9LVVX991VV/U1K6fcAPImRWfEzVVXtyuMxrhYBUC/OoTvyAHEBkWhrb90LIuLFK/FFr6/2fZT9qM5BOsiittwXEuXya//pZNPrOIuy3W432H8RNKwZFSZxiRttGKuS0UORlHQLCwuNnIarV6822IKXL1/GXXfdBQA4efIkgFHOAzDyS/BcnreystJ4PlSD88KmSiQjNL9FiWfeN0L9UVFegRf61b09NMPWi/hGVai1354dq33TsLlr1JO2v7sZbiX68OPB178x4fh/BuCf7bgnBn+h+/1+I47fbrcbMfXFxcXGPorqpOPNihiKGm/3GLMm23jkoNPp5Gvw5VVeBdvQ33yc7XY73HqO0BRetslxst2DBw82HJ6Emiwaq/fjVK2m6qnzGCWI+UOtpg5ZjnQ4RklbGxsb2TR4+OGHa7+trq7mF+3QoUMARi9clPjlDmZ1IHofdaG4lRdHnbLarjsTI0qzshg9qUojXYSaCl4tS00hQhfoqAL3TlEYjQUFBTVMbe6Dr8oR310ZYqo+udNHVXWXjFEyTsR8pIahyVIqcb2SsYawvB9qCqlJQfXbWZR6PDUWLcpCKXzs2LF8bMSw9Fi6Ouc8YUj7plyNcfkZ2kYUZlVmXeQoplbCvrHE3NLSUs7xePvb3w4AuHTpUsOZqOaUS+h2ux1yASKJGxVQ8b6qFHaNUvkvUe1FT/9XjcI1Fk3TVy1Id0znNaPiLXreTlA0hYKCghqmVlNwiaREJXUcuvNHN/Ek1JkWkYCia/M4dW6xH+6sVCdWlNlIqaYOJA/3KdzZpbZoZCsqA9G3RFeyjBdzVV9IVHLNJf/mZrxTlecL8JqaQUkNajAYNFJ+VQtk+yRmvfbaa7kN5ngsLS3hySefrI19bW0t9Dnx/5xvLUbic5mkyIqXV5ubm2uwRVUrde0kKgyrxXJ87nTsGl70VOtIY9bsS3/2d1PAtWgKBQUFNUytpuAhO5XQak+O8zgD29KJkkYJKFpU0ynVkyjHaheqJHCpoFvcM09AV31eX21Bz7XXufAwqIZG6e/wklzavo4vKh2vG+h67ohSzZ3spNqGl2O7du1aDstGeSrUWObn5xt7fHLOhsMhXn31VQDb9RpOnz6dKc9aP0I1IO+/ZqOy//7MqM/EI0c6ZtWunCzm+Rf63dzcXBhpcMmvxCOnOWvtjmjPEG2XY98p0l7qw79ZiBiNBQUFbzoer6rqfTc7qJgPBQUFNUyL+XAFwMrW51uNYyj9UJR+1PGfcz/uv5WDpsJ8AICU0ldvRbUp/Sj9KP24vf0o5kNBQUENZVEoKCioYZoWhcfe6g5sofSjjtKPOr7t+zE1PoWCgoLpwDRpCgUFBVOAqVgUUko/kFI6l1J6NqX08Tt0zdMppT9LKT2ZUvqblNLPbX1/JKX0pymlb259Hr5D/ZlJKX0tpfRHW/9/IKX05a05+d2U0tzN2ngT+nAopfT7KaWnU0pPpZQ+8FbMR0rpH23dk2+klH4npdS9U/ORUvrNlNKllNI35LtwDtII/8tWn76eUvrO29yPf751b76eUvo/U0qH5LdPbPXjXErp7+zl2m/5opBG+0L8KwA/COAdAH48jfaPuN0YAvjHVVW9A8D7AfzM1nU/DuALVVWdBfCFrf/fCfwcgKfk/58E8CtVVT0M4CpGG+zcbnwKwL+vquptAN6z1Z87Oh8ppVMAfhbA+7Y2H5rBaC+ROzUfv4XmPifj5uAHMSo5eBajIsSfvs39uDP7rTDT6636B+ADAP5E/v8JAJ94C/rxOQB/G8A5ACe3vjsJ4NwduPa9GD1s3w/gjwAkjIgps9Ec3aY+HATwPLb8TPL9HZ0PjLYEeBnAEYzIdX8E4O/cyfkAcAbAN242BwD+NwA/Hh13O/phv/03AH576+/aOwPgTwB8YLfXfcs1BWw/BMSu94rYLdJos5vvAPBlACeqqrqw9dNrAE7cgS78KkaFcJlHexTAtaqqmJ98J+bkAQCXAfzrLTPm11NKi7jD81FV1SsA/gWAlwBcAHAdwOO48/OhGDcHb+Wz+9MA/u/b0Y9pWBTeUqSU9gH4twD+YVVVS/pbNVp2b2t4JqXEfTofv53XuQXMAvhOAJ+uquo7MKKd10yFOzQfhzHaaewBAPcAWERTjX7LcCfm4GZIe9hv5VYwDYvCLe8V8WYjpdTGaEH47aqq/mDr64sppZNbv58EcOk2d+N7APxISukFAJ/FyIT4FIBDKSXmptyJOTkP4HxVVV/e+v/vY7RI3On5+FsAnq+q6nJVVesA/gCjObrT86EYNwd3/NlN2/ut/MTWAvWm92MaFoWvADi75V2ew8hh8vnbfdE0SmL/DQBPVVX1L+WnzwP4yNbfH8HI13DbsTRDjgAAASdJREFUUFXVJ6qqureqqjMYjf0/VlX1EwD+DNt7dN6JfrwG4OWU0qNbX30Qo1L9d3Q+MDIb3p9SWti6R+zHHZ0Pw7g5+DyAv78VhXg/gOtiZrzpSHdqv5Xb6TTagUPlhzDypn4LwC/eoWv+lxipgV8H8Ndb/34II3v+CwC+CeD/AXDkDs7D9wL4o62/H9y6sc8C+D8AdO7A9d8L4Ktbc/LvABx+K+YDwP8M4GkA3wDwv2O0x8gdmQ8Av4ORL2MdI+3po+PmACOH8L/aem6fwChicjv78SxGvgM+r78mx//iVj/OAfjBvVy7MBoLCgpqmAbzoaCgYIpQFoWCgoIayqJQUFBQQ1kUCgoKaiiLQkFBQQ1lUSgoKKihLAoFBQU1lEWhoKCghv8f50YJyvTqrU8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generator(batch_size, filenames):\n",
    "    size = 128\n",
    "    shape = (batch_size, size, size, size, 1)\n",
    "\n",
    "    x_out = np.zeros(shape)\n",
    "    y_out = np.zeros(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        filename = random.choice(filenames)\n",
    "        #filename = filenames[0]\n",
    "        x = np.load(data_path.format('x') + filename)\n",
    "        y = np.load(data_path.format('y') + filename)\n",
    "        x = np.reshape(x, (size, size, size, -1))\n",
    "        y = np.reshape(y, (size, size, size, -1))\n",
    "        #x_out[i] = x\n",
    "        #y_out[i] = y\n",
    "    \n",
    "        # bias\n",
    "        bias = 3\n",
    "        x_bias, y_bias, z_bias = random.randint(-bias,bias), random.randint(-bias,bias), random.randint(-bias,bias)\n",
    "        x_range = (x_bias, 127) if x_bias > 0 else (0, size + x_bias -1)\n",
    "        y_range = (y_bias, 127) if y_bias > 0 else (0, size + y_bias -1)\n",
    "        z_range = (z_bias, 127) if z_bias > 0 else (0, size + z_bias -1)\n",
    "        \n",
    "        x_out[i,x_range[0]:x_range[1],y_range[0]:y_range[1],z_range[0]:z_range[1],:] = x[x_range[0]:x_range[1],y_range[0]:y_range[1],z_range[0]:z_range[1]]\n",
    "    \n",
    "    \n",
    "\n",
    "    #yield (x_out, y_out)\n",
    "    x_out = np.reshape(x_out, (128, 128, 128))\n",
    "    plt.imshow(x_out[:,:,65], cmap='gray')\n",
    "        \n",
    "generator(1, training_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(random.randint(-2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 125)\n"
     ]
    }
   ],
   "source": [
    "    x_bias, y_bias, z_bias = random.randint(-2,2), random.randint(-2,2), random.randint(-2,2)\n",
    "    x_range = (x_bias, 127) if x_bias > 0 else (0, size + x_bias -1)\n",
    "    y_range = (y_bias, 127) if y_bias > 0 else (0, size + y_bias -1)\n",
    "    z_range = (z_bias, 127) if z_bias > 0 else (0, size + z_bias -1)\n",
    "    print(x_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
