{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libs loaded\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "import h5py\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import walk\n",
    "import random\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "def setGPU():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    config = tf.ConfigProto()\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))\n",
    "setGPU()\n",
    "\n",
    "print('libs loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case10_label11.npy', 'case6_label11.npy', 'case8_label11.npy', 'case10_label6.npy', 'case9_label19.npy', 'case2_label13.npy', 'case9_label7.npy', 'case5_label3.npy', 'case6_label15.npy', 'case6_label8.npy', 'case10_label4.npy', 'case9_label13.npy', 'case3_label6.npy', 'case2_label5.npy', 'case9_label12.npy', 'case6_label9.npy', 'case9_label17.npy', 'case9_label10.npy', 'case8_label12.npy', 'case8_label4.npy', 'case4_label14.npy', 'case9_label5.npy', 'case4_label6.npy', 'case8_label5.npy', 'case5_label8.npy', 'case4_label3.npy', 'case8_label9.npy', 'case8_label18.npy', 'case8_label15.npy', 'case6_label19.npy', 'case8_label10.npy', 'case9_label8.npy', 'case4_label11.npy', 'case5_label17.npy', 'case4_label12.npy', 'case3_label16.npy', 'case2_label8.npy', 'case2_label7.npy', 'case8_label19.npy', 'case3_label7.npy', 'case8_label8.npy', 'case4_label19.npy', 'case3_label15.npy', 'case6_label6.npy', 'case5_label18.npy', 'case4_label15.npy', 'case5_label5.npy', 'case4_label4.npy', 'case5_label2.npy', 'case3_label14.npy', 'case3_label13.npy', 'case10_label19.npy', 'case6_label5.npy', 'case10_label18.npy', 'case8_label3.npy', 'case3_label11.npy', 'case6_label18.npy', 'case4_label1.npy', 'case10_label5.npy', 'case4_label8.npy', 'case5_label12.npy', 'case8_label7.npy', 'case5_label10.npy', 'case5_label4.npy', 'case8_label14.npy', 'case3_label10.npy', 'case6_label4.npy', 'case4_label13.npy', 'case6_label16.npy', 'case8_label16.npy', 'case5_label19.npy', 'case2_label12.npy', 'case5_label11.npy', 'case2_label6.npy', 'case3_label8.npy', 'case5_label13.npy', 'case10_label17.npy', 'case6_label13.npy', 'case5_label14.npy', 'case2_label9.npy', 'case9_label2.npy', 'case10_label13.npy', 'case3_label5.npy', 'case9_label11.npy', 'case2_label4.npy', 'case8_label2.npy', 'case2_label18.npy', 'case10_label14.npy', 'case2_label3.npy', 'case10_label15.npy', 'case10_label3.npy', 'case5_label6.npy', 'case5_label7.npy', 'case2_label17.npy', 'case10_label8.npy', 'case2_label19.npy', 'case5_label16.npy', 'case4_label2.npy', 'case2_label10.npy', 'case3_label17.npy', 'case3_label3.npy', 'case9_label3.npy', 'case2_label16.npy', 'case9_label16.npy', 'case4_label7.npy', 'case6_label3.npy', 'case10_label12.npy', 'case4_label10.npy', 'case3_label9.npy', 'case6_label17.npy', 'case6_label7.npy', 'case2_label15.npy', 'case6_label10.npy', 'case2_label11.npy', 'case3_label18.npy', 'case9_label9.npy', 'case8_label6.npy', 'case10_label10.npy', 'case3_label19.npy', 'case6_label12.npy', 'case4_label9.npy', 'case8_label17.npy', 'case10_label9.npy', 'case9_label15.npy', 'case10_label16.npy', 'case8_label13.npy', 'case6_label14.npy', 'case9_label14.npy', 'case9_label18.npy', 'case5_label15.npy', 'case4_label5.npy', 'case3_label4.npy', 'case4_label16.npy', 'case9_label6.npy', 'case10_label7.npy', 'case2_label14.npy', 'case5_label9.npy', 'case3_label12.npy', 'case4_label17.npy', 'case9_label4.npy', 'case4_label18.npy', 'case10_label11.npy', 'case6_label11.npy', 'case8_label11.npy', 'case10_label6.npy', 'case9_label19.npy', 'case2_label13.npy', 'case9_label7.npy', 'case5_label3.npy', 'case6_label15.npy', 'case6_label8.npy', 'case10_label4.npy', 'case9_label13.npy', 'case3_label6.npy', 'case2_label5.npy', 'case9_label12.npy', 'case6_label9.npy', 'case9_label17.npy', 'case9_label10.npy', 'case8_label12.npy', 'case8_label4.npy', 'case4_label14.npy', 'case9_label5.npy', 'case4_label6.npy', 'case8_label5.npy', 'case5_label8.npy', 'case4_label3.npy', 'case8_label9.npy', 'case8_label18.npy', 'case8_label15.npy', 'case6_label19.npy', 'case8_label10.npy', 'case9_label8.npy', 'case4_label11.npy', 'case5_label17.npy', 'case4_label12.npy', 'case3_label16.npy', 'case2_label8.npy', 'case2_label7.npy', 'case8_label19.npy', 'case3_label7.npy', 'case8_label8.npy', 'case4_label19.npy', 'case3_label15.npy', 'case6_label6.npy', 'case5_label18.npy', 'case4_label15.npy', 'case5_label5.npy', 'case4_label4.npy', 'case5_label2.npy', 'case3_label14.npy', 'case3_label13.npy', 'case10_label19.npy', 'case6_label5.npy', 'case10_label18.npy', 'case8_label3.npy', 'case3_label11.npy', 'case6_label18.npy', 'case4_label1.npy', 'case10_label5.npy', 'case4_label8.npy', 'case5_label12.npy', 'case8_label7.npy', 'case5_label10.npy', 'case5_label4.npy', 'case8_label14.npy', 'case3_label10.npy', 'case6_label4.npy', 'case4_label13.npy', 'case6_label16.npy', 'case8_label16.npy', 'case5_label19.npy', 'case2_label12.npy', 'case5_label11.npy', 'case2_label6.npy', 'case3_label8.npy', 'case5_label13.npy', 'case10_label17.npy', 'case6_label13.npy', 'case5_label14.npy', 'case2_label9.npy', 'case9_label2.npy', 'case10_label13.npy', 'case3_label5.npy', 'case9_label11.npy', 'case2_label4.npy', 'case8_label2.npy', 'case2_label18.npy', 'case10_label14.npy', 'case2_label3.npy', 'case10_label15.npy', 'case10_label3.npy', 'case5_label6.npy', 'case5_label7.npy', 'case2_label17.npy', 'case10_label8.npy', 'case2_label19.npy', 'case5_label16.npy', 'case4_label2.npy', 'case2_label10.npy', 'case3_label17.npy', 'case3_label3.npy', 'case9_label3.npy', 'case2_label16.npy', 'case9_label16.npy', 'case4_label7.npy', 'case6_label3.npy', 'case10_label12.npy', 'case4_label10.npy', 'case3_label9.npy', 'case6_label17.npy', 'case6_label7.npy', 'case2_label15.npy', 'case6_label10.npy', 'case2_label11.npy', 'case3_label18.npy', 'case9_label9.npy', 'case8_label6.npy', 'case10_label10.npy', 'case3_label19.npy', 'case6_label12.npy', 'case4_label9.npy', 'case8_label17.npy', 'case10_label9.npy', 'case9_label15.npy', 'case10_label16.npy', 'case8_label13.npy', 'case6_label14.npy', 'case9_label14.npy', 'case9_label18.npy', 'case5_label15.npy', 'case4_label5.npy', 'case3_label4.npy', 'case4_label16.npy', 'case9_label6.npy', 'case10_label7.npy', 'case2_label14.npy', 'case5_label9.npy', 'case3_label12.npy', 'case4_label17.npy', 'case9_label4.npy', 'case4_label18.npy']\n",
      "['case1_label9.npy', 'case1_label6.npy', 'case1_label4.npy', 'case1_label19.npy', 'case1_label18.npy', 'case1_label3.npy', 'case1_label10.npy', 'case1_label2.npy', 'case1_label8.npy', 'case1_label17.npy', 'case1_label14.npy', 'case1_label16.npy', 'case1_label5.npy', 'case1_label15.npy', 'case1_label12.npy', 'case1_label11.npy', 'case1_label7.npy', 'case1_label13.npy', 'case1_label9.npy', 'case1_label6.npy', 'case1_label4.npy', 'case1_label19.npy', 'case1_label18.npy', 'case1_label3.npy', 'case1_label10.npy', 'case1_label2.npy', 'case1_label8.npy', 'case1_label17.npy', 'case1_label14.npy', 'case1_label16.npy', 'case1_label5.npy', 'case1_label15.npy', 'case1_label12.npy', 'case1_label11.npy', 'case1_label7.npy', 'case1_label13.npy']\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "data_path = '../data/seg_data/{}/'\n",
    "size = 128\n",
    "\n",
    "def load_filenames(path):\n",
    "    filenames = []\n",
    "    for (dirpath, dirnames, filenames) in walk(data_path.format('x')):\n",
    "        filenames.extend(filenames)\n",
    "        break\n",
    "    for f in filenames:\n",
    "        if not '.npy' in f:\n",
    "            del f\n",
    "    training_filenames = []\n",
    "    validation_filenames = []\n",
    "    for f in filenames:\n",
    "        if 'case1_' in f:\n",
    "            validation_filenames.append(f)\n",
    "        else:\n",
    "            training_filenames.append(f)\n",
    "            \n",
    "            \n",
    "    return training_filenames, validation_filenames\n",
    "    \n",
    "training_filenames, validation_filenames = load_filenames(data_path)\n",
    "\n",
    "print(training_filenames)\n",
    "print(validation_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 128, 128, 128, 1)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 128, 128, 128, 8)  224       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 128, 128, 128, 8)  32        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128, 128, 128, 8)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 128, 128, 128, 8)  1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 128, 128, 128, 8)  32        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128, 128, 128, 8)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 64, 64, 64, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 64, 64, 64, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64, 64, 64, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 64, 64, 64, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 64, 64, 64, 16)    6928      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 64, 64, 64, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 64, 64, 64, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 32, 32, 32, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_25 (Conv3D)           (None, 32, 32, 32, 32)    13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 32, 32, 32, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 32, 32, 32, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_26 (Conv3D)           (None, 32, 32, 32, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 32, 32, 32, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 32, 32, 32, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 16, 16, 16, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 16, 16, 16, 64)    55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 16, 16, 16, 64)    256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16, 16, 16, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 16, 16, 16, 64)    110656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 16, 16, 16, 64)    256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16, 16, 16, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 8, 8, 8, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_29 (Conv3D)           (None, 8, 8, 8, 128)      221312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 8, 8, 8, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 8, 8, 8, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 8, 8, 8, 128)      442496    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 8, 8, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8, 8, 8, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                1310740   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                420       \n",
      "=================================================================\n",
      "Total params: 2,196,864\n",
      "Trainable params: 2,195,872\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def model():\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, Concatenate, BatchNormalization, Activation, Dense, Flatten\n",
    "    \n",
    "    def conv(input_tensor, depth):\n",
    "        conv_tensor = Conv3D(depth, 3, padding = 'same', kernel_initializer = 'he_normal')(input_tensor)\n",
    "        bn = BatchNormalization()(conv_tensor) # use_bias=False\n",
    "        output_tensor = Activation('relu')(bn)\n",
    "        return output_tensor\n",
    "    \n",
    "    def pool(input_tensor):\n",
    "        output_tensor = MaxPooling3D(pool_size=(2, 2, 2))(input_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "    def up(input_tensor):\n",
    "        output_tensor = UpSampling3D(size=(2, 2, 2))(input_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "    def skip(input_tensor1, input_tensor2):\n",
    "        output_tensor = Concatenate(axis=-1)([input_tensor1, input_tensor2])\n",
    "        return output_tensor\n",
    "    \n",
    "    input_img = Input(shape=(size, size, size, 1))\n",
    "    \n",
    "    conv1 = conv(input_img, 8)\n",
    "    conv2 = conv(conv1, 8)\n",
    "    \n",
    "    pool1 = pool(conv2)\n",
    "    \n",
    "    conv3 = conv(pool1, 16)\n",
    "    conv4 = conv(conv3, 16)\n",
    "    \n",
    "    pool2 = pool(conv4)\n",
    "    \n",
    "    conv5 = conv(pool2, 32)\n",
    "    conv6 = conv(conv5, 32)\n",
    "    \n",
    "    pool3 = pool(conv6)\n",
    "    \n",
    "    conv7 = conv(pool3, 64)\n",
    "    conv8 = conv(conv7, 64)\n",
    "\n",
    "    pool4 = pool(conv8)\n",
    "    \n",
    "    conv9 = conv(pool4, 128)\n",
    "    conv10 = conv(conv9, 128)\n",
    "    \n",
    "    flat = Flatten()(conv10)\n",
    "    \n",
    "    dense1 = Dense(units = 20, activation = 'relu')(flat)\n",
    "    \n",
    "    output_cat = Dense(units = 20, activation = 'softmax')(dense1)\n",
    "\n",
    "    \n",
    "    # model\n",
    "    model = Model(inputs=input_img, outputs=output_cat)\n",
    "    print (model.output_shape)\n",
    "\n",
    "    # optimizer\n",
    "    opt = keras.optimizers.Adam(lr=1e-4)#32-5,16-3\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generator(batch_size, filenames):\n",
    "    while 1:\n",
    "        shape = (batch_size, size, size, size, 1)\n",
    "        shape2 = (batch_size, 20)\n",
    "        \n",
    "        x_out = np.zeros(shape)\n",
    "        y_out2 = np.zeros(shape2)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            filename = random.choice(filenames)\n",
    "            x = np.load(data_path.format('y') + filename)\n",
    "            #print(filename)\n",
    "            x = np.reshape(x, (size, size, size, -1))\n",
    "            \n",
    "            x_out[i] = x\n",
    "            \n",
    "            label = int(re.sub(\"\\D\", \"\", filename.split('.')[0].split('_')[1]))\n",
    "            label_one_hot = [0] * 20\n",
    "            label_one_hot[label] = 1\n",
    "            \n",
    "            y_out2[i] = label_one_hot\n",
    "            #print(label_one_hot)\n",
    "            \n",
    "        yield (x_out, y_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6821 - acc: 0.1111Epoch 00001: val_loss improved from inf to 2.98702, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.7083 - acc: 0.1000 - val_loss: 2.9870 - val_acc: 0.0938\n",
      "Epoch 2/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6262 - acc: 0.1111Epoch 00002: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.6376 - acc: 0.1250 - val_loss: 3.0063 - val_acc: 0.0250\n",
      "Epoch 3/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5205 - acc: 0.2222Epoch 00003: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.5368 - acc: 0.2125 - val_loss: 3.0059 - val_acc: 0.0625\n",
      "Epoch 4/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5626 - acc: 0.1528Epoch 00004: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.5617 - acc: 0.1500 - val_loss: 3.0074 - val_acc: 0.0437\n",
      "Epoch 5/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6361 - acc: 0.1944Epoch 00005: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.6416 - acc: 0.1875 - val_loss: 2.9952 - val_acc: 0.0500\n",
      "Epoch 6/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6513 - acc: 0.1389Epoch 00006: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.6596 - acc: 0.1250 - val_loss: 3.0046 - val_acc: 0.0437\n",
      "Epoch 7/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3794 - acc: 0.1944Epoch 00007: val_loss improved from 2.98702 to 2.98400, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.3682 - acc: 0.2125 - val_loss: 2.9840 - val_acc: 0.0625\n",
      "Epoch 8/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5043 - acc: 0.2639Epoch 00008: val_loss improved from 2.98400 to 2.97227, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.5473 - acc: 0.2375 - val_loss: 2.9723 - val_acc: 0.0500\n",
      "Epoch 9/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6501 - acc: 0.2222Epoch 00009: val_loss improved from 2.97227 to 2.96454, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.6397 - acc: 0.2000 - val_loss: 2.9645 - val_acc: 0.0563\n",
      "Epoch 10/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5346 - acc: 0.2222Epoch 00010: val_loss improved from 2.96454 to 2.95686, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.5285 - acc: 0.2250 - val_loss: 2.9569 - val_acc: 0.0375\n",
      "Epoch 11/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5146 - acc: 0.2222Epoch 00011: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.4974 - acc: 0.2125 - val_loss: 2.9750 - val_acc: 0.0437\n",
      "Epoch 12/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2812 - acc: 0.3056Epoch 00012: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.2792 - acc: 0.3000 - val_loss: 2.9620 - val_acc: 0.0437\n",
      "Epoch 13/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4369 - acc: 0.2361Epoch 00013: val_loss improved from 2.95686 to 2.95132, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.4885 - acc: 0.2250 - val_loss: 2.9513 - val_acc: 0.0563\n",
      "Epoch 14/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5559 - acc: 0.2222Epoch 00014: val_loss improved from 2.95132 to 2.95000, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.5690 - acc: 0.2625 - val_loss: 2.9500 - val_acc: 0.0375\n",
      "Epoch 15/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2152 - acc: 0.2778Epoch 00015: val_loss improved from 2.95000 to 2.93941, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 13s 1s/step - loss: 2.2620 - acc: 0.2625 - val_loss: 2.9394 - val_acc: 0.0625\n",
      "Epoch 16/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3474 - acc: 0.2083Epoch 00016: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.4070 - acc: 0.1875 - val_loss: 2.9572 - val_acc: 0.0187\n",
      "Epoch 17/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3408 - acc: 0.1667Epoch 00017: val_loss improved from 2.93941 to 2.91186, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.4007 - acc: 0.1500 - val_loss: 2.9119 - val_acc: 0.0625\n",
      "Epoch 18/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3334 - acc: 0.2639Epoch 00018: val_loss improved from 2.91186 to 2.90348, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.3422 - acc: 0.2500 - val_loss: 2.9035 - val_acc: 0.0375\n",
      "Epoch 19/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2773 - acc: 0.2917Epoch 00019: val_loss improved from 2.90348 to 2.85578, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.3461 - acc: 0.2750 - val_loss: 2.8558 - val_acc: 0.2188\n",
      "Epoch 20/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4328 - acc: 0.1667Epoch 00020: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.4080 - acc: 0.1625 - val_loss: 2.8787 - val_acc: 0.1187\n",
      "Epoch 21/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3005 - acc: 0.3333Epoch 00021: val_loss improved from 2.85578 to 2.83399, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.2847 - acc: 0.3375 - val_loss: 2.8340 - val_acc: 0.2500\n",
      "Epoch 22/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1946 - acc: 0.3194Epoch 00022: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.2251 - acc: 0.3125 - val_loss: 2.9011 - val_acc: 0.1000\n",
      "Epoch 23/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5313 - acc: 0.2222Epoch 00023: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.5464 - acc: 0.2000 - val_loss: 2.8404 - val_acc: 0.2062\n",
      "Epoch 24/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4338 - acc: 0.1944Epoch 00024: val_loss improved from 2.83399 to 2.79487, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.4257 - acc: 0.1875 - val_loss: 2.7949 - val_acc: 0.1250\n",
      "Epoch 25/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4397 - acc: 0.1944Epoch 00025: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.4102 - acc: 0.1875 - val_loss: 2.8050 - val_acc: 0.2250\n",
      "Epoch 26/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2539 - acc: 0.2500Epoch 00026: val_loss improved from 2.79487 to 2.63043, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.2200 - acc: 0.2625 - val_loss: 2.6304 - val_acc: 0.3187\n",
      "Epoch 27/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9765 - acc: 0.3611Epoch 00027: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9954 - acc: 0.3625 - val_loss: 2.7127 - val_acc: 0.2750\n",
      "Epoch 28/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2274 - acc: 0.2500Epoch 00028: val_loss improved from 2.63043 to 2.61383, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.2043 - acc: 0.2875 - val_loss: 2.6138 - val_acc: 0.2625\n",
      "Epoch 29/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1537 - acc: 0.2778Epoch 00029: val_loss improved from 2.61383 to 2.59898, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1142 - acc: 0.3000 - val_loss: 2.5990 - val_acc: 0.2938\n",
      "Epoch 30/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0748 - acc: 0.4167Epoch 00030: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1506 - acc: 0.3750 - val_loss: 2.7331 - val_acc: 0.2562\n",
      "Epoch 31/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2642 - acc: 0.3333Epoch 00031: val_loss improved from 2.59898 to 2.57404, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.2504 - acc: 0.3250 - val_loss: 2.5740 - val_acc: 0.3187\n",
      "Epoch 32/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1053 - acc: 0.4028Epoch 00032: val_loss improved from 2.57404 to 2.54068, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0739 - acc: 0.4000 - val_loss: 2.5407 - val_acc: 0.3375\n",
      "Epoch 33/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1272 - acc: 0.3333Epoch 00033: val_loss improved from 2.54068 to 2.48991, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1562 - acc: 0.3375 - val_loss: 2.4899 - val_acc: 0.3875\n",
      "Epoch 34/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1149 - acc: 0.3750Epoch 00034: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0482 - acc: 0.4000 - val_loss: 2.5986 - val_acc: 0.2000\n",
      "Epoch 35/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1101 - acc: 0.3611Epoch 00035: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1325 - acc: 0.3500 - val_loss: 2.5401 - val_acc: 0.2687\n",
      "Epoch 36/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0367 - acc: 0.3056Epoch 00036: val_loss improved from 2.48991 to 2.45024, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0294 - acc: 0.3125 - val_loss: 2.4502 - val_acc: 0.3250\n",
      "Epoch 37/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0078 - acc: 0.3611Epoch 00037: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0188 - acc: 0.3500 - val_loss: 2.5163 - val_acc: 0.2687\n",
      "Epoch 38/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2266 - acc: 0.2917Epoch 00038: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1934 - acc: 0.3000 - val_loss: 2.4544 - val_acc: 0.3625\n",
      "Epoch 39/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8385 - acc: 0.4306Epoch 00039: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7455 - acc: 0.4625 - val_loss: 2.5374 - val_acc: 0.2500\n",
      "Epoch 40/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0435 - acc: 0.4028Epoch 00040: val_loss improved from 2.45024 to 2.43649, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0477 - acc: 0.3875 - val_loss: 2.4365 - val_acc: 0.2625\n",
      "Epoch 41/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8701 - acc: 0.4167Epoch 00041: val_loss improved from 2.43649 to 2.41070, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9024 - acc: 0.4000 - val_loss: 2.4107 - val_acc: 0.2938\n",
      "Epoch 42/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1434 - acc: 0.3194Epoch 00042: val_loss improved from 2.41070 to 2.29833, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1458 - acc: 0.3125 - val_loss: 2.2983 - val_acc: 0.3625\n",
      "Epoch 43/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9262 - acc: 0.4306Epoch 00043: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9768 - acc: 0.4000 - val_loss: 2.4947 - val_acc: 0.2875\n",
      "Epoch 44/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0744 - acc: 0.3194Epoch 00044: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1158 - acc: 0.3125 - val_loss: 2.3494 - val_acc: 0.3125\n",
      "Epoch 45/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1471 - acc: 0.3611Epoch 00045: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1434 - acc: 0.3500 - val_loss: 2.7096 - val_acc: 0.2250\n",
      "Epoch 46/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8668 - acc: 0.4167Epoch 00046: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8922 - acc: 0.4000 - val_loss: 2.4469 - val_acc: 0.2938\n",
      "Epoch 47/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1612 - acc: 0.3333Epoch 00047: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1673 - acc: 0.3250 - val_loss: 2.7090 - val_acc: 0.3375\n",
      "Epoch 48/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7973 - acc: 0.4306Epoch 00048: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8507 - acc: 0.4125 - val_loss: 2.4207 - val_acc: 0.3000\n",
      "Epoch 49/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8993 - acc: 0.4306Epoch 00049: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9122 - acc: 0.4125 - val_loss: 2.5352 - val_acc: 0.3063\n",
      "Epoch 50/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0731 - acc: 0.4167Epoch 00050: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0179 - acc: 0.4375 - val_loss: 2.4321 - val_acc: 0.3375\n",
      "Epoch 51/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1623 - acc: 0.3333Epoch 00051: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1745 - acc: 0.3375 - val_loss: 2.4460 - val_acc: 0.2313\n",
      "Epoch 52/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0454 - acc: 0.2917Epoch 00052: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9949 - acc: 0.3125 - val_loss: 2.4360 - val_acc: 0.3563\n",
      "Epoch 53/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0559 - acc: 0.3472Epoch 00053: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0982 - acc: 0.3250 - val_loss: 2.5511 - val_acc: 0.2125\n",
      "Epoch 54/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1316 - acc: 0.2778Epoch 00054: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1739 - acc: 0.2625 - val_loss: 2.4082 - val_acc: 0.3125\n",
      "Epoch 55/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2360 - acc: 0.2361Epoch 00055: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1919 - acc: 0.2500 - val_loss: 2.3606 - val_acc: 0.3500\n",
      "Epoch 56/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0604 - acc: 0.3472Epoch 00056: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0753 - acc: 0.3375 - val_loss: 2.5331 - val_acc: 0.2562\n",
      "Epoch 57/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7929 - acc: 0.4028Epoch 00057: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7983 - acc: 0.4000 - val_loss: 2.3732 - val_acc: 0.3125\n",
      "Epoch 58/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9847 - acc: 0.3194Epoch 00058: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9831 - acc: 0.3250 - val_loss: 2.8477 - val_acc: 0.2875\n",
      "Epoch 59/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8976 - acc: 0.3889Epoch 00059: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8851 - acc: 0.3875 - val_loss: 2.5981 - val_acc: 0.2000\n",
      "Epoch 60/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0893 - acc: 0.2917Epoch 00060: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0689 - acc: 0.2875 - val_loss: 2.5906 - val_acc: 0.3000\n",
      "Epoch 61/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0522 - acc: 0.3472Epoch 00061: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0994 - acc: 0.3250 - val_loss: 2.3574 - val_acc: 0.2687\n",
      "Epoch 62/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8316 - acc: 0.3472Epoch 00062: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8603 - acc: 0.3500 - val_loss: 2.5743 - val_acc: 0.3000\n",
      "Epoch 63/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9659 - acc: 0.3889Epoch 00063: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9784 - acc: 0.3750 - val_loss: 2.3913 - val_acc: 0.3000\n",
      "Epoch 64/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0516 - acc: 0.3333Epoch 00064: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0582 - acc: 0.3250 - val_loss: 2.4973 - val_acc: 0.3063\n",
      "Epoch 65/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8010 - acc: 0.3472Epoch 00065: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7955 - acc: 0.3625 - val_loss: 2.5319 - val_acc: 0.3312\n",
      "Epoch 66/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5987 - acc: 0.4861Epoch 00066: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6028 - acc: 0.4875 - val_loss: 2.4725 - val_acc: 0.3563\n",
      "Epoch 67/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8534 - acc: 0.3750Epoch 00067: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7852 - acc: 0.4000 - val_loss: 2.5340 - val_acc: 0.3187\n",
      "Epoch 68/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1150 - acc: 0.3194Epoch 00068: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0811 - acc: 0.3250 - val_loss: 2.6222 - val_acc: 0.2750\n",
      "Epoch 69/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9709 - acc: 0.3333Epoch 00069: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8469 - acc: 0.3750 - val_loss: 2.5445 - val_acc: 0.2375\n",
      "Epoch 70/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7961 - acc: 0.4583Epoch 00070: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8599 - acc: 0.4500 - val_loss: 2.3964 - val_acc: 0.3875\n",
      "Epoch 71/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7959 - acc: 0.3889Epoch 00071: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8772 - acc: 0.3750 - val_loss: 2.7521 - val_acc: 0.3563\n",
      "Epoch 72/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7569 - acc: 0.4722Epoch 00072: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7184 - acc: 0.4750 - val_loss: 2.3632 - val_acc: 0.3937\n",
      "Epoch 73/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8836 - acc: 0.3056Epoch 00073: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9157 - acc: 0.3000 - val_loss: 2.2990 - val_acc: 0.3812\n",
      "Epoch 74/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8877 - acc: 0.3889Epoch 00074: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9029 - acc: 0.3750 - val_loss: 2.5670 - val_acc: 0.3625\n",
      "Epoch 75/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8323 - acc: 0.3750Epoch 00075: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8508 - acc: 0.3625 - val_loss: 2.3648 - val_acc: 0.3063\n",
      "Epoch 76/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6217 - acc: 0.4583Epoch 00076: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6950 - acc: 0.4250 - val_loss: 2.5458 - val_acc: 0.3063\n",
      "Epoch 77/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9932 - acc: 0.3333Epoch 00077: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0630 - acc: 0.3375 - val_loss: 2.5098 - val_acc: 0.2875\n",
      "Epoch 78/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8184 - acc: 0.3750Epoch 00078: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8851 - acc: 0.3750 - val_loss: 2.8355 - val_acc: 0.2062\n",
      "Epoch 79/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9658 - acc: 0.3194Epoch 00079: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9639 - acc: 0.3125 - val_loss: 2.7519 - val_acc: 0.2062\n",
      "Epoch 80/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6444 - acc: 0.4444Epoch 00080: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6812 - acc: 0.4375 - val_loss: 2.3657 - val_acc: 0.3187\n",
      "Epoch 81/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7649 - acc: 0.4028Epoch 00081: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7359 - acc: 0.4125 - val_loss: 2.5885 - val_acc: 0.3250\n",
      "Epoch 82/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9053 - acc: 0.3472Epoch 00082: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9553 - acc: 0.3250 - val_loss: 2.7601 - val_acc: 0.3438\n",
      "Epoch 83/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0464 - acc: 0.3056Epoch 00083: val_loss improved from 2.29833 to 2.10942, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9489 - acc: 0.3500 - val_loss: 2.1094 - val_acc: 0.4688\n",
      "Epoch 84/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9311 - acc: 0.3611Epoch 00084: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9262 - acc: 0.3875 - val_loss: 2.4594 - val_acc: 0.2938\n",
      "Epoch 85/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0523 - acc: 0.2917Epoch 00085: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1140 - acc: 0.2625 - val_loss: 2.6319 - val_acc: 0.2750\n",
      "Epoch 86/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6898 - acc: 0.4028Epoch 00086: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7221 - acc: 0.4000 - val_loss: 2.4803 - val_acc: 0.3500\n",
      "Epoch 87/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9066 - acc: 0.3472Epoch 00087: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7800 - acc: 0.3875 - val_loss: 2.7734 - val_acc: 0.2938\n",
      "Epoch 88/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6082 - acc: 0.4028Epoch 00088: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7198 - acc: 0.4000 - val_loss: 2.2963 - val_acc: 0.3937\n",
      "Epoch 89/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0671 - acc: 0.2917Epoch 00089: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.1025 - acc: 0.2875 - val_loss: 2.3449 - val_acc: 0.3563\n",
      "Epoch 90/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7877 - acc: 0.3889Epoch 00090: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8436 - acc: 0.3875 - val_loss: 2.5196 - val_acc: 0.3312\n",
      "Epoch 91/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5295 - acc: 0.4306Epoch 00091: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.5579 - acc: 0.4625 - val_loss: 2.6508 - val_acc: 0.2812\n",
      "Epoch 92/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9453 - acc: 0.2639Epoch 00092: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 2.0339 - acc: 0.2500 - val_loss: 2.3160 - val_acc: 0.3937\n",
      "Epoch 93/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4226 - acc: 0.4444Epoch 00093: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.4335 - acc: 0.4625 - val_loss: 2.2029 - val_acc: 0.4562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7989 - acc: 0.3889Epoch 00094: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8117 - acc: 0.3750 - val_loss: 2.7248 - val_acc: 0.2625\n",
      "Epoch 95/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7902 - acc: 0.3889Epoch 00095: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8278 - acc: 0.3625 - val_loss: 2.6772 - val_acc: 0.3500\n",
      "Epoch 96/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8293 - acc: 0.3194Epoch 00096: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8274 - acc: 0.3125 - val_loss: 2.3508 - val_acc: 0.3000\n",
      "Epoch 97/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7787 - acc: 0.3889Epoch 00097: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8616 - acc: 0.3500 - val_loss: 2.3419 - val_acc: 0.3187\n",
      "Epoch 98/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7647 - acc: 0.3472Epoch 00098: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7921 - acc: 0.3375 - val_loss: 2.4890 - val_acc: 0.3250\n",
      "Epoch 99/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6794 - acc: 0.3472Epoch 00099: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6861 - acc: 0.3500 - val_loss: 2.5551 - val_acc: 0.3063\n",
      "Epoch 100/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8308 - acc: 0.3472Epoch 00100: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8286 - acc: 0.3500 - val_loss: 2.6477 - val_acc: 0.2375\n",
      "Epoch 101/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8314 - acc: 0.3889Epoch 00101: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8788 - acc: 0.3625 - val_loss: 2.4766 - val_acc: 0.3500\n",
      "Epoch 102/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8713 - acc: 0.3194Epoch 00102: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8851 - acc: 0.3250 - val_loss: 2.5807 - val_acc: 0.3187\n",
      "Epoch 103/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6095 - acc: 0.4583Epoch 00103: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.5734 - acc: 0.4625 - val_loss: 2.7431 - val_acc: 0.3063\n",
      "Epoch 104/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4845 - acc: 0.4583Epoch 00104: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.4680 - acc: 0.4750 - val_loss: 2.6423 - val_acc: 0.3000\n",
      "Epoch 105/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8841 - acc: 0.4028Epoch 00105: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8501 - acc: 0.4000 - val_loss: 2.9993 - val_acc: 0.2313\n",
      "Epoch 106/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7016 - acc: 0.3889Epoch 00106: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7856 - acc: 0.3750 - val_loss: 2.6840 - val_acc: 0.2375\n",
      "Epoch 107/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8344 - acc: 0.3472Epoch 00107: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8544 - acc: 0.3375 - val_loss: 2.8352 - val_acc: 0.2000\n",
      "Epoch 108/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9312 - acc: 0.3333Epoch 00108: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9317 - acc: 0.3125 - val_loss: 2.4964 - val_acc: 0.2875\n",
      "Epoch 109/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5914 - acc: 0.4583Epoch 00109: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6090 - acc: 0.4500 - val_loss: 2.3989 - val_acc: 0.3500\n",
      "Epoch 110/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8696 - acc: 0.3889Epoch 00110: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7990 - acc: 0.4250 - val_loss: 2.5009 - val_acc: 0.3250\n",
      "Epoch 111/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8115 - acc: 0.3194Epoch 00111: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7866 - acc: 0.3250 - val_loss: 2.3559 - val_acc: 0.3063\n",
      "Epoch 112/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7597 - acc: 0.3889Epoch 00112: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7791 - acc: 0.3875 - val_loss: 2.5453 - val_acc: 0.2562\n",
      "Epoch 113/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5830 - acc: 0.4583Epoch 00113: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6247 - acc: 0.4375 - val_loss: 2.3953 - val_acc: 0.3438\n",
      "Epoch 114/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7711 - acc: 0.3750Epoch 00114: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8216 - acc: 0.3750 - val_loss: 2.3656 - val_acc: 0.2375\n",
      "Epoch 115/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7914 - acc: 0.2917Epoch 00115: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8152 - acc: 0.2750 - val_loss: 2.6001 - val_acc: 0.2437\n",
      "Epoch 116/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7350 - acc: 0.3750Epoch 00116: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7224 - acc: 0.3750 - val_loss: 2.3971 - val_acc: 0.2687\n",
      "Epoch 117/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6620 - acc: 0.3472Epoch 00117: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7027 - acc: 0.3250 - val_loss: 2.4913 - val_acc: 0.2687\n",
      "Epoch 118/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6897 - acc: 0.4028Epoch 00118: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6340 - acc: 0.4375 - val_loss: 2.3313 - val_acc: 0.3750\n",
      "Epoch 119/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6731 - acc: 0.4306Epoch 00119: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6428 - acc: 0.4375 - val_loss: 2.8404 - val_acc: 0.2938\n",
      "Epoch 120/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6020 - acc: 0.4306Epoch 00120: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6425 - acc: 0.4125 - val_loss: 2.1714 - val_acc: 0.3250\n",
      "Epoch 121/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7154 - acc: 0.3472Epoch 00121: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7213 - acc: 0.3500 - val_loss: 2.5390 - val_acc: 0.2562\n",
      "Epoch 122/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7653 - acc: 0.3750Epoch 00122: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7038 - acc: 0.3750 - val_loss: 2.4530 - val_acc: 0.3375\n",
      "Epoch 123/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4955 - acc: 0.4722Epoch 00123: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.5011 - acc: 0.4625 - val_loss: 2.5563 - val_acc: 0.3625\n",
      "Epoch 124/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7808 - acc: 0.3611Epoch 00124: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7178 - acc: 0.3750 - val_loss: 2.5588 - val_acc: 0.2812\n",
      "Epoch 125/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5169 - acc: 0.4306Epoch 00125: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.5687 - acc: 0.4125 - val_loss: 2.5562 - val_acc: 0.3187\n",
      "Epoch 126/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5261 - acc: 0.4167Epoch 00126: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.4765 - acc: 0.4250 - val_loss: 2.5548 - val_acc: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7880 - acc: 0.3611Epoch 00127: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7679 - acc: 0.3625 - val_loss: 2.5477 - val_acc: 0.3250\n",
      "Epoch 128/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6663 - acc: 0.4306Epoch 00128: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6619 - acc: 0.4250 - val_loss: 2.2670 - val_acc: 0.3250\n",
      "Epoch 129/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4452 - acc: 0.4167Epoch 00129: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.3923 - acc: 0.4375 - val_loss: 2.5867 - val_acc: 0.3500\n",
      "Epoch 130/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5671 - acc: 0.3472Epoch 00130: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.5589 - acc: 0.3625 - val_loss: 2.2873 - val_acc: 0.4062\n",
      "Epoch 131/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3998 - acc: 0.4444Epoch 00131: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.4132 - acc: 0.4375 - val_loss: 2.3606 - val_acc: 0.4188\n",
      "Epoch 132/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5667 - acc: 0.4583Epoch 00132: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.5219 - acc: 0.4625 - val_loss: 2.2708 - val_acc: 0.3125\n",
      "Epoch 133/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6920 - acc: 0.3056Epoch 00133: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7060 - acc: 0.3125 - val_loss: 2.4231 - val_acc: 0.3438\n",
      "Epoch 134/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6650 - acc: 0.3750Epoch 00134: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6187 - acc: 0.4000 - val_loss: 2.3014 - val_acc: 0.3125\n",
      "Epoch 135/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9143 - acc: 0.2639Epoch 00135: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9361 - acc: 0.2625 - val_loss: 2.2335 - val_acc: 0.3937\n",
      "Epoch 136/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7427 - acc: 0.3472Epoch 00136: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.7786 - acc: 0.3375 - val_loss: 2.3653 - val_acc: 0.3000\n",
      "Epoch 137/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8792 - acc: 0.2639Epoch 00137: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8897 - acc: 0.2750 - val_loss: 2.4261 - val_acc: 0.3250\n",
      "Epoch 138/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9106 - acc: 0.2917Epoch 00138: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.9035 - acc: 0.2875 - val_loss: 2.4160 - val_acc: 0.2812\n",
      "Epoch 139/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6222 - acc: 0.3472Epoch 00139: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6268 - acc: 0.3500 - val_loss: 2.1951 - val_acc: 0.2875\n",
      "Epoch 140/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8383 - acc: 0.3056Epoch 00140: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8273 - acc: 0.3125 - val_loss: 2.3779 - val_acc: 0.2812\n",
      "Epoch 141/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5169 - acc: 0.4167Epoch 00141: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6103 - acc: 0.3875 - val_loss: 2.2877 - val_acc: 0.2812\n",
      "Epoch 142/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3288 - acc: 0.5278Epoch 00142: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.3024 - acc: 0.5250 - val_loss: 2.2982 - val_acc: 0.2000\n",
      "Epoch 143/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6706 - acc: 0.3611Epoch 00143: val_loss did not improve\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6536 - acc: 0.3750 - val_loss: 2.4337 - val_acc: 0.2313\n",
      "Epoch 144/2000\n",
      " 2/10 [=====>........................] - ETA: 5s - loss: 1.7490 - acc: 0.4375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-047209e9a792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                 callbacks=[checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2094\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2095\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss',\n",
    "#                                min_delta=0,\n",
    "#                                patience=2,\n",
    "#                                verbose=0,\n",
    "#                                mode='auto')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./save/model.weights.best.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='min') \n",
    "\n",
    "history = model.fit_generator(  generator(8, training_filenames),\n",
    "                                validation_data=generator(16, validation_filenames),\n",
    "                                steps_per_epoch=10,\n",
    "                                validation_steps = 10,\n",
    "                                epochs=2000,\n",
    "                                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result():\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.save_weights('./save/model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case1_label9.npy\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = validation_filenames[0]\n",
    "print(filename)\n",
    "x = np.load(data_path.format('y') + filename)\n",
    "plt.imshow(x[:,:,65], cmap='gray')\n",
    "x = np.reshape(x, (1, 128, 128, 128, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05600699 0.03173709 0.03822409 0.05903802 0.03061834 0.03771988\n",
      " 0.0723184  0.07143686 0.03328035 0.06424842 0.03549782 0.03662397\n",
      " 0.03973443 0.05227344 0.04826288 0.05150586 0.06920768 0.07430369\n",
      " 0.06837983 0.02958193]\n",
      "0.074303694\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "y_cat = model.predict(x)\n",
    "print(y_cat[0])\n",
    "print(max(y_cat[0]))\n",
    "print(np.argmax(y_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(data_path.format('y') + validation_filenames[0])\n",
    "plt.imshow(y[:,:,65], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./save/model.label.weights.final.h5')\n",
    "#model.load_weights('./save/model.weights.best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case9\n",
      "9\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def generator(batch_size, filenames):\n",
    "    size = 128\n",
    "    \n",
    "    shape = (batch_size, size, size, size, 1)\n",
    "\n",
    "    x_out = np.zeros(shape)\n",
    "    y_out = np.zeros(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        filename = random.choice(filenames)\n",
    "        #filename = filenames[0]\n",
    "        x = np.load(data_path.format('x') + filename)\n",
    "        y = np.load(data_path.format('y') + filename)\n",
    "        x = np.reshape(x, (size, size, size, -1))\n",
    "        y = np.reshape(y, (size, size, size, -1))\n",
    "\n",
    "        x_out[i] = x\n",
    "        y_out[i] = y\n",
    "    \n",
    "        print(filename.split('_')[0])\n",
    "        print(int(re.sub(\"\\D\", \"\", filename.split('_')[0])))\n",
    "        \n",
    "        label = int(re.sub(\"\\D\", \"\", filename.split('_')[0]))\n",
    "        y_out2 = [0] * 20\n",
    "        y_out2[label] = 1\n",
    "        print(y_out2)\n",
    "        \n",
    "generator(1, training_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(random.randint(-2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 125)\n"
     ]
    }
   ],
   "source": [
    "    x_bias, y_bias, z_bias = random.randint(-2,2), random.randint(-2,2), random.randint(-2,2)\n",
    "    x_range = (x_bias, 127) if x_bias > 0 else (0, size + x_bias -1)\n",
    "    y_range = (y_bias, 127) if y_bias > 0 else (0, size + y_bias -1)\n",
    "    z_range = (z_bias, 127) if z_bias > 0 else (0, size + z_bias -1)\n",
    "    print(x_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
