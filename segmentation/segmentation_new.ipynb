{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libs loaded\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "import h5py\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import walk\n",
    "import random\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "def setGPU():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    config = tf.ConfigProto()\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))\n",
    "setGPU()\n",
    "\n",
    "print('libs loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case10_label11.npy', 'case6_label11.npy', 'case8_label11.npy', 'case10_label6.npy', 'case9_label19.npy', 'case2_label13.npy', 'case9_label7.npy', 'case5_label3.npy', 'case6_label15.npy', 'case6_label8.npy', 'case10_label4.npy', 'case9_label13.npy', 'case3_label6.npy', 'case2_label5.npy', 'case9_label12.npy', 'case6_label9.npy', 'case9_label17.npy', 'case9_label10.npy', 'case8_label12.npy', 'case8_label4.npy', 'case4_label14.npy', 'case9_label5.npy', 'case4_label6.npy', 'case8_label5.npy', 'case5_label8.npy', 'case4_label3.npy', 'case8_label9.npy', 'case8_label18.npy', 'case8_label15.npy', 'case6_label19.npy', 'case8_label10.npy', 'case9_label8.npy', 'case4_label11.npy', 'case5_label17.npy', 'case4_label12.npy', 'case3_label16.npy', 'case2_label8.npy', 'case2_label7.npy', 'case8_label19.npy', 'case3_label7.npy', 'case8_label8.npy', 'case4_label19.npy', 'case3_label15.npy', 'case6_label6.npy', 'case5_label18.npy', 'case4_label15.npy', 'case5_label5.npy', 'case4_label4.npy', 'case5_label2.npy', 'case3_label14.npy', 'case3_label13.npy', 'case10_label19.npy', 'case6_label5.npy', 'case10_label18.npy', 'case8_label3.npy', 'case3_label11.npy', 'case6_label18.npy', 'case4_label1.npy', 'case10_label5.npy', 'case4_label8.npy', 'case5_label12.npy', 'case8_label7.npy', 'case5_label10.npy', 'case5_label4.npy', 'case8_label14.npy', 'case3_label10.npy', 'case6_label4.npy', 'case4_label13.npy', 'case6_label16.npy', 'case8_label16.npy', 'case5_label19.npy', 'case2_label12.npy', 'case5_label11.npy', 'case2_label6.npy', 'case3_label8.npy', 'case5_label13.npy', 'case10_label17.npy', 'case6_label13.npy', 'case5_label14.npy', 'case2_label9.npy', 'case9_label2.npy', 'case10_label13.npy', 'case3_label5.npy', 'case9_label11.npy', 'case2_label4.npy', 'case8_label2.npy', 'case2_label18.npy', 'case10_label14.npy', 'case2_label3.npy', 'case10_label15.npy', 'case10_label3.npy', 'case5_label6.npy', 'case5_label7.npy', 'case2_label17.npy', 'case10_label8.npy', 'case2_label19.npy', 'case5_label16.npy', 'case4_label2.npy', 'case2_label10.npy', 'case3_label17.npy', 'case3_label3.npy', 'case9_label3.npy', 'case2_label16.npy', 'case9_label16.npy', 'case4_label7.npy', 'case6_label3.npy', 'case10_label12.npy', 'case4_label10.npy', 'case3_label9.npy', 'case6_label17.npy', 'case6_label7.npy', 'case2_label15.npy', 'case6_label10.npy', 'case2_label11.npy', 'case3_label18.npy', 'case9_label9.npy', 'case8_label6.npy', 'case10_label10.npy', 'case3_label19.npy', 'case6_label12.npy', 'case4_label9.npy', 'case8_label17.npy', 'case10_label9.npy', 'case9_label15.npy', 'case10_label16.npy', 'case8_label13.npy', 'case6_label14.npy', 'case9_label14.npy', 'case9_label18.npy', 'case5_label15.npy', 'case4_label5.npy', 'case3_label4.npy', 'case4_label16.npy', 'case9_label6.npy', 'case10_label7.npy', 'case2_label14.npy', 'case5_label9.npy', 'case3_label12.npy', 'case4_label17.npy', 'case9_label4.npy', 'case4_label18.npy', 'case10_label11.npy', 'case6_label11.npy', 'case8_label11.npy', 'case10_label6.npy', 'case9_label19.npy', 'case2_label13.npy', 'case9_label7.npy', 'case5_label3.npy', 'case6_label15.npy', 'case6_label8.npy', 'case10_label4.npy', 'case9_label13.npy', 'case3_label6.npy', 'case2_label5.npy', 'case9_label12.npy', 'case6_label9.npy', 'case9_label17.npy', 'case9_label10.npy', 'case8_label12.npy', 'case8_label4.npy', 'case4_label14.npy', 'case9_label5.npy', 'case4_label6.npy', 'case8_label5.npy', 'case5_label8.npy', 'case4_label3.npy', 'case8_label9.npy', 'case8_label18.npy', 'case8_label15.npy', 'case6_label19.npy', 'case8_label10.npy', 'case9_label8.npy', 'case4_label11.npy', 'case5_label17.npy', 'case4_label12.npy', 'case3_label16.npy', 'case2_label8.npy', 'case2_label7.npy', 'case8_label19.npy', 'case3_label7.npy', 'case8_label8.npy', 'case4_label19.npy', 'case3_label15.npy', 'case6_label6.npy', 'case5_label18.npy', 'case4_label15.npy', 'case5_label5.npy', 'case4_label4.npy', 'case5_label2.npy', 'case3_label14.npy', 'case3_label13.npy', 'case10_label19.npy', 'case6_label5.npy', 'case10_label18.npy', 'case8_label3.npy', 'case3_label11.npy', 'case6_label18.npy', 'case4_label1.npy', 'case10_label5.npy', 'case4_label8.npy', 'case5_label12.npy', 'case8_label7.npy', 'case5_label10.npy', 'case5_label4.npy', 'case8_label14.npy', 'case3_label10.npy', 'case6_label4.npy', 'case4_label13.npy', 'case6_label16.npy', 'case8_label16.npy', 'case5_label19.npy', 'case2_label12.npy', 'case5_label11.npy', 'case2_label6.npy', 'case3_label8.npy', 'case5_label13.npy', 'case10_label17.npy', 'case6_label13.npy', 'case5_label14.npy', 'case2_label9.npy', 'case9_label2.npy', 'case10_label13.npy', 'case3_label5.npy', 'case9_label11.npy', 'case2_label4.npy', 'case8_label2.npy', 'case2_label18.npy', 'case10_label14.npy', 'case2_label3.npy', 'case10_label15.npy', 'case10_label3.npy', 'case5_label6.npy', 'case5_label7.npy', 'case2_label17.npy', 'case10_label8.npy', 'case2_label19.npy', 'case5_label16.npy', 'case4_label2.npy', 'case2_label10.npy', 'case3_label17.npy', 'case3_label3.npy', 'case9_label3.npy', 'case2_label16.npy', 'case9_label16.npy', 'case4_label7.npy', 'case6_label3.npy', 'case10_label12.npy', 'case4_label10.npy', 'case3_label9.npy', 'case6_label17.npy', 'case6_label7.npy', 'case2_label15.npy', 'case6_label10.npy', 'case2_label11.npy', 'case3_label18.npy', 'case9_label9.npy', 'case8_label6.npy', 'case10_label10.npy', 'case3_label19.npy', 'case6_label12.npy', 'case4_label9.npy', 'case8_label17.npy', 'case10_label9.npy', 'case9_label15.npy', 'case10_label16.npy', 'case8_label13.npy', 'case6_label14.npy', 'case9_label14.npy', 'case9_label18.npy', 'case5_label15.npy', 'case4_label5.npy', 'case3_label4.npy', 'case4_label16.npy', 'case9_label6.npy', 'case10_label7.npy', 'case2_label14.npy', 'case5_label9.npy', 'case3_label12.npy', 'case4_label17.npy', 'case9_label4.npy', 'case4_label18.npy']\n",
      "['case1_label9.npy', 'case1_label6.npy', 'case1_label4.npy', 'case1_label19.npy', 'case1_label18.npy', 'case1_label3.npy', 'case1_label10.npy', 'case1_label2.npy', 'case1_label8.npy', 'case1_label17.npy', 'case1_label14.npy', 'case1_label16.npy', 'case1_label5.npy', 'case1_label15.npy', 'case1_label12.npy', 'case1_label11.npy', 'case1_label7.npy', 'case1_label13.npy', 'case1_label9.npy', 'case1_label6.npy', 'case1_label4.npy', 'case1_label19.npy', 'case1_label18.npy', 'case1_label3.npy', 'case1_label10.npy', 'case1_label2.npy', 'case1_label8.npy', 'case1_label17.npy', 'case1_label14.npy', 'case1_label16.npy', 'case1_label5.npy', 'case1_label15.npy', 'case1_label12.npy', 'case1_label11.npy', 'case1_label7.npy', 'case1_label13.npy']\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "data_path = '../data/seg_data/{}/'\n",
    "size = 128\n",
    "\n",
    "def load_filenames(path):\n",
    "    filenames = []\n",
    "    for (dirpath, dirnames, filenames) in walk(data_path.format('x')):\n",
    "        filenames.extend(filenames)\n",
    "        break\n",
    "    for f in filenames:\n",
    "        if not '.npy' in f:\n",
    "            del f\n",
    "    training_filenames = []\n",
    "    validation_filenames = []\n",
    "    for f in filenames:\n",
    "        if 'case1_' in f:\n",
    "            validation_filenames.append(f)\n",
    "        else:\n",
    "            training_filenames.append(f)\n",
    "            \n",
    "            \n",
    "    return training_filenames, validation_filenames\n",
    "    \n",
    "training_filenames, validation_filenames = load_filenames(data_path)\n",
    "\n",
    "print(training_filenames)\n",
    "print(validation_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 128, 128, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128, 128, 128 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 128, 128, 128 112         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 128 16          conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 128 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_17 (Conv3D)              (None, 128, 128, 128 872         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 128 32          conv3d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 128 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3D)  (None, 64, 64, 64, 8 0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_18 (Conv3D)              (None, 64, 64, 64, 8 1736        max_pooling3d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 64, 8 32          conv3d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 64, 64, 8 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_19 (Conv3D)              (None, 64, 64, 64, 1 3472        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 64, 64, 1 64          conv3d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 64, 1 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3D)  (None, 32, 32, 32, 1 0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_20 (Conv3D)              (None, 32, 32, 32, 1 6928        max_pooling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 32, 1 64          conv3d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 32, 1 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_21 (Conv3D)              (None, 32, 32, 32, 3 13856       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 32, 3 128         conv3d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 32, 3 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3D)  (None, 16, 16, 16, 3 0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_22 (Conv3D)              (None, 16, 16, 16, 3 27680       max_pooling3d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 16, 3 128         conv3d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 16, 3 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_23 (Conv3D)              (None, 16, 16, 16, 6 55360       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 16, 6 256         conv3d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 16, 6 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_4 (UpSampling3D)  (None, 32, 32, 32, 6 0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 32, 9 0           activation_21[0][0]              \n",
      "                                                                 up_sampling3d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_24 (Conv3D)              (None, 32, 32, 32, 3 82976       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 32, 3 128         conv3d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 32, 3 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_25 (Conv3D)              (None, 32, 32, 32, 3 27680       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 32, 32, 3 128         conv3d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 32, 32, 3 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_5 (UpSampling3D)  (None, 64, 64, 64, 3 0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 64, 4 0           activation_19[0][0]              \n",
      "                                                                 up_sampling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_26 (Conv3D)              (None, 64, 64, 64, 1 20752       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 64, 64, 1 64          conv3d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64, 64, 64, 1 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_27 (Conv3D)              (None, 64, 64, 64, 1 6928        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 64, 1 64          conv3d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 64, 64, 64, 1 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_6 (UpSampling3D)  (None, 128, 128, 128 0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 128, 128 0           activation_17[0][0]              \n",
      "                                                                 up_sampling3d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_28 (Conv3D)              (None, 128, 128, 128 5192        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 128, 128, 128 32          conv3d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 128, 128, 128 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_29 (Conv3D)              (None, 128, 128, 128 1736        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 128, 128, 128 32          conv3d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 128, 128, 128 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_30 (Conv3D)              (None, 128, 128, 128 217         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 128, 128, 128 4           conv3d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 128, 128, 128 0           batch_normalization_30[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 256,669\n",
      "Trainable params: 256,083\n",
      "Non-trainable params: 586\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def model():\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, Concatenate, BatchNormalization, Activation, Dense, Flatten\n",
    "    \n",
    "    def conv(input_tensor, depth):\n",
    "        conv_tensor = Conv3D(depth, 3, padding = 'same', kernel_initializer = 'he_normal')(input_tensor)\n",
    "        bn = BatchNormalization()(conv_tensor) # use_bias=False\n",
    "        output_tensor = Activation('relu')(bn)\n",
    "        return output_tensor\n",
    "    \n",
    "    def pool(input_tensor):\n",
    "        output_tensor = MaxPooling3D(pool_size=(2, 2, 2))(input_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "    def up(input_tensor):\n",
    "        output_tensor = UpSampling3D(size=(2, 2, 2))(input_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "    def skip(input_tensor1, input_tensor2):\n",
    "        output_tensor = Concatenate(axis=-1)([input_tensor1, input_tensor2])\n",
    "        return output_tensor\n",
    "    \n",
    "    depth = 4\n",
    "    \n",
    "    input_img = Input(shape=(size, size, size, 1))\n",
    "    \n",
    "    conv1 = conv(input_img, depth)\n",
    "    conv2 = conv(conv1, depth * 2)\n",
    "    \n",
    "    pool1 = pool(conv2)\n",
    "    \n",
    "    conv3 = conv(pool1, depth * 2)\n",
    "    conv4 = conv(conv3, depth * 4)\n",
    "    \n",
    "    pool2 = pool(conv4)\n",
    "    \n",
    "    conv5 = conv(pool2, depth * 4)\n",
    "    conv6 = conv(conv5, depth * 8)\n",
    "    \n",
    "    pool3 = pool(conv6)\n",
    "    \n",
    "    conv7 = conv(pool3, depth * 8)\n",
    "    conv8 = conv(conv7, depth * 16)   \n",
    "\n",
    "    up1 = up(conv8)\n",
    "    \n",
    "    skip1 = skip(conv6, up1)\n",
    "    \n",
    "    conv9 = conv(skip1, depth * 8)\n",
    "    conv10 = conv(conv9, depth * 8)\n",
    "    \n",
    "    up2   = up(conv10)\n",
    "    \n",
    "    skip2 = skip(conv4, up2)\n",
    "    \n",
    "    conv11 = conv(skip2, depth * 4)\n",
    "    conv12 = conv(conv11, depth * 4)\n",
    "    \n",
    "    up3   = up(conv12)\n",
    "    \n",
    "    skip3 = skip(conv2, up3)\n",
    "    \n",
    "    conv13 = conv(skip3, depth * 2)\n",
    "    conv14 = conv(conv13, depth * 2)    \n",
    "   \n",
    "    conv15 = conv(conv14, 1)\n",
    "\n",
    "    output_img = conv15\n",
    "    \n",
    "    # model\n",
    "    model = Model(inputs=input_img, outputs=output_img)\n",
    "    print (model.output_shape)\n",
    "\n",
    "    # optimizer\n",
    "    opt = keras.optimizers.Adam(lr=1e-2)#32-5,16-3\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss = 'mse',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generator(batch_size, filenames):\n",
    "    while 1:\n",
    "        shape = (batch_size, size, size, size, 1)\n",
    "        shape2 = (batch_size, 20)\n",
    "        \n",
    "        x_out = np.zeros(shape)\n",
    "        y_out = np.zeros(shape)\n",
    "        y_out2 = np.zeros(shape2)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            filename = random.choice(filenames)\n",
    "            #filename = filenames[0]\n",
    "            x = np.load(data_path.format('x') + filename)\n",
    "            y = np.load(data_path.format('y') + filename)\n",
    "            x = np.reshape(x, (size, size, size, -1))\n",
    "            y = np.reshape(y, (size, size, size, -1))\n",
    "            \n",
    "            x_out[i] = x\n",
    "            y_out[i] = y\n",
    "            \n",
    "            label = int(re.sub(\"\\D\", \"\", filename.split('_')[0]))\n",
    "            label_one_hot = [0] * 20\n",
    "            label_one_hot[label] = 1\n",
    "            \n",
    "            y_out2[i] = label_one_hot\n",
    "            \n",
    "        #yield (x_out, y_out)\n",
    "        yield (x_out, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0127 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.01309, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0132 - acc: 0.9819 - val_loss: 0.0131 - val_acc: 0.9869\n",
      "Epoch 2/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0127 - acc: 0.9835Epoch 00002: val_loss improved from 0.01309 to 0.01205, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0124 - acc: 0.9841 - val_loss: 0.0121 - val_acc: 0.9859\n",
      "Epoch 3/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0125 - acc: 0.9833Epoch 00003: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0124 - acc: 0.9837 - val_loss: 0.0123 - val_acc: 0.9873\n",
      "Epoch 4/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0120 - acc: 0.9836Epoch 00004: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0118 - acc: 0.9838 - val_loss: 0.0137 - val_acc: 0.9851\n",
      "Epoch 5/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0132 - acc: 0.9815Epoch 00005: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0131 - acc: 0.9818 - val_loss: 0.0179 - val_acc: 0.9832\n",
      "Epoch 6/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0115 - acc: 0.9840Epoch 00006: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0119 - acc: 0.9834 - val_loss: 0.0139 - val_acc: 0.9863\n",
      "Epoch 7/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0110 - acc: 0.9844Epoch 00007: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0110 - acc: 0.9843 - val_loss: 0.0126 - val_acc: 0.9869\n",
      "Epoch 8/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0102 - acc: 0.9854Epoch 00008: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0106 - acc: 0.9847 - val_loss: 0.0131 - val_acc: 0.9869\n",
      "Epoch 9/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0103 - acc: 0.9855Epoch 00009: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0105 - acc: 0.9851 - val_loss: 0.0132 - val_acc: 0.9868\n",
      "Epoch 10/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0109 - acc: 0.9832Epoch 00010: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0108 - acc: 0.9834 - val_loss: 0.0137 - val_acc: 0.9863\n",
      "Epoch 11/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0106 - acc: 0.9853Epoch 00011: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0105 - acc: 0.9854 - val_loss: 0.0126 - val_acc: 0.9874\n",
      "Epoch 12/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0105 - acc: 0.9856Epoch 00012: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0105 - acc: 0.9855 - val_loss: 0.0123 - val_acc: 0.9877\n",
      "Epoch 13/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0100 - acc: 0.9862Epoch 00013: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0101 - acc: 0.9861 - val_loss: 0.0125 - val_acc: 0.9873\n",
      "Epoch 14/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0099 - acc: 0.9859Epoch 00014: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0099 - acc: 0.9859 - val_loss: 0.0130 - val_acc: 0.9870\n",
      "Epoch 15/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0102 - acc: 0.9858Epoch 00015: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0101 - acc: 0.9858 - val_loss: 0.0129 - val_acc: 0.9871\n",
      "Epoch 16/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0088 - acc: 0.9876Epoch 00016: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0089 - acc: 0.9875 - val_loss: 0.0207 - val_acc: 0.9720\n",
      "Epoch 17/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0095 - acc: 0.9861Epoch 00017: val_loss improved from 0.01205 to 0.01055, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0095 - acc: 0.9862 - val_loss: 0.0106 - val_acc: 0.9869\n",
      "Epoch 18/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0083 - acc: 0.9885Epoch 00018: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0083 - acc: 0.9886 - val_loss: 0.0134 - val_acc: 0.9866\n",
      "Epoch 19/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0085 - acc: 0.9877Epoch 00019: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0084 - acc: 0.9879 - val_loss: 0.0219 - val_acc: 0.9725\n",
      "Epoch 20/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0080 - acc: 0.9886Epoch 00020: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0080 - acc: 0.9885 - val_loss: 0.0128 - val_acc: 0.9863\n",
      "Epoch 21/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0082 - acc: 0.9879Epoch 00021: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0082 - acc: 0.9879 - val_loss: 0.0146 - val_acc: 0.9841\n",
      "Epoch 22/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0082 - acc: 0.9879Epoch 00022: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0081 - acc: 0.9882 - val_loss: 0.0350 - val_acc: 0.9590\n",
      "Epoch 23/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0077 - acc: 0.9888Epoch 00023: val_loss improved from 0.01055 to 0.01046, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0077 - acc: 0.9888 - val_loss: 0.0105 - val_acc: 0.9886\n",
      "Epoch 24/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0081 - acc: 0.9887Epoch 00024: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0082 - acc: 0.9883 - val_loss: 0.0110 - val_acc: 0.9878\n",
      "Epoch 25/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0069 - acc: 0.9897Epoch 00025: val_loss improved from 0.01046 to 0.00908, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0071 - acc: 0.9894 - val_loss: 0.0091 - val_acc: 0.9888\n",
      "Epoch 26/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0074 - acc: 0.9892Epoch 00026: val_loss improved from 0.00908 to 0.00860, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0074 - acc: 0.9892 - val_loss: 0.0086 - val_acc: 0.9873\n",
      "Epoch 27/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0073 - acc: 0.9889Epoch 00027: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0072 - acc: 0.9892 - val_loss: 0.0093 - val_acc: 0.9882\n",
      "Epoch 28/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0071 - acc: 0.9898Epoch 00028: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0071 - acc: 0.9898 - val_loss: 0.0159 - val_acc: 0.9790\n",
      "Epoch 29/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0082 - acc: 0.9880Epoch 00029: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0080 - acc: 0.9883 - val_loss: 0.0115 - val_acc: 0.9876\n",
      "Epoch 30/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0069 - acc: 0.9902Epoch 00030: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0069 - acc: 0.9902 - val_loss: 0.0278 - val_acc: 0.9614\n",
      "Epoch 31/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0068 - acc: 0.9905Epoch 00031: val_loss improved from 0.00860 to 0.00727, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0067 - acc: 0.9906 - val_loss: 0.0073 - val_acc: 0.9900\n",
      "Epoch 32/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0074 - acc: 0.9893Epoch 00032: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0073 - acc: 0.9895 - val_loss: 0.0104 - val_acc: 0.9868\n",
      "Epoch 33/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0069 - acc: 0.9900Epoch 00033: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0072 - acc: 0.9896 - val_loss: 0.0082 - val_acc: 0.9893\n",
      "Epoch 34/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0068 - acc: 0.9902Epoch 00034: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0067 - acc: 0.9904 - val_loss: 0.0116 - val_acc: 0.9864\n",
      "Epoch 35/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0066 - acc: 0.9906Epoch 00035: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0065 - acc: 0.9906 - val_loss: 0.0175 - val_acc: 0.9809\n",
      "Epoch 36/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0061 - acc: 0.9913Epoch 00036: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0062 - acc: 0.9912 - val_loss: 0.0111 - val_acc: 0.9874\n",
      "Epoch 37/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0061 - acc: 0.9914Epoch 00037: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0062 - acc: 0.9913 - val_loss: 0.0122 - val_acc: 0.9851\n",
      "Epoch 38/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0063 - acc: 0.9911Epoch 00038: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0061 - acc: 0.9914 - val_loss: 0.0337 - val_acc: 0.9490\n",
      "Epoch 39/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0060 - acc: 0.9913Epoch 00039: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0059 - acc: 0.9916 - val_loss: 0.0116 - val_acc: 0.9852\n",
      "Epoch 40/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0057 - acc: 0.9921Epoch 00040: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0057 - acc: 0.9920 - val_loss: 0.0127 - val_acc: 0.9849\n",
      "Epoch 41/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0061 - acc: 0.9913Epoch 00041: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0061 - acc: 0.9913 - val_loss: 0.0128 - val_acc: 0.9838\n",
      "Epoch 42/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0056 - acc: 0.9920Epoch 00042: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0054 - acc: 0.9922 - val_loss: 0.0115 - val_acc: 0.9874\n",
      "Epoch 43/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0053 - acc: 0.9923Epoch 00043: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0055 - acc: 0.9921 - val_loss: 0.0103 - val_acc: 0.9891\n",
      "Epoch 44/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0063 - acc: 0.9912Epoch 00044: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0064 - acc: 0.9911 - val_loss: 0.0101 - val_acc: 0.9877\n",
      "Epoch 45/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0057 - acc: 0.9920Epoch 00045: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0057 - acc: 0.9919 - val_loss: 0.0134 - val_acc: 0.9866\n",
      "Epoch 46/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0055 - acc: 0.9923Epoch 00046: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0056 - acc: 0.9923 - val_loss: 0.0087 - val_acc: 0.9892\n",
      "Epoch 47/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0051 - acc: 0.9927Epoch 00047: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0051 - acc: 0.9927 - val_loss: 0.0110 - val_acc: 0.9845\n",
      "Epoch 48/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9926Epoch 00048: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0051 - acc: 0.9927 - val_loss: 0.0112 - val_acc: 0.9875\n",
      "Epoch 49/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0051 - acc: 0.9927Epoch 00049: val_loss improved from 0.00727 to 0.00486, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0052 - acc: 0.9928 - val_loss: 0.0049 - val_acc: 0.9931\n",
      "Epoch 50/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0049 - acc: 0.9933Epoch 00050: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0048 - acc: 0.9933 - val_loss: 0.0131 - val_acc: 0.9867\n",
      "Epoch 51/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9924Epoch 00051: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0051 - acc: 0.9926 - val_loss: 0.0068 - val_acc: 0.9912\n",
      "Epoch 52/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0050 - acc: 0.9928Epoch 00052: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0050 - acc: 0.9928 - val_loss: 0.0113 - val_acc: 0.9868\n",
      "Epoch 53/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - acc: 0.9933Epoch 00053: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0047 - acc: 0.9932 - val_loss: 0.0137 - val_acc: 0.9863\n",
      "Epoch 54/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - acc: 0.9935Epoch 00054: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0046 - acc: 0.9935 - val_loss: 0.0071 - val_acc: 0.9915\n",
      "Epoch 55/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9940Epoch 00055: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0044 - acc: 0.9939 - val_loss: 0.0114 - val_acc: 0.9875\n",
      "Epoch 56/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9940Epoch 00056: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0044 - acc: 0.9938 - val_loss: 0.0132 - val_acc: 0.9866\n",
      "Epoch 57/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9938Epoch 00057: val_loss improved from 0.00486 to 0.00431, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0043 - acc: 0.9939 - val_loss: 0.0043 - val_acc: 0.9943\n",
      "Epoch 58/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9943Epoch 00058: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0040 - acc: 0.9943 - val_loss: 0.0071 - val_acc: 0.9919\n",
      "Epoch 59/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0042 - acc: 0.9942Epoch 00059: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0043 - acc: 0.9941 - val_loss: 0.0111 - val_acc: 0.9882\n",
      "Epoch 60/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9935Epoch 00060: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0046 - acc: 0.9936 - val_loss: 0.0124 - val_acc: 0.9866\n",
      "Epoch 61/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0044 - acc: 0.9938Epoch 00061: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0043 - acc: 0.9938 - val_loss: 0.0076 - val_acc: 0.9911\n",
      "Epoch 62/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0045 - acc: 0.9940Epoch 00062: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0045 - acc: 0.9940 - val_loss: 0.0124 - val_acc: 0.9876\n",
      "Epoch 63/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9940Epoch 00063: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0044 - acc: 0.9938 - val_loss: 0.0066 - val_acc: 0.9918\n",
      "Epoch 64/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9945Epoch 00064: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0039 - acc: 0.9946 - val_loss: 0.0097 - val_acc: 0.9888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9937Epoch 00065: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0046 - acc: 0.9937 - val_loss: 0.0079 - val_acc: 0.9908\n",
      "Epoch 66/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9940Epoch 00066: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0042 - acc: 0.9941 - val_loss: 0.0120 - val_acc: 0.9871\n",
      "Epoch 67/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9945Epoch 00067: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0039 - acc: 0.9945 - val_loss: 0.0094 - val_acc: 0.9897\n",
      "Epoch 68/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9947Epoch 00068: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0038 - acc: 0.9948 - val_loss: 0.0084 - val_acc: 0.9909\n",
      "Epoch 69/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9948Epoch 00069: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0039 - acc: 0.9946 - val_loss: 0.0140 - val_acc: 0.9860\n",
      "Epoch 70/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9947Epoch 00070: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0039 - acc: 0.9947 - val_loss: 0.0122 - val_acc: 0.9872\n",
      "Epoch 71/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9949Epoch 00071: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0038 - acc: 0.9949 - val_loss: 0.0064 - val_acc: 0.9921\n",
      "Epoch 72/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - acc: 0.9946Epoch 00072: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0040 - acc: 0.9946 - val_loss: 0.0050 - val_acc: 0.9935\n",
      "Epoch 73/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0045 - acc: 0.9940Epoch 00073: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0044 - acc: 0.9941 - val_loss: 0.0149 - val_acc: 0.9851\n",
      "Epoch 74/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9952Epoch 00074: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0035 - acc: 0.9953 - val_loss: 0.0092 - val_acc: 0.9889\n",
      "Epoch 75/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9944Epoch 00075: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0041 - acc: 0.9944 - val_loss: 0.0104 - val_acc: 0.9886\n",
      "Epoch 76/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052 - acc: 0.9935Epoch 00076: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0050 - acc: 0.9936 - val_loss: 0.0126 - val_acc: 0.9862\n",
      "Epoch 77/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0048 - acc: 0.9935Epoch 00077: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0048 - acc: 0.9934 - val_loss: 0.0050 - val_acc: 0.9939\n",
      "Epoch 78/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0045 - acc: 0.9940Epoch 00078: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0045 - acc: 0.9939 - val_loss: 0.0131 - val_acc: 0.9865\n",
      "Epoch 79/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0051 - acc: 0.9935Epoch 00079: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0050 - acc: 0.9936 - val_loss: 0.0117 - val_acc: 0.9879\n",
      "Epoch 80/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9937Epoch 00080: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0047 - acc: 0.9937 - val_loss: 0.0123 - val_acc: 0.9873\n",
      "Epoch 81/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9949Epoch 00081: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0040 - acc: 0.9947 - val_loss: 0.0114 - val_acc: 0.9876\n",
      "Epoch 82/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9949Epoch 00082: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0039 - acc: 0.9948 - val_loss: 0.0136 - val_acc: 0.9864\n",
      "Epoch 83/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9950Epoch 00083: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0036 - acc: 0.9950 - val_loss: 0.0127 - val_acc: 0.9863\n",
      "Epoch 84/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9953Epoch 00084: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0034 - acc: 0.9955 - val_loss: 0.0101 - val_acc: 0.9883\n",
      "Epoch 85/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9952Epoch 00085: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0035 - acc: 0.9952 - val_loss: 0.0095 - val_acc: 0.9884\n",
      "Epoch 86/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9948Epoch 00086: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0038 - acc: 0.9948 - val_loss: 0.0086 - val_acc: 0.9885\n",
      "Epoch 87/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9956Epoch 00087: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0032 - acc: 0.9957 - val_loss: 0.0078 - val_acc: 0.9899\n",
      "Epoch 88/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9957Epoch 00088: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0032 - acc: 0.9957 - val_loss: 0.0051 - val_acc: 0.9928\n",
      "Epoch 89/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9960Epoch 00089: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0030 - acc: 0.9959 - val_loss: 0.0052 - val_acc: 0.9930\n",
      "Epoch 90/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9958Epoch 00090: val_loss improved from 0.00431 to 0.00409, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0030 - acc: 0.9958 - val_loss: 0.0041 - val_acc: 0.9948\n",
      "Epoch 91/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9961Epoch 00091: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0030 - acc: 0.9960 - val_loss: 0.0090 - val_acc: 0.9895\n",
      "Epoch 92/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9959Epoch 00092: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0033 - acc: 0.9956 - val_loss: 0.0062 - val_acc: 0.9918\n",
      "Epoch 93/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9961Epoch 00093: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0029 - acc: 0.9960 - val_loss: 0.0065 - val_acc: 0.9923\n",
      "Epoch 94/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0030 - acc: 0.9960Epoch 00094: val_loss improved from 0.00409 to 0.00331, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0030 - acc: 0.9959 - val_loss: 0.0033 - val_acc: 0.9956\n",
      "Epoch 95/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9958Epoch 00095: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0030 - acc: 0.9958 - val_loss: 0.0038 - val_acc: 0.9948\n",
      "Epoch 96/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9957Epoch 00096: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0031 - acc: 0.9958 - val_loss: 0.0073 - val_acc: 0.9916\n",
      "Epoch 97/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9963Epoch 00097: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0030 - acc: 0.9961 - val_loss: 0.0064 - val_acc: 0.9925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9965Epoch 00098: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0026 - acc: 0.9965 - val_loss: 0.0085 - val_acc: 0.9902\n",
      "Epoch 99/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9963Epoch 00099: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0027 - acc: 0.9963 - val_loss: 0.0042 - val_acc: 0.9941\n",
      "Epoch 100/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - acc: 0.9954Epoch 00100: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0033 - acc: 0.9956 - val_loss: 0.0038 - val_acc: 0.9952\n",
      "Epoch 101/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9959Epoch 00101: val_loss improved from 0.00331 to 0.00318, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0030 - acc: 0.9960 - val_loss: 0.0032 - val_acc: 0.9958\n",
      "Epoch 102/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9963Epoch 00102: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0028 - acc: 0.9962 - val_loss: 0.0093 - val_acc: 0.9866\n",
      "Epoch 103/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9959Epoch 00103: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0030 - acc: 0.9960 - val_loss: 0.0127 - val_acc: 0.9808\n",
      "Epoch 104/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9962Epoch 00104: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0028 - acc: 0.9963 - val_loss: 0.0058 - val_acc: 0.9924\n",
      "Epoch 105/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9964Epoch 00105: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0026 - acc: 0.9964 - val_loss: 0.0056 - val_acc: 0.9932\n",
      "Epoch 106/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9959Epoch 00106: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0033 - acc: 0.9956 - val_loss: 0.0045 - val_acc: 0.9942\n",
      "Epoch 107/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0032 - acc: 0.9959Epoch 00107: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0032 - acc: 0.9959 - val_loss: 0.0094 - val_acc: 0.9854\n",
      "Epoch 108/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9965Epoch 00108: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0036 - val_acc: 0.9953\n",
      "Epoch 109/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9950Epoch 00109: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0038 - acc: 0.9949 - val_loss: 0.0043 - val_acc: 0.9941\n",
      "Epoch 110/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9965Epoch 00110: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0026 - acc: 0.9965 - val_loss: 0.0080 - val_acc: 0.9911\n",
      "Epoch 111/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0029 - acc: 0.9960Epoch 00111: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0029 - acc: 0.9960 - val_loss: 0.0032 - val_acc: 0.9956\n",
      "Epoch 112/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9966Epoch 00112: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0025 - acc: 0.9966 - val_loss: 0.0035 - val_acc: 0.9953\n",
      "Epoch 113/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9966Epoch 00119: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0026 - acc: 0.9966 - val_loss: 0.0038 - val_acc: 0.9951\n",
      "Epoch 120/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00120: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0023 - acc: 0.9971 - val_loss: 0.0075 - val_acc: 0.9893\n",
      "Epoch 121/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00121: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0033 - val_acc: 0.9957\n",
      "Epoch 122/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9969Epoch 00122: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0023 - acc: 0.9970 - val_loss: 0.0046 - val_acc: 0.9941\n",
      "Epoch 123/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9968Epoch 00123: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0023 - acc: 0.9969 - val_loss: 0.0091 - val_acc: 0.9876\n",
      "Epoch 124/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9966Epoch 00124: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0025 - acc: 0.9967 - val_loss: 0.0077 - val_acc: 0.9905\n",
      "Epoch 125/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00125: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0061 - val_acc: 0.9922\n",
      "Epoch 126/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9968Epoch 00126: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0025 - acc: 0.9968 - val_loss: 0.0027 - val_acc: 0.9965\n",
      "Epoch 127/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9970Epoch 00127: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0023 - acc: 0.9970 - val_loss: 0.0034 - val_acc: 0.9955\n",
      "Epoch 128/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00128: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9972 - val_loss: 0.0029 - val_acc: 0.9964\n",
      "Epoch 129/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9973Epoch 00129: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0060 - val_acc: 0.9917\n",
      "Epoch 130/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9970Epoch 00130: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0023 - acc: 0.9969 - val_loss: 0.0029 - val_acc: 0.9961\n",
      "Epoch 131/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00131: val_loss improved from 0.00236 to 0.00226, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9976 - val_loss: 0.0023 - val_acc: 0.9970\n",
      "Epoch 132/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00132: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0026 - val_acc: 0.9965\n",
      "Epoch 133/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9973Epoch 00133: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0025 - val_acc: 0.9968\n",
      "Epoch 134/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9975Epoch 00134: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9975 - val_loss: 0.0026 - val_acc: 0.9966\n",
      "Epoch 135/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9969Epoch 00135: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0025 - acc: 0.9968 - val_loss: 0.0028 - val_acc: 0.9962\n",
      "Epoch 136/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9969Epoch 00136: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0024 - acc: 0.9969 - val_loss: 0.0029 - val_acc: 0.9963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9974Epoch 00137: val_loss improved from 0.00226 to 0.00220, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0022 - val_acc: 0.9971\n",
      "Epoch 138/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9974Epoch 00138: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 139/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9977Epoch 00139: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0055 - val_acc: 0.9929\n",
      "Epoch 140/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9972Epoch 00140: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9972 - val_loss: 0.0030 - val_acc: 0.9959\n",
      "Epoch 141/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9967Epoch 00141: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0024 - acc: 0.9968 - val_loss: 0.0037 - val_acc: 0.9950\n",
      "Epoch 142/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9974Epoch 00142: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0061 - val_acc: 0.9930\n",
      "Epoch 143/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00143: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0069 - val_acc: 0.9905\n",
      "Epoch 144/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9970Epoch 00144: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0023 - acc: 0.9971 - val_loss: 0.0034 - val_acc: 0.9957\n",
      "Epoch 145/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00145: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0046 - val_acc: 0.9945\n",
      "Epoch 146/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00146: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0024 - val_acc: 0.9969\n",
      "Epoch 147/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00147: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0029 - val_acc: 0.9963\n",
      "Epoch 148/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9970Epoch 00148: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0054 - val_acc: 0.9928\n",
      "Epoch 149/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0023 - acc: 0.9969Epoch 00149: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0023 - acc: 0.9969 - val_loss: 0.0026 - val_acc: 0.9968\n",
      "Epoch 150/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00150: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0032 - val_acc: 0.9961\n",
      "Epoch 151/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00151: val_loss improved from 0.00220 to 0.00215, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0022 - val_acc: 0.9972\n",
      "Epoch 152/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9974Epoch 00152: val_loss improved from 0.00215 to 0.00210, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9974 - val_loss: 0.0021 - val_acc: 0.9973\n",
      "Epoch 153/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00153: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9975 - val_loss: 0.0025 - val_acc: 0.9966\n",
      "Epoch 154/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9976Epoch 00154: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9977 - val_loss: 0.0022 - val_acc: 0.9971\n",
      "Epoch 155/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9974Epoch 00155: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9975 - val_loss: 0.0025 - val_acc: 0.9968\n",
      "Epoch 156/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00156: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0045 - val_acc: 0.9943\n",
      "Epoch 157/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00157: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 0.9953\n",
      "Epoch 158/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9967Epoch 00158: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0025 - acc: 0.9968 - val_loss: 0.0041 - val_acc: 0.9950\n",
      "Epoch 159/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9975Epoch 00159: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9976 - val_loss: 0.0037 - val_acc: 0.9953\n",
      "Epoch 160/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00160: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0064 - val_acc: 0.9924\n",
      "Epoch 161/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00161: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0017 - acc: 0.9978 - val_loss: 0.0041 - val_acc: 0.9948\n",
      "Epoch 162/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00162: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0026 - val_acc: 0.9967\n",
      "Epoch 163/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00163: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0025 - val_acc: 0.9968\n",
      "Epoch 164/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9974Epoch 00164: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9973 - val_loss: 0.0028 - val_acc: 0.9964\n",
      "Epoch 165/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00165: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0065 - val_acc: 0.9920\n",
      "Epoch 166/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00166: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0040 - val_acc: 0.9953\n",
      "Epoch 167/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00167: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0034 - val_acc: 0.9956\n",
      "Epoch 168/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9977Epoch 00168: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0017 - acc: 0.9978 - val_loss: 0.0037 - val_acc: 0.9953\n",
      "Epoch 169/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00169: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0027 - val_acc: 0.9967\n",
      "Epoch 170/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00170: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0026 - val_acc: 0.9968\n",
      "Epoch 171/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00171: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0018 - acc: 0.9977 - val_loss: 0.0052 - val_acc: 0.9932\n",
      "Epoch 172/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9979Epoch 00172: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0017 - acc: 0.9978 - val_loss: 0.0094 - val_acc: 0.9872\n",
      "Epoch 173/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00173: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0027 - val_acc: 0.9964\n",
      "Epoch 174/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9975Epoch 00174: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9976 - val_loss: 0.0049 - val_acc: 0.9941\n",
      "Epoch 175/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00175: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0059 - val_acc: 0.9920\n",
      "Epoch 176/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00176: val_loss improved from 0.00210 to 0.00193, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0017 - acc: 0.9978 - val_loss: 0.0019 - val_acc: 0.9975\n",
      "Epoch 177/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9979Epoch 00177: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0030 - val_acc: 0.9962\n",
      "Epoch 178/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00178: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0026 - val_acc: 0.9964\n",
      "Epoch 179/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00179: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9977 - val_loss: 0.0050 - val_acc: 0.9932\n",
      "Epoch 180/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9976Epoch 00180: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0023 - acc: 0.9975 - val_loss: 0.0037 - val_acc: 0.9950\n",
      "Epoch 181/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9974Epoch 00181: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.0031 - val_acc: 0.9958\n",
      "Epoch 182/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9971Epoch 00182: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0037 - val_acc: 0.9951\n",
      "Epoch 183/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9974Epoch 00183: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0030 - val_acc: 0.9959\n",
      "Epoch 184/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00184: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0022 - acc: 0.9971 - val_loss: 0.0025 - val_acc: 0.9968\n",
      "Epoch 185/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00185: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 0.9965\n",
      "Epoch 186/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00186: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9980 - val_loss: 0.0057 - val_acc: 0.9929\n",
      "Epoch 187/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00187: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0033 - val_acc: 0.9961\n",
      "Epoch 188/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00188: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9976 - val_loss: 0.0028 - val_acc: 0.9965\n",
      "Epoch 189/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00189: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0022 - val_acc: 0.9971\n",
      "Epoch 190/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00190: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0033 - val_acc: 0.9957\n",
      "Epoch 191/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00191: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0054 - val_acc: 0.9927\n",
      "Epoch 192/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00192: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0025 - val_acc: 0.9969\n",
      "Epoch 193/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00193: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0024 - val_acc: 0.9969\n",
      "Epoch 194/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00194: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0017 - acc: 0.9978 - val_loss: 0.0029 - val_acc: 0.9963\n",
      "Epoch 195/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00195: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0045 - val_acc: 0.9941\n",
      "Epoch 196/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00196: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 0.9966\n",
      "Epoch 197/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9981Epoch 00197: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0023 - val_acc: 0.9968\n",
      "Epoch 198/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00198: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9983 - val_loss: 0.0029 - val_acc: 0.9962\n",
      "Epoch 199/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00199: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 200/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00200: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0069 - val_acc: 0.9914\n",
      "Epoch 201/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00201: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0025 - val_acc: 0.9969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00202: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0036 - val_acc: 0.9958\n",
      "Epoch 203/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00203: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0024 - val_acc: 0.9970\n",
      "Epoch 204/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00204: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0021 - val_acc: 0.9973\n",
      "Epoch 205/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9980Epoch 00205: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 206/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00206: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9982 - val_loss: 0.0022 - val_acc: 0.9971\n",
      "Epoch 207/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00207: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 208/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00208: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 0.9974\n",
      "Epoch 209/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00209: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0020 - val_acc: 0.9973\n",
      "Epoch 210/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00210: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0021 - val_acc: 0.9974\n",
      "Epoch 211/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00211: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0024 - val_acc: 0.9969\n",
      "Epoch 212/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00212: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9983 - val_loss: 0.0043 - val_acc: 0.9943\n",
      "Epoch 213/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00213: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 214/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00214: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0026 - val_acc: 0.9969\n",
      "Epoch 215/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00215: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0029 - val_acc: 0.9964\n",
      "Epoch 216/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00216: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0051 - val_acc: 0.9939\n",
      "Epoch 217/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00217: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0021 - val_acc: 0.9973\n",
      "Epoch 218/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00218: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 219/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9982Epoch 00219: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0034 - val_acc: 0.9958\n",
      "Epoch 220/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00220: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 221/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00221: val_loss improved from 0.00193 to 0.00185, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0019 - val_acc: 0.9975\n",
      "Epoch 222/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9979Epoch 00222: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0019 - val_acc: 0.9975\n",
      "Epoch 223/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00223: val_loss improved from 0.00185 to 0.00167, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0017 - val_acc: 0.9979\n",
      "Epoch 224/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00224: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0025 - val_acc: 0.9969\n",
      "Epoch 225/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00225: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0022 - val_acc: 0.9972\n",
      "Epoch 226/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9982Epoch 00226: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9982 - val_loss: 0.0024 - val_acc: 0.9968\n",
      "Epoch 227/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0014 - acc: 0.9982Epoch 00227: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0028 - val_acc: 0.9965\n",
      "Epoch 228/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00228: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0021 - val_acc: 0.9973\n",
      "Epoch 229/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00229: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0017 - val_acc: 0.9978\n",
      "Epoch 230/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00230: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0019 - val_acc: 0.9975\n",
      "Epoch 231/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00231: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9977\n",
      "Epoch 232/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00232: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0019 - val_acc: 0.9977\n",
      "Epoch 233/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00233: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0023 - val_acc: 0.9972\n",
      "Epoch 234/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00234: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0021 - val_acc: 0.9973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00235: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0021 - val_acc: 0.9974\n",
      "Epoch 236/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00236: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0021 - val_acc: 0.9974\n",
      "Epoch 237/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00237: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0027 - val_acc: 0.9966\n",
      "Epoch 238/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00238: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0023 - val_acc: 0.9972\n",
      "Epoch 239/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00239: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0026 - val_acc: 0.9969\n",
      "Epoch 240/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00240: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0031 - val_acc: 0.9961\n",
      "Epoch 241/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9985Epoch 00241: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0024 - val_acc: 0.9970\n",
      "Epoch 242/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00242: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0023 - val_acc: 0.9972\n",
      "Epoch 243/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9985Epoch 00243: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0026 - val_acc: 0.9969\n",
      "Epoch 244/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.4634e-04 - acc: 0.9988Epoch 00244: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.5233e-04 - acc: 0.9988 - val_loss: 0.0025 - val_acc: 0.9970\n",
      "Epoch 245/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00245: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 246/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00246: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0019 - val_acc: 0.9975\n",
      "Epoch 247/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00247: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0022 - val_acc: 0.9972\n",
      "Epoch 248/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00248: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 249/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00249: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9980 - val_loss: 0.0025 - val_acc: 0.9970\n",
      "Epoch 250/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9980Epoch 00250: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9980 - val_loss: 0.0022 - val_acc: 0.9970\n",
      "Epoch 251/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00251: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0020 - val_acc: 0.9974\n",
      "Epoch 252/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00252: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0031 - val_acc: 0.9961\n",
      "Epoch 253/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00253: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0025 - val_acc: 0.9968\n",
      "Epoch 254/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00254: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9982 - val_loss: 0.0022 - val_acc: 0.9971\n",
      "Epoch 255/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00255: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 256/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00256: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0021 - val_acc: 0.9972\n",
      "Epoch 257/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00257: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0026 - val_acc: 0.9967\n",
      "Epoch 258/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9984Epoch 00258: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 259/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00259: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0019 - val_acc: 0.9974\n",
      "Epoch 260/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00260: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0026 - val_acc: 0.9967\n",
      "Epoch 261/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00261: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0021 - val_acc: 0.9974\n",
      "Epoch 262/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987   Epoch 00262: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.7505e-04 - acc: 0.9987 - val_loss: 0.0018 - val_acc: 0.9977\n",
      "Epoch 263/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00263: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0018 - val_acc: 0.9976\n",
      "Epoch 264/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9981Epoch 00264: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0015 - acc: 0.9981 - val_loss: 0.0063 - val_acc: 0.9927\n",
      "Epoch 265/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9979Epoch 00265: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0016 - acc: 0.9979 - val_loss: 0.0130 - val_acc: 0.9830\n",
      "Epoch 266/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9972Epoch 00266: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0137 - val_acc: 0.9822\n",
      "Epoch 267/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9976Epoch 00267: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9976 - val_loss: 0.0038 - val_acc: 0.9953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9975Epoch 00268: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9975 - val_loss: 0.0178 - val_acc: 0.9761\n",
      "Epoch 269/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0019 - acc: 0.9975Epoch 00269: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0020 - acc: 0.9974 - val_loss: 0.0053 - val_acc: 0.9935\n",
      "Epoch 270/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9972Epoch 00270: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0047 - val_acc: 0.9940\n",
      "Epoch 271/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 0.9978Epoch 00271: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0017 - acc: 0.9978 - val_loss: 0.0183 - val_acc: 0.9749\n",
      "Epoch 272/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9977Epoch 00272: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0019 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 0.9948\n",
      "Epoch 273/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9976Epoch 00273: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0018 - acc: 0.9976 - val_loss: 0.0037 - val_acc: 0.9951\n",
      "Epoch 274/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00274: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9984 - val_loss: 0.0032 - val_acc: 0.9958\n",
      "Epoch 275/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00275: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0055 - val_acc: 0.9926\n",
      "Epoch 276/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9985Epoch 00276: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0025 - val_acc: 0.9968\n",
      "Epoch 277/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00277: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0045 - val_acc: 0.9942\n",
      "Epoch 278/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9982Epoch 00278: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0014 - acc: 0.9981 - val_loss: 0.0024 - val_acc: 0.9970\n",
      "Epoch 279/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9984Epoch 00279: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9984 - val_loss: 0.0037 - val_acc: 0.9953\n",
      "Epoch 280/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9985Epoch 00280: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0034 - val_acc: 0.9956\n",
      "Epoch 281/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00281: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9986 - val_loss: 0.0024 - val_acc: 0.9970\n",
      "Epoch 282/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9985Epoch 00282: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0025 - val_acc: 0.9968\n",
      "Epoch 283/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00283: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0032 - val_acc: 0.9960\n",
      "Epoch 284/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986  Epoch 00284: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0042 - val_acc: 0.9944\n",
      "Epoch 285/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9987Epoch 00285: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0035 - val_acc: 0.9956\n",
      "Epoch 286/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00286: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0027 - val_acc: 0.9967\n",
      "Epoch 287/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00287: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0036 - val_acc: 0.9960\n",
      "Epoch 288/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.5798e-04 - acc: 0.9988Epoch 00288: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.5723e-04 - acc: 0.9988 - val_loss: 0.0026 - val_acc: 0.9966\n",
      "Epoch 289/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00289: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0027 - val_acc: 0.9968\n",
      "Epoch 290/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.0011e-04 - acc: 0.9989Epoch 00290: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.0047e-04 - acc: 0.9989 - val_loss: 0.0029 - val_acc: 0.9965\n",
      "Epoch 291/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00291: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0025 - val_acc: 0.9967\n",
      "Epoch 292/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987    Epoch 00292: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0021 - val_acc: 0.9974\n",
      "Epoch 293/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.7183e-04 - acc: 0.9987Epoch 00293: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.5384e-04 - acc: 0.9988 - val_loss: 0.0034 - val_acc: 0.9956\n",
      "Epoch 294/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00294: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0028 - val_acc: 0.9964\n",
      "Epoch 295/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.0971e-04 - acc: 0.9988Epoch 00295: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.1901e-04 - acc: 0.9988 - val_loss: 0.0022 - val_acc: 0.9972\n",
      "Epoch 296/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.8218e-04 - acc: 0.9988Epoch 00296: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.9920e-04 - acc: 0.9988 - val_loss: 0.0024 - val_acc: 0.9969\n",
      "Epoch 297/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.5275e-04 - acc: 0.9989Epoch 00297: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.4260e-04 - acc: 0.9989 - val_loss: 0.0024 - val_acc: 0.9969\n",
      "Epoch 298/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00298: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 299/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.2753e-04 - acc: 0.9988Epoch 00299: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.4329e-04 - acc: 0.9988 - val_loss: 0.0021 - val_acc: 0.9975\n",
      "Epoch 300/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.9650e-04 - acc: 0.9987Epoch 00300: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.7868e-04 - acc: 0.9988 - val_loss: 0.0020 - val_acc: 0.9974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.1390e-04 - acc: 0.9988Epoch 00301: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.0089e-04 - acc: 0.9988 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 302/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985    Epoch 00302: val_loss improved from 0.00167 to 0.00161, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0016 - val_acc: 0.9979\n",
      "Epoch 303/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.6761e-04 - acc: 0.9988Epoch 00303: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.9167e-04 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9975\n",
      "Epoch 304/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.5599e-04 - acc: 0.9987Epoch 00304: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.5770e-04 - acc: 0.9987 - val_loss: 0.0029 - val_acc: 0.9961\n",
      "Epoch 305/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0013 - acc: 0.9983Epoch 00305: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0036 - val_acc: 0.9952\n",
      "Epoch 306/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.4028e-04 - acc: 0.9988Epoch 00306: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.6191e-04 - acc: 0.9987 - val_loss: 0.0022 - val_acc: 0.9971\n",
      "Epoch 307/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9985Epoch 00307: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0022 - val_acc: 0.9971\n",
      "Epoch 308/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9986Epoch 00308: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0017 - val_acc: 0.9978\n",
      "Epoch 309/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9986Epoch 00309: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 310/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.7103e-04 - acc: 0.9987Epoch 00310: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.3276e-04 - acc: 0.9988 - val_loss: 0.0018 - val_acc: 0.9977\n",
      "Epoch 311/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.6376e-04 - acc: 0.9987Epoch 00311: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.4321e-04 - acc: 0.9988 - val_loss: 0.0023 - val_acc: 0.9972\n",
      "Epoch 312/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.6361e-04 - acc: 0.9989Epoch 00312: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.5802e-04 - acc: 0.9989 - val_loss: 0.0023 - val_acc: 0.9970\n",
      "Epoch 313/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.2140e-04 - acc: 0.9988Epoch 00313: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.0770e-04 - acc: 0.9988 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 314/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987  Epoch 00314: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 315/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.9051e-04 - acc: 0.9987Epoch 00315: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0020 - val_acc: 0.9976\n",
      "Epoch 316/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.5189e-04 - acc: 0.9989Epoch 00316: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.4660e-04 - acc: 0.9989 - val_loss: 0.0024 - val_acc: 0.9971\n",
      "Epoch 317/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.8379e-04 - acc: 0.9987Epoch 00317: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.4352e-04 - acc: 0.9988 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 318/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.2532e-04 - acc: 0.9988Epoch 00318: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.1403e-04 - acc: 0.9988 - val_loss: 0.0029 - val_acc: 0.9965\n",
      "Epoch 319/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.9146e-04 - acc: 0.9987Epoch 00319: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.7747e-04 - acc: 0.9987 - val_loss: 0.0027 - val_acc: 0.9967\n",
      "Epoch 320/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00320: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 321/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987    Epoch 00321: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.9963e-04 - acc: 0.9987 - val_loss: 0.0024 - val_acc: 0.9970\n",
      "Epoch 322/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.0354e-04 - acc: 0.9988Epoch 00322: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.1289e-04 - acc: 0.9988 - val_loss: 0.0026 - val_acc: 0.9969\n",
      "Epoch 323/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.6578e-04 - acc: 0.9987Epoch 00323: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.6690e-04 - acc: 0.9987 - val_loss: 0.0027 - val_acc: 0.9967\n",
      "Epoch 324/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00324: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0027 - val_acc: 0.9968\n",
      "Epoch 325/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.8717e-04 - acc: 0.9987Epoch 00325: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.9304e-04 - acc: 0.9987 - val_loss: 0.0021 - val_acc: 0.9973\n",
      "Epoch 326/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986  Epoch 00326: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0030 - val_acc: 0.9964\n",
      "Epoch 327/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.6174e-04 - acc: 0.9987Epoch 00327: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.5762e-04 - acc: 0.9987 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 328/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00328: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0027 - val_acc: 0.9966\n",
      "Epoch 329/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987    Epoch 00329: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.9793e-04 - acc: 0.9987 - val_loss: 0.0022 - val_acc: 0.9973\n",
      "Epoch 330/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00330: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0022 - val_acc: 0.9973\n",
      "Epoch 331/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00331: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.9758e-04 - acc: 0.9987 - val_loss: 0.0025 - val_acc: 0.9970\n",
      "Epoch 332/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.5193e-04 - acc: 0.9988Epoch 00332: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0023 - val_acc: 0.9971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 333/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.1555e-04 - acc: 0.9988Epoch 00333: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.9865e-04 - acc: 0.9988 - val_loss: 0.0021 - val_acc: 0.9973\n",
      "Epoch 334/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9985Epoch 00334: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.0018 - val_acc: 0.9977\n",
      "Epoch 335/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987   Epoch 00335: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9975\n",
      "Epoch 336/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986 Epoch 00336: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 337/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987   Epoch 00337: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 338/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.7003e-04 - acc: 0.9987Epoch 00338: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 339/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.5664e-04 - acc: 0.9989Epoch 00339: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.5114e-04 - acc: 0.9989 - val_loss: 0.0022 - val_acc: 0.9973\n",
      "Epoch 340/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00340: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0022 - val_acc: 0.9973\n",
      "Epoch 341/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.5714e-04 - acc: 0.9988Epoch 00341: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.8312e-04 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9977\n",
      "Epoch 342/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.5370e-04 - acc: 0.9989Epoch 00342: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.4676e-04 - acc: 0.9989 - val_loss: 0.0021 - val_acc: 0.9974\n",
      "Epoch 343/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.8495e-04 - acc: 0.9987Epoch 00343: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.7132e-04 - acc: 0.9987 - val_loss: 0.0023 - val_acc: 0.9971\n",
      "Epoch 344/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.9076e-04 - acc: 0.9988Epoch 00344: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.6190e-04 - acc: 0.9988 - val_loss: 0.0028 - val_acc: 0.9966\n",
      "Epoch 345/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.8913e-04 - acc: 0.9987Epoch 00345: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.8869e-04 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9977\n",
      "Epoch 346/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.9316e-04 - acc: 0.9987Epoch 00346: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.8034e-04 - acc: 0.9988 - val_loss: 0.0024 - val_acc: 0.9971\n",
      "Epoch 347/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.1226e-04 - acc: 0.9988Epoch 00347: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.2737e-04 - acc: 0.9988 - val_loss: 0.0026 - val_acc: 0.9969\n",
      "Epoch 348/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.1127e-04 - acc: 0.9989Epoch 00348: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 7.9176e-04 - acc: 0.9990 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 349/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.3483e-04 - acc: 0.9988Epoch 00349: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.2475e-04 - acc: 0.9988 - val_loss: 0.0028 - val_acc: 0.9967\n",
      "Epoch 350/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.1295e-04 - acc: 0.9988Epoch 00350: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.1392e-04 - acc: 0.9988 - val_loss: 0.0022 - val_acc: 0.9972\n",
      "Epoch 351/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.9425e-04 - acc: 0.9987Epoch 00351: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.5978e-04 - acc: 0.9987 - val_loss: 0.0020 - val_acc: 0.9976\n",
      "Epoch 352/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.8341e-04 - acc: 0.9988Epoch 00352: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.7193e-04 - acc: 0.9989 - val_loss: 0.0027 - val_acc: 0.9968\n",
      "Epoch 353/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00353: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0024 - val_acc: 0.9971\n",
      "Epoch 354/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.6492e-04 - acc: 0.9989Epoch 00354: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.7949e-04 - acc: 0.9988 - val_loss: 0.0022 - val_acc: 0.9973\n",
      "Epoch 355/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987   Epoch 00355: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 356/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.3828e-04 - acc: 0.9989Epoch 00356: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.2349e-04 - acc: 0.9989 - val_loss: 0.0017 - val_acc: 0.9979\n",
      "Epoch 357/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.2385e-04 - acc: 0.9989Epoch 00357: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.6240e-04 - acc: 0.9989 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 358/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.6629e-04 - acc: 0.9989Epoch 00358: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.6585e-04 - acc: 0.9989 - val_loss: 0.0020 - val_acc: 0.9975\n",
      "Epoch 359/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.1494e-04 - acc: 0.9989Epoch 00359: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.1484e-04 - acc: 0.9989 - val_loss: 0.0022 - val_acc: 0.9973\n",
      "Epoch 360/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.2475e-04 - acc: 0.9989Epoch 00360: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.1470e-04 - acc: 0.9989 - val_loss: 0.0023 - val_acc: 0.9972\n",
      "Epoch 361/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.3809e-04 - acc: 0.9990Epoch 00361: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 7.4943e-04 - acc: 0.9990 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 362/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.7754e-04 - acc: 0.9990Epoch 00362: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 7.6716e-04 - acc: 0.9990 - val_loss: 0.0021 - val_acc: 0.9974\n",
      "Epoch 363/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.8104e-04 - acc: 0.9990Epoch 00363: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 7.7942e-04 - acc: 0.9990 - val_loss: 0.0025 - val_acc: 0.9970\n",
      "Epoch 364/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.9200e-04 - acc: 0.9989Epoch 00364: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.6772e-04 - acc: 0.9989 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 365/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.2858e-04 - acc: 0.9988Epoch 00365: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.5085e-04 - acc: 0.9988 - val_loss: 0.0022 - val_acc: 0.9972\n",
      "Epoch 366/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.5912e-04 - acc: 0.9989Epoch 00366: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.4610e-04 - acc: 0.9989 - val_loss: 0.0021 - val_acc: 0.9975\n",
      "Epoch 367/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.6152e-04 - acc: 0.9987Epoch 00367: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.4061e-04 - acc: 0.9988 - val_loss: 0.0023 - val_acc: 0.9972\n",
      "Epoch 368/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987 Epoch 00368: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0025 - val_acc: 0.9969\n",
      "Epoch 369/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00369: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9987 - val_loss: 0.0017 - val_acc: 0.9978\n",
      "Epoch 370/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987   Epoch 00370: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0021 - val_acc: 0.9974\n",
      "Epoch 371/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00371: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0021 - val_acc: 0.9973\n",
      "Epoch 372/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.5615e-04 - acc: 0.9989Epoch 00372: val_loss improved from 0.00161 to 0.00161, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.6157e-04 - acc: 0.9989 - val_loss: 0.0016 - val_acc: 0.9980\n",
      "Epoch 373/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987    Epoch 00373: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 374/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0012 - acc: 0.9986  Epoch 00374: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9986 - val_loss: 0.0025 - val_acc: 0.9970\n",
      "Epoch 375/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.8267e-04 - acc: 0.9989Epoch 00375: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.5405e-04 - acc: 0.9989 - val_loss: 0.0026 - val_acc: 0.9968\n",
      "Epoch 376/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00376: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 0.9987 - val_loss: 0.0026 - val_acc: 0.9966\n",
      "Epoch 377/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986Epoch 00377: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 378/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.2843e-04 - acc: 0.9989Epoch 00378: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.2545e-04 - acc: 0.9989 - val_loss: 0.0020 - val_acc: 0.9976\n",
      "Epoch 379/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987Epoch 00379: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0023 - val_acc: 0.9972\n",
      "Epoch 380/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.9333e-04 - acc: 0.9987Epoch 00380: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 9.8982e-04 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 381/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.8091e-04 - acc: 0.9988Epoch 00381: val_loss improved from 0.00161 to 0.00157, saving model to ./save/model.weights.best.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.7904e-04 - acc: 0.9988 - val_loss: 0.0016 - val_acc: 0.9980\n",
      "Epoch 382/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.5267e-04 - acc: 0.9989Epoch 00382: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.6704e-04 - acc: 0.9989 - val_loss: 0.0016 - val_acc: 0.9980\n",
      "Epoch 383/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.1831e-04 - acc: 0.9989Epoch 00383: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 7.8802e-04 - acc: 0.9990 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 384/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0010 - acc: 0.9987    Epoch 00384: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 385/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.2887e-04 - acc: 0.9989Epoch 00385: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.2164e-04 - acc: 0.9989 - val_loss: 0.0022 - val_acc: 0.9973\n",
      "Epoch 386/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.9116e-04 - acc: 0.9990Epoch 00386: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.1815e-04 - acc: 0.9989 - val_loss: 0.0019 - val_acc: 0.9976\n",
      "Epoch 387/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0011 - acc: 0.9986    Epoch 00387: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0025 - val_acc: 0.9970\n",
      "Epoch 388/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.1578e-04 - acc: 0.9989Epoch 00388: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.2772e-04 - acc: 0.9989 - val_loss: 0.0031 - val_acc: 0.9963\n",
      "Epoch 389/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.8061e-04 - acc: 0.9988Epoch 00389: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.7718e-04 - acc: 0.9988 - val_loss: 0.0021 - val_acc: 0.9975\n",
      "Epoch 390/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 8.1780e-04 - acc: 0.9989Epoch 00390: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 8.0771e-04 - acc: 0.9990 - val_loss: 0.0017 - val_acc: 0.9979\n",
      "Epoch 391/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 9.6844e-04 - acc: 0.9987"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00906: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 6.2640e-04 - acc: 0.9991 - val_loss: 0.0024 - val_acc: 0.9972\n",
      "Epoch 907/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.2853e-04 - acc: 0.9990Epoch 00907: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 7.3464e-04 - acc: 0.9990 - val_loss: 0.0031 - val_acc: 0.9964\n",
      "Epoch 908/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.1943e-04 - acc: 0.9990Epoch 00908: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 7.0414e-04 - acc: 0.9991 - val_loss: 0.0028 - val_acc: 0.9967\n",
      "Epoch 909/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 6.2103e-04 - acc: 0.9992Epoch 00909: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 6.1189e-04 - acc: 0.9992 - val_loss: 0.0026 - val_acc: 0.9969\n",
      "Epoch 910/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 5.7067e-04 - acc: 0.9992Epoch 00910: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 5.9226e-04 - acc: 0.9992 - val_loss: 0.0020 - val_acc: 0.9976\n",
      "Epoch 911/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.0587e-04 - acc: 0.9991Epoch 00911: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 6.7246e-04 - acc: 0.9991 - val_loss: 0.0036 - val_acc: 0.9957\n",
      "Epoch 912/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 6.5634e-04 - acc: 0.9991Epoch 00912: val_loss did not improve\n",
      "10/10 [==============================] - 17s 2s/step - loss: 6.3585e-04 - acc: 0.9992 - val_loss: 0.0027 - val_acc: 0.9968\n",
      "Epoch 913/2000\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 7.2489e-04 - acc: 0.9991"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-9ccf9c36d346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                 callbacks=[checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2116\u001b[0m                                 \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2118\u001b[0;31m                                 use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   2119\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2231\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2233\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1854\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss',\n",
    "#                                min_delta=0,\n",
    "#                                patience=2,\n",
    "#                                verbose=0,\n",
    "#                                mode='auto')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./save/model.weights.best.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='min') \n",
    "\n",
    "history = model.fit_generator(  generator(4, training_filenames),\n",
    "                                validation_data=generator(8, validation_filenames),\n",
    "                                steps_per_epoch=10,\n",
    "                                validation_steps = 10,\n",
    "                                epochs=2000,\n",
    "                                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result():\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('./save/model.d4.weights.last.h5')\n",
    "#model.save_weights('./save/model.d4.weights.last.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case1_label10.npy\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = validation_filenames[6]\n",
    "print(filename)\n",
    "x = np.load(data_path.format('x') + filename)\n",
    "plt.imshow(x[:,:,65], cmap='gray')\n",
    "x = np.reshape(x, (1, 128, 128, 128, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 128, 1)\n",
      "1.0539596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f488d7bce80>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = model.predict(x)\n",
    "print(y.shape)\n",
    "y = np.reshape(y, (128, 128, 128))\n",
    "print(np.max(y))\n",
    "plt.imshow(y[:,:,65], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(data_path.format('y') + validation_filenames[0])\n",
    "plt.imshow(y[:,:,65], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./save/model.weights.best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case9\n",
      "9\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def generator(batch_size, filenames):\n",
    "    size = 128\n",
    "    \n",
    "    shape = (batch_size, size, size, size, 1)\n",
    "\n",
    "    x_out = np.zeros(shape)\n",
    "    y_out = np.zeros(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        filename = random.choice(filenames)\n",
    "        #filename = filenames[0]\n",
    "        x = np.load(data_path.format('x') + filename)\n",
    "        y = np.load(data_path.format('y') + filename)\n",
    "        x = np.reshape(x, (size, size, size, -1))\n",
    "        y = np.reshape(y, (size, size, size, -1))\n",
    "\n",
    "        x_out[i] = x\n",
    "        y_out[i] = y\n",
    "    \n",
    "        print(filename.split('_')[0])\n",
    "        print(int(re.sub(\"\\D\", \"\", filename.split('_')[0])))\n",
    "        \n",
    "        label = int(re.sub(\"\\D\", \"\", filename.split('_')[0]))\n",
    "        y_out2 = [0] * 20\n",
    "        y_out2[label] = 1\n",
    "        print(y_out2)\n",
    "        \n",
    "generator(1, training_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(random.randint(-2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 125)\n"
     ]
    }
   ],
   "source": [
    "    x_bias, y_bias, z_bias = random.randint(-2,2), random.randint(-2,2), random.randint(-2,2)\n",
    "    x_range = (x_bias, 127) if x_bias > 0 else (0, size + x_bias -1)\n",
    "    y_range = (y_bias, 127) if y_bias > 0 else (0, size + y_bias -1)\n",
    "    z_range = (z_bias, 127) if z_bias > 0 else (0, size + z_bias -1)\n",
    "    print(x_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
